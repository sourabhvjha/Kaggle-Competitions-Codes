{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "import re\n",
    "from collections import Counter\n",
    "from collections import namedtuple\n",
    "import os\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.layers.core import Dense\n",
    "from tensorflow.python.ops.rnn_cell_impl import _zero_state_tensors\n",
    "import time\n",
    "import sys\n",
    "import random\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sourabhjha/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/home/sourabhjha/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/home/sourabhjha/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if sys.path[0] == '':\n",
      "/home/sourabhjha/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9918441, 10) (956046, 3)\n",
      "(7935384, 10) (1983057, 10)\n",
      "PLAIN         7353693\n",
      "PUNCT         1880507\n",
      "DATE           258348\n",
      "LETTERS        152795\n",
      "CARDINAL       133744\n",
      "VERBATIM        78108\n",
      "MEASURE         14783\n",
      "ORDINAL         12703\n",
      "DECIMAL          9821\n",
      "MONEY            6128\n",
      "DIGIT            5442\n",
      "ELECTRONIC       5162\n",
      "TELEPHONE        4024\n",
      "TIME             1465\n",
      "FRACTION         1196\n",
      "ADDRESS           522\n",
      "Name: class, dtype: int64\n",
      "PLAIN         5882948\n",
      "PUNCT         1505157\n",
      "DATE           206643\n",
      "LETTERS        122165\n",
      "CARDINAL       106879\n",
      "VERBATIM        62691\n",
      "MEASURE         11768\n",
      "ORDINAL         10090\n",
      "DECIMAL          7905\n",
      "MONEY            4918\n",
      "DIGIT            4363\n",
      "ELECTRONIC       4156\n",
      "TELEPHONE        3186\n",
      "TIME             1182\n",
      "FRACTION          917\n",
      "ADDRESS           416\n",
      "Name: class, dtype: int64\n",
      "PLAIN         1470745\n",
      "PUNCT          375350\n",
      "DATE            51705\n",
      "LETTERS         30630\n",
      "CARDINAL        26865\n",
      "VERBATIM        15417\n",
      "MEASURE          3015\n",
      "ORDINAL          2613\n",
      "DECIMAL          1916\n",
      "MONEY            1210\n",
      "DIGIT            1079\n",
      "ELECTRONIC       1006\n",
      "TELEPHONE         838\n",
      "TIME              283\n",
      "FRACTION          279\n",
      "ADDRESS           106\n",
      "Name: class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv(\"../Data/en_train.csv\")\n",
    "test_data = pd.read_csv(\"../Data/en_test_2.csv\")\n",
    "sample_submission = pd.read_csv(\"../Data/en_sample_submission_2.csv\")\n",
    "train_data.before = train_data.before.astype(str)\n",
    "train_data.after = train_data.after.astype(str)\n",
    "test_data.before = test_data.before.astype(str)\n",
    "train_data['prev_before'] = train_data['before'].shift(1)\n",
    "train_data['prev_before'][train_data.sentence_id != train_data.sentence_id.shift(1)] = \"\"\n",
    "train_data['prev_prev_before'] = train_data['before'].shift(2)\n",
    "train_data['prev_prev_before'][train_data.sentence_id != train_data.sentence_id.shift(2)] = \"\"\n",
    "train_data['next_before'] = train_data['before'].shift(-1)\n",
    "train_data['next_before'][train_data.sentence_id != train_data.sentence_id.shift(-1)] = \"\"\n",
    "train_data['next_next_before'] = train_data['before'].shift(-2)\n",
    "train_data['next_next_before'][train_data.sentence_id != train_data.sentence_id.shift(-2)] = \"\"\n",
    "\n",
    "train_data['before_phrase'] = (train_data.prev_prev_before + \"<SEP>\" + train_data.prev_before + \"<SEP>\" +  train_data.before \n",
    "                                + \"<SEP>\" + train_data.next_before + \"<SEP>\" + train_data.next_next_before)\n",
    "def truncate(x, phrase_len = 10):\n",
    "    words = x.split(\"<SEP>\")\n",
    "    truncated_words = []\n",
    "    if len(words[1]) < phrase_len:\n",
    "        if len(words[0]) < phrase_len - len(words[1]):\n",
    "            truncated_words.append(words[0])\n",
    "            truncated_words.append(words[1])\n",
    "        else:\n",
    "            truncated_words.append(words[0][-(phrase_len - len(words[1])) : ])\n",
    "            truncated_words.append(words[1])\n",
    "    else:\n",
    "        truncated_words.append(\"\")\n",
    "        truncated_words.append(words[1][-phrase_len : ])\n",
    "    truncated_words.append(words[2])\n",
    "    if len(words[3]) < phrase_len:\n",
    "        if len(words[4]) < phrase_len - len(words[3]):\n",
    "            truncated_words.append(words[3])\n",
    "            truncated_words.append(words[4])\n",
    "        else:\n",
    "            truncated_words.append(words[3])\n",
    "            truncated_words.append(words[4][-(phrase_len - len(words[3])) : ])\n",
    "    else:\n",
    "        truncated_words.append(words[3][-phrase_len : ])\n",
    "        truncated_words.append(\"\")\n",
    "    return \"<SEP>\".join(truncated_words)\n",
    "train_data['before_phrase'] = train_data['before_phrase'].map(truncate)\n",
    "print train_data.shape, test_data.shape\n",
    "random.seed(5)\n",
    "dev_sentences = set(random.sample(list(set(train_data.sentence_id)),int(0.8* train_data.sentence_id.nunique())))\n",
    "dev_data = train_data[train_data.sentence_id.isin(dev_sentences)]\n",
    "val_data = train_data[train_data.sentence_id.isin(dev_sentences) == False]\n",
    "print dev_data.shape, val_data.shape\n",
    "\n",
    "print train_data['class'].value_counts()\n",
    "print dev_data['class'].value_counts()\n",
    "print val_data['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DATE          206643\n",
       "CARDINAL      103386\n",
       "MEASURE        11689\n",
       "ORDINAL         8843\n",
       "DECIMAL         7905\n",
       "MONEY           4918\n",
       "DIGIT           4363\n",
       "TELEPHONE       3186\n",
       "TIME            1182\n",
       "FRACTION         907\n",
       "ADDRESS          416\n",
       "VERBATIM         226\n",
       "ELECTRONIC        37\n",
       "PLAIN             18\n",
       "PUNCT              2\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rx = re.compile(\"[A-Za-z0-9]\\.[a-z]\")\n",
    "dev_data_numeric = dev_data[(dev_data.before.map(lambda x : max([i in x for i in \"0123456789\"])) == 1) & \n",
    "                       (dev_data.before.map(lambda x : 1.0 if len(rx.findall(x)) > 0 and \n",
    "                        \" \" not in x and len(x) > 5 else 0.0) == 0.0)]\n",
    "val_data_numeric = val_data[(val_data.before.map(lambda x : max([i in x for i in \"0123456789\"])) == 1) & \n",
    "                       (val_data.before.map(lambda x : 1.0 if len(rx.findall(x)) > 0 and \n",
    "                        \" \" not in x and len(x) > 5 else 0.0) == 0.0)]\n",
    "dev_data_numeric['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LETTERS       99718\n",
       "PLAIN         31670\n",
       "VERBATIM       6887\n",
       "CARDINAL       3015\n",
       "ORDINAL         935\n",
       "ELECTRONIC       25\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_data_letters = dev_data[(dev_data.index.isin(dev_data_numeric.index) == False) & \n",
    "                ((dev_data.before.map(lambda x : x.upper() == x)) | \n",
    "                 (dev_data.before.map(lambda x : max([i in \"aeiouy\" for i in x.lower()])) == 0)) &\n",
    "                (dev_data.before.map(lambda x : min([i in \"abcdefghijklmnopqrstuvwxyz. \" for i in x.lower()])) == 1) &\n",
    "                (dev_data.before.map(len) > 1)]\n",
    "\n",
    "val_data_letters = val_data[(val_data.index.isin(val_data_numeric.index) == False) & \n",
    "                ((val_data.before.map(lambda x : x.upper() == x)) | \n",
    "                 (val_data.before.map(lambda x : max([i in \"aeiouy\" for i in x.lower()])) == 0)) &\n",
    "                (val_data.before.map(lambda x : min([i in \"abcdefghijklmnopqrstuvwxyz. \" for i in x.lower()])) == 1) &\n",
    "                (val_data.before.map(len) > 1)]\n",
    "\n",
    "dev_data_letters['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1496295, 10) 0.990732442466 (362398, 10) 1.0\n",
      "(5987031, 10) 0.990733303369 (1452382, 10) 0.999999311476\n",
      "53228\n",
      "(197767, 10) 0.949137115899 (1298528, 10) 0.99706744868\n",
      "(793548, 10) 0.949560707103 (5193483, 10) 0.99702434763\n",
      "(196846, 10) 0.953577923859 (921, 10) 0.757871878393\n",
      "(789742, 10) 0.954136920665 (3806, 10) 0.786915396742\n",
      "(69215, 10) 0.892942281297 (127631, 10) 0.986460969514\n",
      "(278029, 10) 0.895118854508 (511713, 10) 0.986203203749\n"
     ]
    }
   ],
   "source": [
    "dev_rest = dev_data[(dev_data.index.isin(dev_data_numeric.index) == False) & \n",
    "         (dev_data.index.isin(dev_data_letters.index) == False)]\n",
    "val_rest = val_data[(val_data.index.isin(val_data_numeric.index) == False) & \n",
    "         (val_data.index.isin(val_data_letters.index) == False)]\n",
    "punctuations = set(train_data[train_data['class'] =='PUNCT']['before']) - {\"-\", \":\"}\n",
    "val_punct = val_rest[val_rest.before.isin(punctuations)]\n",
    "val_rest = val_rest[val_rest.before.isin(punctuations) == False]\n",
    "print val_rest.shape, np.mean(val_rest.before == val_rest.after), val_punct.shape, np.mean(val_punct.before == val_punct.after)\n",
    "\n",
    "dev_punct = dev_rest[dev_rest.before.isin(punctuations)]\n",
    "dev_rest = dev_rest[dev_rest.before.isin(punctuations) == False]\n",
    "print dev_rest.shape, np.mean(dev_rest.before == dev_rest.after), dev_punct.shape,np.mean(dev_punct.before == dev_punct.after)\n",
    "\n",
    "clean_words = set(word.lower() for word in nltk.corpus.treebank.words()).union(\n",
    "                                set(word.lower() for word in nltk.corpus.brown.words()))\n",
    "clean_words.remove(\"-\")\n",
    "clean_words.remove(\":\")\n",
    "clean_words.remove(\"i\")\n",
    "clean_words.remove(\"no\")\n",
    "\n",
    "print len(clean_words)\n",
    "val_clean = val_rest[val_rest.before.map(lambda x : x.lower() in clean_words)]\n",
    "val_rest = val_rest[val_rest.before.map(lambda x : x.lower() in clean_words) == False]\n",
    "print val_rest.shape, np.mean(val_rest.before == val_rest.after), val_clean.shape, np.mean(val_clean.before == val_clean.after)\n",
    "\n",
    "dev_clean = dev_rest[dev_rest.before.map(lambda x : x.lower() in clean_words)]\n",
    "dev_rest = dev_rest[dev_rest.before.map(lambda x : x.lower() in clean_words) == False]\n",
    "print dev_rest.shape, np.mean(dev_rest.before == dev_rest.after), dev_clean.shape, np.mean(dev_clean.before == dev_clean.after)\n",
    "\n",
    "def clean_elec(x):\n",
    "    return \" \".join(x).replace(\".\", \"dot\").replace(\":\", \"colon\").replace(\"/\", \"s l a s h\")\n",
    "rel = re.compile(\"[A-Za-z]\\.[a-z]+\")\n",
    "val_elec = val_rest[val_rest.before.map(lambda x : len(rel.findall(x)) > 0)]\n",
    "val_rest = val_rest[val_rest.before.map(lambda x : len(rel.findall(x)) == 0)]\n",
    "print val_rest.shape, np.mean(val_rest.before == val_rest.after),val_elec.shape, np.mean(val_elec.after != val_elec.before.map(clean_elec))\n",
    "\n",
    "dev_elec = dev_rest[dev_rest.before.map(lambda x : len(rel.findall(x)) > 0)]\n",
    "dev_rest = dev_rest[dev_rest.before.map(lambda x : len(rel.findall(x)) == 0)]\n",
    "print dev_rest.shape, np.mean(dev_rest.before == dev_rest.after),dev_elec.shape, np.mean(dev_elec.after != dev_elec.before.map(clean_elec))\n",
    "\n",
    "\n",
    "rpn = re.compile(\"[A-Z][a-z]+\")\n",
    "val_proper_nouns = val_rest[val_rest.before.map(lambda x : len(rpn.findall(x)) > 0)]\n",
    "val_rest = val_rest[val_rest.before.map(lambda x : len(rpn.findall(x)) == 0)]\n",
    "print val_rest.shape, np.mean(val_rest.before == val_rest.after), val_proper_nouns.shape, np.mean(val_proper_nouns.before == val_proper_nouns.after)\n",
    "\n",
    "dev_proper_nouns = dev_rest[dev_rest.before.map(lambda x : len(rpn.findall(x)) > 0)]\n",
    "dev_rest = dev_rest[dev_rest.before.map(lambda x : len(rpn.findall(x)) == 0)]\n",
    "print dev_rest.shape, np.mean(dev_rest.before == dev_rest.after), dev_proper_nouns.shape, np.mean(dev_proper_nouns.before == dev_proper_nouns.after)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "183"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    chars_numeric = pickle.load(open(\"./chars_numeric.pkl\", 'rb'))\n",
    "except IOError:\n",
    "    chars_numeric = set(c for token in list(dev_data_numeric.before_phrase.map(lambda x : x.replace(\"<SEP>\",\"\"))) + \n",
    "                list(dev_data_numeric.after) for c in token)\n",
    "    chars_numeric = ['<PAD>','<EOS>','<GO>','<SELF>', '<SEP>'] + list(chars_numeric)\n",
    "    pickle.dump(chars_numeric, open(\"./chars_numeric.pkl\",\"wb\"))\n",
    "char_to_int_numeric = {c : i for i,c in enumerate(chars_numeric)}\n",
    "int_to_char_numeric = {i : c for i,c in enumerate(chars_numeric)}\n",
    "len(chars_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "178"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    chars_letters = pickle.load(open(\"./chars_letters.pkl\", 'rb'))\n",
    "except IOError:\n",
    "    chars_letters = set(c for token in list(dev_data_letters.before_phrase.map(lambda x : x.replace(\"<SEP>\",\"\"))) + \n",
    "                list(dev_data_letters.after) for c in token)\n",
    "    chars_letters = ['<PAD>','<EOS>','<GO>','<SELF>', '<SEP>'] + list(chars_letters)\n",
    "    pickle.dump(chars_letters, open(\"./chars_letters.pkl\",\"wb\"))\n",
    "char_to_int_letters = {c : i for i,c in enumerate(chars_letters)}\n",
    "int_to_char_letters = {i : c for i,c in enumerate(chars_letters)}\n",
    "len(set(chars_letters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    chars_rest = pickle.load(open(\"./chars_rest.pkl\", 'rb'))\n",
    "except IOError:\n",
    "    chars_rest = set(c for token in list(dev_rest.before_phrase.map(lambda x : x.replace(\"<SEP>\",\"\"))) + \n",
    "                list(dev_rest.after) for c in token)\n",
    "    chars_rest = ['<PAD>','<EOS>','<GO>','<SELF>', '<SEP>'] + list(chars_rest)\n",
    "    pickle.dump(chars_rest, open(\"./chars_rest.pkl\",\"wb\"))\n",
    "char_to_int_rest = {c : i for i,c in enumerate(chars_rest)}\n",
    "int_to_char_rest = {i : c for i,c in enumerate(chars_rest)}\n",
    "len(chars_rest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "188"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    chars_proper_nouns = pickle.load(open(\"./chars_proper_nouns.pkl\", 'rb'))\n",
    "except IOError:\n",
    "    chars_proper_nouns = set(c for token in list(dev_proper_nouns.before_phrase.map(lambda x : x.replace(\"<SEP>\",\"\"))) + \n",
    "                list(dev_proper_nouns.after) for c in token)\n",
    "    chars_proper_nouns = ['<PAD>','<EOS>','<GO>','<SELF>', '<SEP>'] + list(chars_proper_nouns)\n",
    "    pickle.dump(chars_proper_nouns, open(\"./chars_proper_nouns.pkl\",\"wb\"))\n",
    "char_to_int_proper_nouns = {c : i for i,c in enumerate(chars_proper_nouns)}\n",
    "int_to_char_proper_nouns = {i : c for i,c in enumerate(chars_proper_nouns)}\n",
    "len(chars_proper_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "325360 325360 84776 84776\n",
      "130746 130746 33434 33434\n",
      "261277 261277 66917 66917\n",
      "261277 261277 66917 66917\n"
     ]
    }
   ],
   "source": [
    "def get_char_to_int_numeric(c):\n",
    "    try:\n",
    "        return char_to_int_numeric[c]\n",
    "    except KeyError:\n",
    "        return char_to_int_numeric[\" \"]\n",
    "def get_char_to_int_letters(c):\n",
    "    try:\n",
    "        return char_to_int_letters[c]\n",
    "    except KeyError:\n",
    "        return char_to_int_letters[\" \"]\n",
    "\n",
    "def get_char_to_int_proper_nouns(c):\n",
    "    try:\n",
    "        return char_to_int_proper_nouns[c]\n",
    "    except KeyError:\n",
    "        return char_to_int_proper_nouns[\" \"]\n",
    "\n",
    "def get_char_to_int_rest(c):\n",
    "    try:\n",
    "        return char_to_int_rest[c]\n",
    "    except KeyError:\n",
    "        return char_to_int_rest[\" \"]\n",
    "    \n",
    "int_before_dev_numeric = []\n",
    "int_after_dev_numeric = []\n",
    "int_before_val_numeric = []\n",
    "int_after_val_numeric = []\n",
    "\n",
    "int_before_dev_letters = []\n",
    "int_after_dev_letters = []\n",
    "int_before_val_letters = []\n",
    "int_after_val_letters = []\n",
    "\n",
    "int_before_dev_rest = []\n",
    "int_after_dev_rest = []\n",
    "int_before_val_rest = []\n",
    "int_after_val_rest = []\n",
    "\n",
    "int_before_dev_proper_nouns = []\n",
    "int_after_dev_proper_nouns = []\n",
    "int_before_val_proper_nouns = []\n",
    "int_after_val_proper_nouns = []\n",
    "\n",
    "dev_words_numeric = dev_data_numeric[['before_phrase', 'after']].drop_duplicates()\n",
    "for before, after in zip(dev_words_numeric.before_phrase, dev_words_numeric.after):\n",
    "    words = before.split(\"<SEP>\")\n",
    "    if words[2] == after:\n",
    "        int_before_dev_numeric.append([get_char_to_int_numeric(c) for c in words[0]] + [char_to_int_numeric['<SEP>']] \n",
    "                                    + [get_char_to_int_numeric(c) for c in words[1]] + [char_to_int_numeric['<SEP>']] \n",
    "                                    + [get_char_to_int_numeric(c) for c in words[2]] + [char_to_int_numeric['<SEP>']] \n",
    "                                    + [get_char_to_int_numeric(c) for c in words[3]] + [char_to_int_numeric['<SEP>']] \n",
    "                                    + [get_char_to_int_numeric(c) for c in words[4]])\n",
    "        int_after_dev_numeric.append([get_char_to_int_numeric('<SELF>')])\n",
    "    else:\n",
    "        int_before_dev_numeric.append([get_char_to_int_numeric(c) for c in words[0]] + [char_to_int_numeric['<SEP>']] \n",
    "                                    + [get_char_to_int_numeric(c) for c in words[1]] + [char_to_int_numeric['<SEP>']] \n",
    "                                    + [get_char_to_int_numeric(c) for c in words[2]] + [char_to_int_numeric['<SEP>']] \n",
    "                                    + [get_char_to_int_numeric(c) for c in words[3]] + [char_to_int_numeric['<SEP>']] \n",
    "                                    + [get_char_to_int_numeric(c) for c in words[4]])\n",
    "        int_after_dev_numeric.append([get_char_to_int_numeric(c) for c in after])\n",
    "\n",
    "        \n",
    "val_words_numeric = val_data_numeric[['before_phrase', 'after']].drop_duplicates()\n",
    "for before, after in zip(val_words_numeric.before_phrase, val_words_numeric.after):\n",
    "    words = before.split(\"<SEP>\")\n",
    "    if words[2] == after:\n",
    "        int_before_val_numeric.append([get_char_to_int_numeric(c) for c in words[0]] + [char_to_int_numeric['<SEP>']] \n",
    "                                    + [get_char_to_int_numeric(c) for c in words[1]] + [char_to_int_numeric['<SEP>']] \n",
    "                                    + [get_char_to_int_numeric(c) for c in words[2]] + [char_to_int_numeric['<SEP>']] \n",
    "                                    + [get_char_to_int_numeric(c) for c in words[3]] + [char_to_int_numeric['<SEP>']] \n",
    "                                    + [get_char_to_int_numeric(c) for c in words[4]])\n",
    "        int_after_val_numeric.append([get_char_to_int_numeric('<SELF>')])\n",
    "    else:\n",
    "        int_before_val_numeric.append([get_char_to_int_numeric(c) for c in words[0]] + [char_to_int_numeric['<SEP>']] \n",
    "                                    + [get_char_to_int_numeric(c) for c in words[1]] + [char_to_int_numeric['<SEP>']] \n",
    "                                    + [get_char_to_int_numeric(c) for c in words[2]] + [char_to_int_numeric['<SEP>']] \n",
    "                                    + [get_char_to_int_numeric(c) for c in words[3]] + [char_to_int_numeric['<SEP>']] \n",
    "                                    + [get_char_to_int_numeric(c) for c in words[4]])\n",
    "        int_after_val_numeric.append([get_char_to_int_numeric(c) for c in after])\n",
    "\n",
    "dev_words_letters = dev_data_letters[['before_phrase', 'after']].drop_duplicates()\n",
    "for before, after in zip(dev_words_letters.before_phrase, dev_words_letters.after):\n",
    "    words = before.split(\"<SEP>\")\n",
    "    if words[2] == after:\n",
    "        int_before_dev_letters.append([get_char_to_int_letters(c) for c in words[0]] + [char_to_int_letters['<SEP>']] \n",
    "                                    + [get_char_to_int_letters(c) for c in words[1]] + [char_to_int_letters['<SEP>']] \n",
    "                                    + [get_char_to_int_letters(c) for c in words[2]] + [char_to_int_letters['<SEP>']] \n",
    "                                    + [get_char_to_int_letters(c) for c in words[3]] + [char_to_int_letters['<SEP>']] \n",
    "                                    + [get_char_to_int_letters(c) for c in words[4]])\n",
    "        int_after_dev_letters.append([get_char_to_int_letters('<SELF>')])\n",
    "    else:\n",
    "        int_before_dev_letters.append([get_char_to_int_letters(c) for c in words[0]] + [char_to_int_letters['<SEP>']] \n",
    "                                    + [get_char_to_int_letters(c) for c in words[1]] + [char_to_int_letters['<SEP>']] \n",
    "                                    + [get_char_to_int_letters(c) for c in words[2]] + [char_to_int_letters['<SEP>']] \n",
    "                                    + [get_char_to_int_letters(c) for c in words[3]] + [char_to_int_letters['<SEP>']] \n",
    "                                    + [get_char_to_int_letters(c) for c in words[4]])\n",
    "        int_after_dev_letters.append([get_char_to_int_letters(c) for c in after])\n",
    "\n",
    "        \n",
    "val_words_letters = val_data_letters[['before_phrase', 'after']].drop_duplicates()\n",
    "for before, after in zip(val_words_letters.before_phrase, val_words_letters.after):\n",
    "    words = before.split(\"<SEP>\")\n",
    "    if words[2] == after:\n",
    "        int_before_val_letters.append([get_char_to_int_letters(c) for c in words[0]] + [char_to_int_letters['<SEP>']] \n",
    "                                    + [get_char_to_int_letters(c) for c in words[1]] + [char_to_int_letters['<SEP>']] \n",
    "                                    + [get_char_to_int_letters(c) for c in words[2]] + [char_to_int_letters['<SEP>']] \n",
    "                                    + [get_char_to_int_letters(c) for c in words[3]] + [char_to_int_letters['<SEP>']] \n",
    "                                    + [get_char_to_int_letters(c) for c in words[4]])\n",
    "        int_after_val_letters.append([get_char_to_int_letters('<SELF>')])\n",
    "    else:\n",
    "        int_before_val_letters.append([get_char_to_int_letters(c) for c in words[0]] + [char_to_int_letters['<SEP>']] \n",
    "                                    + [get_char_to_int_letters(c) for c in words[1]] + [char_to_int_letters['<SEP>']] \n",
    "                                    + [get_char_to_int_letters(c) for c in words[2]] + [char_to_int_letters['<SEP>']] \n",
    "                                    + [get_char_to_int_letters(c) for c in words[3]] + [char_to_int_letters['<SEP>']] \n",
    "                                    + [get_char_to_int_letters(c) for c in words[4]])\n",
    "        int_after_val_letters.append([get_char_to_int_letters(c) for c in after])\n",
    "\n",
    "dev_words_rest = dev_rest[['before_phrase', 'after']].drop_duplicates()\n",
    "for before, after in zip(dev_words_rest.before_phrase, dev_words_rest.after):\n",
    "    words = before.split(\"<SEP>\")\n",
    "    if words[2] == after:\n",
    "        int_before_dev_rest.append([get_char_to_int_rest(c) for c in words[0]] + [char_to_int_rest['<SEP>']] \n",
    "                                    + [get_char_to_int_rest(c) for c in words[1]] + [char_to_int_rest['<SEP>']] \n",
    "                                    + [get_char_to_int_rest(c) for c in words[2]] + [char_to_int_rest['<SEP>']] \n",
    "                                    + [get_char_to_int_rest(c) for c in words[3]] + [char_to_int_rest['<SEP>']] \n",
    "                                    + [get_char_to_int_rest(c) for c in words[4]])\n",
    "        int_after_dev_rest.append([get_char_to_int_rest('<SELF>')])\n",
    "    else:\n",
    "        int_before_dev_rest.append([get_char_to_int_rest(c) for c in words[0]] + [char_to_int_rest['<SEP>']] \n",
    "                                    + [get_char_to_int_rest(c) for c in words[1]] + [char_to_int_rest['<SEP>']] \n",
    "                                    + [get_char_to_int_rest(c) for c in words[2]] + [char_to_int_rest['<SEP>']] \n",
    "                                    + [get_char_to_int_rest(c) for c in words[3]] + [char_to_int_rest['<SEP>']] \n",
    "                                    + [get_char_to_int_rest(c) for c in words[4]])\n",
    "        int_after_dev_rest.append([get_char_to_int_rest(c) for c in after])\n",
    "\n",
    "        \n",
    "val_words_rest = val_rest[['before_phrase', 'after']].drop_duplicates()\n",
    "for before, after in zip(val_words_rest.before_phrase, val_words_rest.after):\n",
    "    words = before.split(\"<SEP>\")\n",
    "    if words[2] == after:\n",
    "        int_before_val_rest.append([get_char_to_int_rest(c) for c in words[0]] + [char_to_int_rest['<SEP>']] \n",
    "                                    + [get_char_to_int_rest(c) for c in words[1]] + [char_to_int_rest['<SEP>']] \n",
    "                                    + [get_char_to_int_rest(c) for c in words[2]] + [char_to_int_rest['<SEP>']] \n",
    "                                    + [get_char_to_int_rest(c) for c in words[3]] + [char_to_int_rest['<SEP>']] \n",
    "                                    + [get_char_to_int_rest(c) for c in words[4]])\n",
    "        int_after_val_rest.append([get_char_to_int_rest('<SELF>')])\n",
    "    else:\n",
    "        int_before_val_rest.append([get_char_to_int_rest(c) for c in words[0]] + [char_to_int_rest['<SEP>']] \n",
    "                                    + [get_char_to_int_rest(c) for c in words[1]] + [char_to_int_rest['<SEP>']] \n",
    "                                    + [get_char_to_int_rest(c) for c in words[2]] + [char_to_int_rest['<SEP>']] \n",
    "                                    + [get_char_to_int_rest(c) for c in words[3]] + [char_to_int_rest['<SEP>']] \n",
    "                                    + [get_char_to_int_rest(c) for c in words[4]])\n",
    "        int_after_val_rest.append([get_char_to_int_rest(c) for c in after])\n",
    "        \n",
    "dev_words_proper_nouns = dev_proper_nouns[['before_phrase', 'after']].drop_duplicates()\n",
    "for before, after in zip(dev_words_proper_nouns.before_phrase, dev_words_proper_nouns.after):\n",
    "    words = before.split(\"<SEP>\")\n",
    "    if words[2] == after:\n",
    "        int_before_dev_proper_nouns.append([get_char_to_int_proper_nouns(c) for c in words[0]] + [char_to_int_proper_nouns['<SEP>']] \n",
    "                                    + [get_char_to_int_proper_nouns(c) for c in words[1]] + [char_to_int_proper_nouns['<SEP>']] \n",
    "                                    + [get_char_to_int_proper_nouns(c) for c in words[2]] + [char_to_int_proper_nouns['<SEP>']] \n",
    "                                    + [get_char_to_int_proper_nouns(c) for c in words[3]] + [char_to_int_proper_nouns['<SEP>']] \n",
    "                                    + [get_char_to_int_proper_nouns(c) for c in words[4]])\n",
    "        int_after_dev_proper_nouns.append([get_char_to_int_proper_nouns('<SELF>')])\n",
    "    else:\n",
    "        int_before_dev_proper_nouns.append([get_char_to_int_proper_nouns(c) for c in words[0]] + [char_to_int_proper_nouns['<SEP>']] \n",
    "                                    + [get_char_to_int_proper_nouns(c) for c in words[1]] + [char_to_int_proper_nouns['<SEP>']] \n",
    "                                    + [get_char_to_int_proper_nouns(c) for c in words[2]] + [char_to_int_proper_nouns['<SEP>']] \n",
    "                                    + [get_char_to_int_proper_nouns(c) for c in words[3]] + [char_to_int_proper_nouns['<SEP>']] \n",
    "                                    + [get_char_to_int_proper_nouns(c) for c in words[4]])\n",
    "        int_after_dev_proper_nouns.append([get_char_to_int_proper_nouns(c) for c in after])\n",
    "\n",
    "        \n",
    "val_words_proper_nouns = val_proper_nouns[['before_phrase', 'after']].drop_duplicates()\n",
    "for before, after in zip(val_words_proper_nouns.before_phrase, val_words_proper_nouns.after):\n",
    "    words = before.split(\"<SEP>\")\n",
    "    if words[2] == after:\n",
    "        int_before_val_proper_nouns.append([get_char_to_int_proper_nouns(c) for c in words[0]] + [char_to_int_proper_nouns['<SEP>']] \n",
    "                                    + [get_char_to_int_proper_nouns(c) for c in words[1]] + [char_to_int_proper_nouns['<SEP>']] \n",
    "                                    + [get_char_to_int_proper_nouns(c) for c in words[2]] + [char_to_int_proper_nouns['<SEP>']] \n",
    "                                    + [get_char_to_int_proper_nouns(c) for c in words[3]] + [char_to_int_proper_nouns['<SEP>']] \n",
    "                                    + [get_char_to_int_proper_nouns(c) for c in words[4]])\n",
    "        int_after_val_proper_nouns.append([get_char_to_int_proper_nouns('<SELF>')])\n",
    "    else:\n",
    "        int_before_val_proper_nouns.append([get_char_to_int_proper_nouns(c) for c in words[0]] + [char_to_int_proper_nouns['<SEP>']] \n",
    "                                    + [get_char_to_int_proper_nouns(c) for c in words[1]] + [char_to_int_proper_nouns['<SEP>']] \n",
    "                                    + [get_char_to_int_proper_nouns(c) for c in words[2]] + [char_to_int_proper_nouns['<SEP>']] \n",
    "                                    + [get_char_to_int_proper_nouns(c) for c in words[3]] + [char_to_int_proper_nouns['<SEP>']] \n",
    "                                    + [get_char_to_int_proper_nouns(c) for c in words[4]])\n",
    "        int_after_val_proper_nouns.append([get_char_to_int_proper_nouns(c) for c in after])\n",
    "        \n",
    "print len(int_before_dev_numeric), len(int_after_dev_numeric), len(int_before_val_numeric), len(int_after_val_numeric)\n",
    "print len(int_before_dev_letters), len(int_after_dev_letters), len(int_before_val_letters), len(int_after_val_letters)\n",
    "print len(int_before_dev_rest), len(int_after_dev_rest), len(int_before_val_rest), len(int_after_val_rest)\n",
    "print len(int_before_dev_rest), len(int_after_dev_rest), len(int_before_val_rest), len(int_after_val_rest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_inputs():\n",
    "    with tf.name_scope('inputs'):\n",
    "        inputs = tf.placeholder(tf.int32, [None, None], name='inputs')\n",
    "    with tf.name_scope('targets'):\n",
    "        targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
    "    keep_prob = tf.placeholder(tf.float32, name = 'keep_prob')\n",
    "    inputs_length = tf.placeholder(tf.int32, (None,), name = 'inputs_length')\n",
    "    targets_length = tf.placeholder(tf.int32, (None,), name = 'targets_length')\n",
    "    max_targets_length = tf.reduce_max(targets_length, name = 'max_target_len')\n",
    "    \n",
    "    return inputs, targets, keep_prob, inputs_length, targets_length, max_targets_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_encoding_input(targets, vocab_to_int, batch_size):\n",
    "    with tf.name_scope(\"process_encoding\"):\n",
    "        ending = tf.strided_slice(targets, [0,0], [batch_size, - 1], [1,1])\n",
    "        dec_input = tf.concat([tf.fill([batch_size, 1], vocab_to_int['<GO>']), ending],1)\n",
    "    return dec_input\n",
    "\n",
    "def encoding_layer(rnn_size, sequence_length, num_layers, rnn_inputs, keep_prob, direction):\n",
    "    if direction == 1:\n",
    "        with tf.name_scope(\"RNN_Encoder_Cell_1D\"):\n",
    "            for layer in range(num_layers):\n",
    "                with tf.variable_scope('encoder_{}'.format(layer)):\n",
    "                    lstm = tf.contrib.rnn.LSTMCell(rnn_size)\n",
    "                    drop = tf.contrib.rnn.DropoutWrapper(lstm, input_keep_prob = keep_prob)\n",
    "                    enc_output, enc_state = tf.nn.dynamic_rnn(drop,rnn_inputs,sequence_length, dtype=tf.float32)\n",
    "            return enc_output, enc_state\n",
    "    if direction ==2:\n",
    "        with tf.name_scope(\"RNN_Encoder_Cell_2D\"):\n",
    "            for layer in range(num_layers):\n",
    "                with tf.variable_scope('encoder_{}'.format(layer)):\n",
    "                    cell_fw = tf.contrib.rnn.LSTMCell(rnn_size)\n",
    "                    cell_fw = tf.contrib.rnn.DropoutWrapper(cell_fw, input_keep_prob = keep_prob)\n",
    "                    cell_bw = tf.contrib.rnn.LSTMCell(rnn_size)\n",
    "                    cell_bw = tf.contrib.rnn.DropoutWrapper(cell_bw, input_keep_prob = keep_prob)\n",
    "                    \n",
    "                    enc_output, enc_state = tf.nn.bidirectional_dynamic_rnn(cell_fw, cell_bw,\n",
    "                                                                            rnn_inputs,sequence_length, \n",
    "                                                                            dtype=tf.float32)\n",
    "            enc_output = tf.concat(enc_output,2)\n",
    "            return enc_output, enc_state[0]\n",
    "        \n",
    "def training_decoding_layer(dec_embed_input, targets_length, dec_cell, initial_state, output_layer,vocab_size,\n",
    "                           max_target_length):\n",
    "    with tf.name_scope(\"Training_Decoder\"):\n",
    "        training_helper = tf.contrib.seq2seq.TrainingHelper(inputs = dec_embed_input, sequence_length = targets_length,\n",
    "                                                           time_major = False)\n",
    "        training_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell, training_helper, initial_state, output_layer)\n",
    "        training_logits, _ = tf.contrib.seq2seq.dynamic_decode(training_decoder, output_time_major = False,\n",
    "                                                              impute_finished = True,\n",
    "                                                              maximum_iterations = max_target_length)\n",
    "        return training_logits\n",
    "\n",
    "def inference_decoding_layer(embeddings, start_token, end_token, dec_cell, initial_state, output_layer, \n",
    "                             max_target_length, batch_size):\n",
    "    with tf.name_scope(\"Inference_Decoder\"):\n",
    "        start_tokens = tf.tile(tf.constant([start_token], dtype=tf.int32), [batch_size], name='start_tokens')\n",
    "        inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embeddings, start_tokens, end_token)\n",
    "        inference_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell, inference_helper, initial_state, output_layer)\n",
    "        inference_logits, _ = tf.contrib.seq2seq.dynamic_decode(inference_decoder, output_time_major=False,\n",
    "                                                               impute_finished = True,\n",
    "                                                               maximum_iterations = max_target_length)\n",
    "        return inference_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decoding_layer(dec_embed_input, embeddings, enc_output, enc_state, vocab_size, inputs_length, targets_length, \n",
    "                   max_target_length, rnn_size, vocab_to_int, keep_prob, batch_size, num_layers, direction):\n",
    "    '''Create the decoding cell and attention for the training and inference decoding layers'''\n",
    "    \n",
    "    with tf.name_scope(\"RNN_Decoder_Cell\"):\n",
    "        for layer in range(num_layers):\n",
    "            with tf.variable_scope('decoder_{}'.format(layer)):\n",
    "                lstm = tf.contrib.rnn.LSTMCell(rnn_size)\n",
    "                dec_cell = tf.contrib.rnn.DropoutWrapper(lstm, input_keep_prob = keep_prob)\n",
    "    \n",
    "    output_layer = Dense(vocab_size,kernel_initializer = tf.truncated_normal_initializer(mean = 0.0, stddev=0.1))\n",
    "    \n",
    "    attn_mech = tf.contrib.seq2seq.BahdanauAttention(rnn_size, enc_output,inputs_length,normalize=False,\n",
    "                                                                                  name='BahdanauAttention')\n",
    "    \n",
    "    with tf.name_scope(\"Attention_Wrapper\"):\n",
    "        dec_cell = tf.contrib.seq2seq.DynamicAttentionWrapper(dec_cell,attn_mech, rnn_size)\n",
    "    \n",
    "    initial_state = tf.contrib.seq2seq.DynamicAttentionWrapperState(enc_state,_zero_state_tensors(rnn_size, \n",
    "                                                                                        batch_size, \n",
    "                                                                                        tf.float32))\n",
    "\n",
    "    with tf.variable_scope(\"decode\"):\n",
    "        training_logits = training_decoding_layer(dec_embed_input, targets_length, dec_cell, initial_state,\n",
    "                                                  output_layer,vocab_size, max_target_length)\n",
    "    with tf.variable_scope(\"decode\", reuse=True):\n",
    "        inference_logits = inference_decoding_layer(embeddings, vocab_to_int['<GO>'], vocab_to_int['<EOS>'],\n",
    "                                                    dec_cell, initial_state, output_layer,\n",
    "                                                    max_target_length,batch_size)\n",
    "    return training_logits, inference_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def seq2seq_model(inputs, targets, keep_prob, inputs_length, targets_length, max_target_length, \n",
    "                  vocab_size, rnn_size, num_layers, vocab_to_int, batch_size, embedding_size, direction):\n",
    "    '''Use the previous functions to create the training and inference logits'''\n",
    "    \n",
    "    enc_embeddings = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1, 1))\n",
    "    enc_embed_input = tf.nn.embedding_lookup(enc_embeddings, inputs)\n",
    "    enc_output, enc_state = encoding_layer(rnn_size, inputs_length, num_layers, \n",
    "                                           enc_embed_input, keep_prob, direction)\n",
    "    \n",
    "    dec_embeddings = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1, 1))\n",
    "    dec_input = process_encoding_input(targets, vocab_to_int, batch_size)\n",
    "    dec_embed_input = tf.nn.embedding_lookup(dec_embeddings, dec_input)\n",
    "    \n",
    "    training_logits, inference_logits  = decoding_layer(dec_embed_input, dec_embeddings,enc_output,enc_state, \n",
    "                                                        vocab_size, inputs_length, targets_length, max_target_length,\n",
    "                                                        rnn_size, vocab_to_int, keep_prob, batch_size,num_layers,\n",
    "                                                        direction) \n",
    "    return training_logits, inference_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pad_sentence_batch(sentence_batch, char_to_int):\n",
    "    \"\"\"Pad sentences with <PAD> so that each sentence of a batch has the same length\"\"\"\n",
    "    max_sentence = max([len(sentence) for sentence in sentence_batch])\n",
    "    return [sentence + [char_to_int['<PAD>']] * (max_sentence - len(sentence)) for sentence in sentence_batch]\n",
    "def get_batches(before_tokens, after_tokens, batch_size, char_to_int): \n",
    "    for batch_i in range(0, len(before_tokens)//batch_size):\n",
    "        start_i = batch_i * batch_size\n",
    "        before_tokens_batch = before_tokens[start_i:start_i + batch_size]\n",
    "        after_tokens_batch = after_tokens[start_i:start_i + batch_size]\n",
    "            \n",
    "        after_tokens_batch_eos = []\n",
    "        for token in after_tokens_batch:\n",
    "            token.append(char_to_int['<EOS>'])\n",
    "            after_tokens_batch_eos.append(token)\n",
    "            \n",
    "        pad_after_tokens_batch = np.array(pad_sentence_batch(after_tokens_batch_eos, char_to_int))\n",
    "        pad_before_tokens_batch = np.array(pad_sentence_batch(before_tokens_batch, char_to_int))\n",
    "        \n",
    "        # Need the lengths for the _lengths parameters\n",
    "        pad_before_tokens_lengths = []\n",
    "        for token in pad_before_tokens_batch:\n",
    "            pad_before_tokens_lengths.append(len(token))\n",
    "        \n",
    "        pad_after_tokens_lengths = []\n",
    "        for token in pad_after_tokens_batch:\n",
    "            pad_after_tokens_lengths.append(len(token))\n",
    "        \n",
    "        yield pad_before_tokens_batch, pad_after_tokens_batch, pad_before_tokens_lengths, pad_after_tokens_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_graph(keep_prob, rnn_size, num_layers, batch_size, learning_rate, embedding_size, direction, char_to_int):\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    # Load the model inputs    \n",
    "    inputs, targets, keep_prob, inputs_length, targets_length, max_target_length = model_inputs()\n",
    "\n",
    "    # Create the training and inference logits\n",
    "    training_logits, inference_logits = seq2seq_model(tf.reverse(inputs, [-1]),\n",
    "                                                      targets, \n",
    "                                                      keep_prob,   \n",
    "                                                      inputs_length,\n",
    "                                                      targets_length,\n",
    "                                                      max_target_length,\n",
    "                                                      len(char_to_int)+1,\n",
    "                                                      rnn_size, \n",
    "                                                      num_layers, \n",
    "                                                      char_to_int,\n",
    "                                                      batch_size,\n",
    "                                                      embedding_size,\n",
    "                                                      direction)\n",
    "\n",
    "    # Create tensors for the training logits and inference logits\n",
    "    training_logits = tf.identity(training_logits.rnn_output, 'logits')\n",
    "\n",
    "    with tf.name_scope('predictions'):\n",
    "        predictions = tf.identity(inference_logits.sample_id, name='predictions')\n",
    "        tf.summary.histogram('predictions', predictions)\n",
    "\n",
    "    # Create the weights for sequence_loss\n",
    "    masks = tf.sequence_mask(targets_length, max_target_length, dtype=tf.float32, name='masks')\n",
    "    \n",
    "    with tf.name_scope(\"cost\"):\n",
    "        # Loss function\n",
    "        cost = tf.contrib.seq2seq.sequence_loss(training_logits, \n",
    "                                                targets, \n",
    "                                                masks)\n",
    "        tf.summary.scalar('cost', cost)\n",
    "\n",
    "    with tf.name_scope(\"optimze\"):\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "        # Gradient Clipping\n",
    "        gradients = optimizer.compute_gradients(cost)\n",
    "        capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]\n",
    "        train_op = optimizer.apply_gradients(capped_gradients)\n",
    "\n",
    "    # Merge all of the summaries\n",
    "    merged = tf.summary.merge_all()    \n",
    "\n",
    "    # Export the nodes \n",
    "    export_nodes = ['inputs', 'targets', 'keep_prob', 'cost', 'inputs_length', 'targets_length',\n",
    "                    'predictions', 'merged', 'train_op','optimizer']\n",
    "    Graph = namedtuple('Graph', export_nodes)\n",
    "    local_dict = locals()\n",
    "    graph = Graph(*[local_dict[each] for each in export_nodes])\n",
    "\n",
    "    return graph\n",
    "\n",
    "\n",
    "# ## Training the Model\n",
    "\n",
    "# In[74]:\n",
    "\n",
    "def train(model, epochs, log_string, int_before_train, int_after_train, int_before_test, int_after_test, char_to_int,\n",
    "         reset = True, early_stopping = 3):\n",
    "    '''Train the RNN'''\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        if reset:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "        else:\n",
    "            try:\n",
    "                checkpoint = \"./{}.ckpt\".format(log_string)\n",
    "                saver = tf.train.Saver()\n",
    "                saver.restore(sess, checkpoint)\n",
    "            except tf.errors.NotFoundError:\n",
    "                print \"checkpoint not found...\"\n",
    "                print \"initiating new model...\"\n",
    "                sess.run(tf.global_variables_initializer())\n",
    "        # Used to determine when to stop the training early\n",
    "        testing_loss_summary = []\n",
    "\n",
    "        # Keep track of which batch iteration is being trained\n",
    "        iteration = 0\n",
    "        \n",
    "        display_step = 50 # The progress of the training will be displayed after every 30 batches\n",
    "        stop_early = 0 \n",
    "        stop = early_stopping # If the batch_loss_testing does not decrease in 3 consecutive checks, stop training\n",
    "        per_epoch = 1 # Test the model 3 times per epoch\n",
    "        testing_check = (len(int_before_train)//batch_size//per_epoch)-1\n",
    "\n",
    "        print()\n",
    "        print(\"Training Model: {}\".format(log_string))\n",
    "\n",
    "        train_writer = tf.summary.FileWriter('./logs/1/train/{}'.format(log_string), sess.graph)\n",
    "        test_writer = tf.summary.FileWriter('./logs/1/test/{}'.format(log_string))\n",
    "\n",
    "        for epoch_i in range(1, epochs+1): \n",
    "            batch_loss = 0\n",
    "            batch_time = 0\n",
    "            \n",
    "            for batch_i, (input_batch, target_batch, input_length, target_length) in enumerate(\n",
    "                    get_batches(int_before_train, int_after_train, batch_size, char_to_int)):\n",
    "                start_time = time.time()\n",
    "\n",
    "                summary, loss, _ = sess.run([model.merged,\n",
    "                                             model.cost, \n",
    "                                             model.train_op], \n",
    "                                             {model.inputs: input_batch,\n",
    "                                              model.targets: target_batch,\n",
    "                                              model.inputs_length: input_length,\n",
    "                                              model.targets_length: target_length,\n",
    "                                              model.keep_prob: keep_probability})\n",
    "\n",
    "\n",
    "                batch_loss += loss\n",
    "                end_time = time.time()\n",
    "                batch_time += end_time - start_time\n",
    "#                 sys.stdout.write(str(batch_i) + \".\")\n",
    "                # Record the progress of training\n",
    "                train_writer.add_summary(summary, iteration)\n",
    "\n",
    "                iteration += 1\n",
    "\n",
    "                if batch_i % display_step == 0 and batch_i > 0:\n",
    "                    print('Epoch {:>3}/{} Batch {:>4}/{} - Loss: {:>6.3f}, Seconds: {:>4.2f}'\n",
    "                          .format(epoch_i,\n",
    "                                  epochs, \n",
    "                                  batch_i, \n",
    "                                  len(int_before_train) // batch_size, \n",
    "                                  batch_loss / display_step, \n",
    "                                  batch_time))\n",
    "                    batch_loss = 0\n",
    "                    batch_time = 0\n",
    "\n",
    "                #### Testing ####\n",
    "                if batch_i % testing_check == 0 and batch_i > 0:\n",
    "                    batch_loss_testing = 0\n",
    "                    batch_time_testing = 0\n",
    "                    for batch_i, (input_batch, target_batch, input_length, target_length) in enumerate(\n",
    "                            get_batches(int_before_test, int_after_test, batch_size,char_to_int)):\n",
    "                        start_time_testing = time.time()\n",
    "                        summary, loss = sess.run([model.merged,\n",
    "                                                  model.cost], \n",
    "                                                     {model.inputs: input_batch,\n",
    "                                                      model.targets: target_batch,\n",
    "                                                      model.inputs_length: input_length,\n",
    "                                                      model.targets_length: target_length,\n",
    "                                                      model.keep_prob: 1})\n",
    "\n",
    "                        batch_loss_testing += loss\n",
    "                        end_time_testing = time.time()\n",
    "                        batch_time_testing += end_time_testing - start_time_testing\n",
    "\n",
    "                        # Record the progress of testing\n",
    "                        test_writer.add_summary(summary, iteration)\n",
    "\n",
    "                    n_batches_testing = batch_i + 1\n",
    "                    print('Testing Loss: {:>6.3f}, Seconds: {:>4.2f}'\n",
    "                          .format(batch_loss_testing / n_batches_testing, \n",
    "                                  batch_time_testing))\n",
    "                    \n",
    "                    batch_time_testing = 0\n",
    "\n",
    "                    # If the batch_loss_testing is at a new minimum, save the model\n",
    "                    testing_loss_summary.append(batch_loss_testing)\n",
    "                    if batch_loss_testing <= min(testing_loss_summary):\n",
    "                        print('New Record!') \n",
    "                        stop_early = 0\n",
    "                        checkpoint = \"./{}.ckpt\".format(log_string)\n",
    "                        saver = tf.train.Saver()\n",
    "                        saver.save(sess, checkpoint)\n",
    "\n",
    "                    else:\n",
    "                        print(\"No Improvement.\")\n",
    "                        stop_early += 1\n",
    "                        if stop_early == stop:\n",
    "                            break\n",
    "            sys.stdout.flush()\n",
    "            if stop_early == stop:\n",
    "                print(\"Stopping Training.\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./all_numeric_model_1.ckpt\n",
      "()\n",
      "Training Model: all_numeric_model_1\n",
      "Epoch   1/100 Batch   50/650 - Loss:  0.007, Seconds: 215.52\n",
      "Epoch   1/100 Batch  100/650 - Loss:  0.001, Seconds: 230.97\n",
      "Epoch   1/100 Batch  150/650 - Loss:  0.001, Seconds: 221.05\n",
      "Epoch   1/100 Batch  200/650 - Loss:  0.001, Seconds: 228.79\n",
      "Epoch   1/100 Batch  250/650 - Loss:  0.001, Seconds: 219.00\n",
      "Epoch   1/100 Batch  300/650 - Loss:  0.001, Seconds: 243.66\n",
      "Epoch   1/100 Batch  350/650 - Loss:  0.001, Seconds: 223.90\n",
      "Epoch   1/100 Batch  400/650 - Loss:  0.001, Seconds: 234.34\n",
      "Epoch   1/100 Batch  450/650 - Loss:  0.001, Seconds: 230.42\n",
      "Epoch   1/100 Batch  500/650 - Loss:  0.001, Seconds: 218.54\n",
      "Epoch   1/100 Batch  550/650 - Loss:  0.001, Seconds: 242.34\n",
      "Epoch   1/100 Batch  600/650 - Loss:  0.001, Seconds: 240.95\n",
      "Testing Loss:  0.001, Seconds: 371.77\n",
      "New Record!\n",
      "Epoch   2/100 Batch   50/650 - Loss:  0.013, Seconds: 218.14\n",
      "Epoch   2/100 Batch  100/650 - Loss:  0.001, Seconds: 232.65\n",
      "Epoch   2/100 Batch  150/650 - Loss:  0.001, Seconds: 222.75\n",
      "Epoch   2/100 Batch  200/650 - Loss:  0.001, Seconds: 231.31\n",
      "Epoch   2/100 Batch  250/650 - Loss:  0.001, Seconds: 221.18\n",
      "Epoch   2/100 Batch  300/650 - Loss:  0.001, Seconds: 245.93\n",
      "Epoch   2/100 Batch  350/650 - Loss:  0.001, Seconds: 225.42\n",
      "Epoch   2/100 Batch  400/650 - Loss:  0.001, Seconds: 235.69\n",
      "Epoch   2/100 Batch  450/650 - Loss:  0.001, Seconds: 232.76\n",
      "Epoch   2/100 Batch  500/650 - Loss:  0.001, Seconds: 219.99\n",
      "Epoch   2/100 Batch  550/650 - Loss:  0.001, Seconds: 242.92\n",
      "Epoch   2/100 Batch  600/650 - Loss:  0.001, Seconds: 243.19\n",
      "Testing Loss:  0.001, Seconds: 373.65\n",
      "New Record!\n",
      "Epoch   3/100 Batch   50/650 - Loss:  0.008, Seconds: 220.11\n",
      "Epoch   3/100 Batch  100/650 - Loss:  0.001, Seconds: 234.76\n",
      "Epoch   3/100 Batch  150/650 - Loss:  0.001, Seconds: 224.30\n",
      "Epoch   3/100 Batch  200/650 - Loss:  0.001, Seconds: 232.63\n",
      "Epoch   3/100 Batch  250/650 - Loss:  0.001, Seconds: 223.32\n",
      "Epoch   3/100 Batch  300/650 - Loss:  0.001, Seconds: 248.16\n",
      "Epoch   3/100 Batch  350/650 - Loss:  0.001, Seconds: 227.93\n",
      "Epoch   3/100 Batch  400/650 - Loss:  0.001, Seconds: 237.29\n",
      "Epoch   3/100 Batch  450/650 - Loss:  0.001, Seconds: 234.07\n",
      "Epoch   3/100 Batch  500/650 - Loss:  0.001, Seconds: 222.19\n",
      "Epoch   3/100 Batch  550/650 - Loss:  0.001, Seconds: 243.47\n",
      "Epoch   3/100 Batch  600/650 - Loss:  0.001, Seconds: 246.45\n",
      "Testing Loss:  0.001, Seconds: 376.35\n",
      "New Record!\n",
      "Epoch   4/100 Batch   50/650 - Loss:  0.008, Seconds: 222.40\n",
      "Epoch   4/100 Batch  100/650 - Loss:  0.001, Seconds: 236.95\n",
      "Epoch   4/100 Batch  150/650 - Loss:  0.001, Seconds: 226.41\n",
      "Epoch   4/100 Batch  200/650 - Loss:  0.001, Seconds: 234.83\n",
      "Epoch   4/100 Batch  250/650 - Loss:  0.001, Seconds: 224.83\n",
      "Epoch   4/100 Batch  300/650 - Loss:  0.001, Seconds: 249.30\n",
      "Epoch   4/100 Batch  350/650 - Loss:  0.001, Seconds: 229.06\n",
      "Epoch   4/100 Batch  400/650 - Loss:  0.001, Seconds: 238.91\n",
      "Epoch   4/100 Batch  450/650 - Loss:  0.001, Seconds: 236.78\n",
      "Epoch   4/100 Batch  500/650 - Loss:  0.001, Seconds: 223.86\n",
      "Epoch   4/100 Batch  550/650 - Loss:  0.001, Seconds: 245.99\n",
      "Epoch   4/100 Batch  600/650 - Loss:  0.001, Seconds: 247.37\n",
      "Testing Loss:  0.001, Seconds: 379.09\n",
      "No Improvement.\n",
      "Epoch   5/100 Batch   50/650 - Loss:  0.007, Seconds: 224.20\n",
      "Epoch   5/100 Batch  100/650 - Loss:  0.001, Seconds: 238.94\n",
      "Epoch   5/100 Batch  150/650 - Loss:  0.001, Seconds: 228.73\n",
      "Epoch   5/100 Batch  200/650 - Loss:  0.001, Seconds: 236.79\n",
      "Epoch   5/100 Batch  250/650 - Loss:  0.001, Seconds: 226.87\n",
      "Epoch   5/100 Batch  300/650 - Loss:  0.001, Seconds: 251.56\n",
      "Epoch   5/100 Batch  350/650 - Loss:  0.001, Seconds: 231.12\n",
      "Epoch   5/100 Batch  400/650 - Loss:  0.001, Seconds: 241.01\n",
      "Epoch   5/100 Batch  450/650 - Loss:  0.001, Seconds: 238.66\n",
      "Epoch   5/100 Batch  500/650 - Loss:  0.001, Seconds: 226.12\n",
      "Epoch   5/100 Batch  550/650 - Loss:  0.001, Seconds: 248.03\n",
      "Epoch   5/100 Batch  600/650 - Loss:  0.001, Seconds: 248.60\n",
      "Testing Loss:  0.001, Seconds: 379.54\n",
      "New Record!\n",
      "Epoch   6/100 Batch   50/650 - Loss:  0.011, Seconds: 226.35\n",
      "Epoch   6/100 Batch  100/650 - Loss:  0.001, Seconds: 240.27\n",
      "Epoch   6/100 Batch  150/650 - Loss:  0.001, Seconds: 230.34\n",
      "Epoch   6/100 Batch  200/650 - Loss:  0.001, Seconds: 238.81\n",
      "Epoch   6/100 Batch  250/650 - Loss:  0.001, Seconds: 228.45\n",
      "Epoch   6/100 Batch  300/650 - Loss:  0.001, Seconds: 253.40\n",
      "Epoch   6/100 Batch  350/650 - Loss:  0.001, Seconds: 232.68\n",
      "Epoch   6/100 Batch  400/650 - Loss:  0.001, Seconds: 243.24\n",
      "Epoch   6/100 Batch  450/650 - Loss:  0.001, Seconds: 240.81\n",
      "Epoch   6/100 Batch  500/650 - Loss:  0.001, Seconds: 227.55\n",
      "Epoch   6/100 Batch  550/650 - Loss:  0.001, Seconds: 250.35\n",
      "Epoch   6/100 Batch  600/650 - Loss:  0.001, Seconds: 251.55\n",
      "Testing Loss:  0.001, Seconds: 383.98\n",
      "New Record!\n",
      "Epoch   7/100 Batch   50/650 - Loss:  0.011, Seconds: 229.68\n",
      "Epoch   7/100 Batch  100/650 - Loss:  0.001, Seconds: 244.24\n",
      "Epoch   7/100 Batch  150/650 - Loss:  0.001, Seconds: 233.76\n",
      "Epoch   7/100 Batch  200/650 - Loss:  0.001, Seconds: 242.06\n",
      "Epoch   7/100 Batch  250/650 - Loss:  0.001, Seconds: 230.90\n",
      "Epoch   7/100 Batch  300/650 - Loss:  0.001, Seconds: 255.41\n",
      "Epoch   7/100 Batch  350/650 - Loss:  0.001, Seconds: 236.09\n",
      "Epoch   7/100 Batch  400/650 - Loss:  0.001, Seconds: 245.01\n",
      "Epoch   7/100 Batch  450/650 - Loss:  0.001, Seconds: 242.81\n",
      "Epoch   7/100 Batch  500/650 - Loss:  0.001, Seconds: 229.15\n",
      "Epoch   7/100 Batch  550/650 - Loss:  0.001, Seconds: 252.78\n",
      "Epoch   7/100 Batch  600/650 - Loss:  0.001, Seconds: 251.59\n",
      "Testing Loss:  0.001, Seconds: 384.39\n",
      "No Improvement.\n",
      "Epoch   8/100 Batch   50/650 - Loss:  0.008, Seconds: 230.00\n",
      "Epoch   8/100 Batch  100/650 - Loss:  0.001, Seconds: 244.39\n",
      "Epoch   8/100 Batch  150/650 - Loss:  0.001, Seconds: 234.08\n",
      "Epoch   8/100 Batch  200/650 - Loss:  0.001, Seconds: 242.89\n",
      "Epoch   8/100 Batch  250/650 - Loss:  0.001, Seconds: 232.67\n",
      "Epoch   8/100 Batch  300/650 - Loss:  0.001, Seconds: 257.49\n",
      "Epoch   8/100 Batch  350/650 - Loss:  0.001, Seconds: 236.46\n",
      "Epoch   8/100 Batch  400/650 - Loss:  0.001, Seconds: 247.00\n",
      "Epoch   8/100 Batch  450/650 - Loss:  0.001, Seconds: 244.66\n",
      "Epoch   8/100 Batch  500/650 - Loss:  0.001, Seconds: 231.14\n",
      "Epoch   8/100 Batch  550/650 - Loss:  0.001, Seconds: 252.88\n",
      "Epoch   8/100 Batch  600/650 - Loss:  0.001, Seconds: 253.89\n",
      "Testing Loss:  0.001, Seconds: 387.67\n",
      "New Record!\n",
      "Epoch   9/100 Batch   50/650 - Loss:  0.011, Seconds: 232.58\n",
      "Epoch   9/100 Batch  100/650 - Loss:  0.001, Seconds: 246.23\n",
      "Epoch   9/100 Batch  150/650 - Loss:  0.001, Seconds: 236.45\n",
      "Epoch   9/100 Batch  200/650 - Loss:  0.001, Seconds: 244.80\n",
      "Epoch   9/100 Batch  250/650 - Loss:  0.001, Seconds: 234.06\n",
      "Epoch   9/100 Batch  300/650 - Loss:  0.001, Seconds: 259.65\n",
      "Epoch   9/100 Batch  350/650 - Loss:  0.001, Seconds: 238.09\n",
      "Epoch   9/100 Batch  400/650 - Loss:  0.001, Seconds: 249.06\n",
      "Epoch   9/100 Batch  450/650 - Loss:  0.001, Seconds: 246.23\n",
      "Epoch   9/100 Batch  500/650 - Loss:  0.001, Seconds: 233.55\n",
      "Epoch   9/100 Batch  550/650 - Loss:  0.001, Seconds: 255.30\n",
      "Epoch   9/100 Batch  600/650 - Loss:  0.001, Seconds: 255.70\n",
      "Testing Loss:  0.001, Seconds: 390.24\n",
      "New Record!\n",
      "Epoch  10/100 Batch   50/650 - Loss:  0.008, Seconds: 235.23\n",
      "Epoch  10/100 Batch  100/650 - Loss:  0.001, Seconds: 248.59\n",
      "Epoch  10/100 Batch  150/650 - Loss:  0.001, Seconds: 239.55\n",
      "Epoch  10/100 Batch  200/650 - Loss:  0.001, Seconds: 247.77\n",
      "Epoch  10/100 Batch  250/650 - Loss:  0.001, Seconds: 236.30\n",
      "Epoch  10/100 Batch  300/650 - Loss:  0.001, Seconds: 260.68\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-143-8204e80601d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m                     learning_rate, embedding_size, direction, char_to_int_numeric)\n\u001b[1;32m     13\u001b[0m train(model_numeric, epochs, log_string, int_before_dev_numeric, int_after_dev_numeric, \n\u001b[0;32m---> 14\u001b[0;31m       int_before_val_numeric, int_after_val_numeric, char_to_int_numeric, reset = False)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-16-443e5b783dbd>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, epochs, log_string, int_before_train, int_after_train, int_before_test, int_after_test, char_to_int, reset, early_stopping)\u001b[0m\n\u001b[1;32m    112\u001b[0m                                               \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minput_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m                                               \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtargets_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtarget_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m                                               model.keep_prob: keep_probability})\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sourabhjha/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sourabhjha/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sourabhjha/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/sourabhjha/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sourabhjha/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "batch_size = 500\n",
    "num_layers = 3\n",
    "rnn_size = 128\n",
    "embedding_size = len(chars_numeric)\n",
    "learning_rate = 0.0005\n",
    "direction = 2\n",
    "keep_probability = 0.75\n",
    "\n",
    "log_string = 'all_numeric_model_1'\n",
    "model_numeric = build_graph(keep_probability, rnn_size, num_layers, batch_size, \n",
    "                    learning_rate, embedding_size, direction, char_to_int_numeric)\n",
    "train(model_numeric, epochs, log_string, int_before_dev_numeric, int_after_dev_numeric, \n",
    "      int_before_val_numeric, int_after_val_numeric, char_to_int_numeric, reset = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def token_to_ints_numeric(token):\n",
    "    return [char_to_int_numeric[c] for c in token if c in chars_numeric]\n",
    "\n",
    "\n",
    "def score_all_numeric(phrases, max_target_length = 150):\n",
    "    checkpoint = \"./all_numeric_model_1.ckpt\"\n",
    "    epochs = 100\n",
    "    batch_size = 500\n",
    "    num_layers = 3\n",
    "    rnn_size = 128\n",
    "    embedding_size = len(chars_numeric)\n",
    "    learning_rate = 0.0005\n",
    "    direction = 2\n",
    "    keep_probability = 0.75\n",
    "#     tokens1 = [token_to_ints(prev_token) + [char_to_int['<SEP>']] + token_to_ints(token) \n",
    "#                for prev_token, token in zip(prev_tokens, tokens)] \n",
    "    tokens1 = [reduce(lambda a,b : a + b, [token_to_ints_numeric(p) + [char_to_int_numeric['<SEP>']] \n",
    "                                           for p in phrase.split(\"<SEP>\")])[:-1] for phrase in phrases] \n",
    "    \n",
    "    pad_tokens = pad_sentence_batch(tokens1, char_to_int_numeric)\n",
    "        \n",
    "        # Need the lengths for the _lengths parameters\n",
    "    pad_tokens_lengths = []\n",
    "    pad_targets_lengths = []\n",
    "    for token in pad_tokens:\n",
    "        pad_tokens_lengths.append(len(token))    \n",
    "        pad_targets_lengths.append(len(token) + max_target_length)\n",
    "    model = build_graph(keep_probability, rnn_size, num_layers, len(phrases), \n",
    "                                learning_rate, embedding_size, direction, char_to_int_numeric)\n",
    "    with tf.Session() as sess:\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess, checkpoint)\n",
    "        \n",
    "        output_logits = sess.run(model.predictions, {model.inputs: pad_tokens,\n",
    "                                                    model.inputs_length: pad_tokens_lengths,\n",
    "                                                    model.targets_length: [max_target_length],\n",
    "                                                    model.keep_prob: 1.0})\n",
    "    pad = char_to_int_numeric['<PAD>']\n",
    "    return [\"\".join([int_to_char_numeric[i] for i in output_logit if i != pad]).replace(\"<EOS>\",\"\").replace(\"<SELF>\", phrases[j].split(\"<SEP>\")[2])\n",
    "            for j, output_logit in enumerate(output_logits)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./all_numeric_model_1.ckpt\n",
      "0.992\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>&lt;SEP&gt;The&lt;SEP&gt;12639/12640&lt;SEP&gt;Brindavan&lt;SEP&gt;s</td>\n",
       "      <td>12639/12640</td>\n",
       "      <td>twelve thousand six hundred thirty nine twelve...</td>\n",
       "      <td>twelve thousand six hundred thirty nine one tw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>again&lt;SEP&gt;in&lt;SEP&gt;1907&lt;SEP&gt;-&lt;SEP&gt;1908</td>\n",
       "      <td>1907</td>\n",
       "      <td>one thousand nine hundred seven</td>\n",
       "      <td>nineteen o seven</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>1907&lt;SEP&gt;-&lt;SEP&gt;1908&lt;SEP&gt;.&lt;SEP&gt;</td>\n",
       "      <td>1908</td>\n",
       "      <td>one thousand nine hundred eight</td>\n",
       "      <td>nineteen o eight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547</th>\n",
       "      <td>bill&lt;SEP&gt;AB&lt;SEP&gt;1519&lt;SEP&gt;(&lt;SEP&gt;Ma</td>\n",
       "      <td>1519</td>\n",
       "      <td>one five one nine</td>\n",
       "      <td>fifteen nineteen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>682</th>\n",
       "      <td>,&lt;SEP&gt;p&lt;SEP&gt;6955&lt;SEP&gt;-6960&lt;SEP&gt;lcolm</td>\n",
       "      <td>6955</td>\n",
       "      <td>six nine five five</td>\n",
       "      <td>six thousand nine hundred fifty five</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>695</th>\n",
       "      <td>(&lt;SEP&gt;ISBN&lt;SEP&gt;978-1-78076-355-2&lt;SEP&gt;)&lt;SEP&gt;Driver</td>\n",
       "      <td>978-1-78076-355-2</td>\n",
       "      <td>nine seven eight sil one sil seven eight o sev...</td>\n",
       "      <td>nine seven eight sil one sil one sil seven eig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>757</th>\n",
       "      <td>\"&lt;SEP&gt;BUILD&lt;SEP&gt;2015&lt;SEP&gt;News&lt;SEP&gt;:</td>\n",
       "      <td>2015</td>\n",
       "      <td>twenty fifteen</td>\n",
       "      <td>two o one five</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>957</th>\n",
       "      <td>(&lt;SEP&gt;ID-&lt;SEP&gt;3548&lt;SEP&gt;)&lt;SEP&gt;was</td>\n",
       "      <td>3548</td>\n",
       "      <td>three five four eight</td>\n",
       "      <td>three thousand five hundred forty eight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987</th>\n",
       "      <td>,&lt;SEP&gt;CM-&lt;SEP&gt;4206&lt;SEP&gt;and&lt;SEP&gt;CM-</td>\n",
       "      <td>4206</td>\n",
       "      <td>four two o six</td>\n",
       "      <td>four thousand two hundred six</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1059</th>\n",
       "      <td>metres&lt;SEP&gt;(&lt;SEP&gt;45 yd&lt;SEP&gt;)&lt;SEP&gt;intervals</td>\n",
       "      <td>45 yd</td>\n",
       "      <td>forty five yards</td>\n",
       "      <td>forty five yeards</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1139</th>\n",
       "      <td>the&lt;SEP&gt;Night&lt;SEP&gt;(2000)02&lt;SEP&gt;.&lt;SEP&gt;</td>\n",
       "      <td>(2000)02</td>\n",
       "      <td>two thousand sil o two</td>\n",
       "      <td>two o o o sil o two</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1198</th>\n",
       "      <td>cars&lt;SEP&gt;of&lt;SEP&gt;1601&lt;SEP&gt;-&lt;SEP&gt;2500cc</td>\n",
       "      <td>1601</td>\n",
       "      <td>one thousand six hundred one</td>\n",
       "      <td>sixteen o one</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1330</th>\n",
       "      <td>DFB-&lt;SEP&gt;Pokal&lt;SEP&gt;1989&lt;SEP&gt;-&lt;SEP&gt;90</td>\n",
       "      <td>1989</td>\n",
       "      <td>one thousand nine hundred eighty nine</td>\n",
       "      <td>nineteen eighty nine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1391</th>\n",
       "      <td>arsivi&lt;SEP&gt;,&lt;SEP&gt;12.03.1990&lt;SEP&gt;,&lt;SEP&gt;p</td>\n",
       "      <td>12.03.1990</td>\n",
       "      <td>the third of december nineteen ninety</td>\n",
       "      <td>the third of march nineteen ninety</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1410</th>\n",
       "      <td>&lt;SEP&gt;Between&lt;SEP&gt;1999&lt;SEP&gt;-&lt;SEP&gt;2000</td>\n",
       "      <td>1999</td>\n",
       "      <td>one thousand nine hundred ninety nine</td>\n",
       "      <td>nineteen ninety nine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1524</th>\n",
       "      <td>state&lt;SEP&gt;route&lt;SEP&gt;30&lt;SEP&gt;2&lt;SEP&gt;,</td>\n",
       "      <td>30</td>\n",
       "      <td>three</td>\n",
       "      <td>thirty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1590</th>\n",
       "      <td>Campaign&lt;SEP&gt;\"&lt;SEP&gt;123 &lt;SEP&gt;Systems&lt;SEP&gt;,</td>\n",
       "      <td>123</td>\n",
       "      <td>one two three</td>\n",
       "      <td>one hundred twenty three</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1623</th>\n",
       "      <td>ta's&lt;SEP&gt;boeing&lt;SEP&gt;707&lt;SEP&gt;in&lt;SEP&gt;Pan</td>\n",
       "      <td>707</td>\n",
       "      <td>seven o seven</td>\n",
       "      <td>seven hundred seven</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1832</th>\n",
       "      <td>&lt;SEP&gt;SemEval-&lt;SEP&gt;2007&lt;SEP&gt;task&lt;SEP&gt;05</td>\n",
       "      <td>2007</td>\n",
       "      <td>two o o seven</td>\n",
       "      <td>two thousand seven</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1833</th>\n",
       "      <td>2007&lt;SEP&gt;task&lt;SEP&gt;05&lt;SEP&gt;:&lt;SEP&gt;tilingual</td>\n",
       "      <td>05</td>\n",
       "      <td>o five</td>\n",
       "      <td>five</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1904</th>\n",
       "      <td>Scorpion&lt;SEP&gt;(&lt;SEP&gt;A728&lt;SEP&gt;)&lt;SEP&gt;.</td>\n",
       "      <td>A728</td>\n",
       "      <td>a seven two eight</td>\n",
       "      <td>a seven twenty eight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2095</th>\n",
       "      <td>the&lt;SEP&gt;boeing&lt;SEP&gt;777&lt;SEP&gt;.&lt;SEP&gt;</td>\n",
       "      <td>777</td>\n",
       "      <td>seven seven seven</td>\n",
       "      <td>seven hundred seventy seven</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2102</th>\n",
       "      <td>Scotland&lt;SEP&gt;,&lt;SEP&gt;1930&lt;SEP&gt;-&lt;SEP&gt;31</td>\n",
       "      <td>1930</td>\n",
       "      <td>one thousand nine hundred thirty</td>\n",
       "      <td>nineteen thirty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2162</th>\n",
       "      <td>&lt;SEP&gt;The&lt;SEP&gt;1878 &lt;SEP&gt;was&lt;SEP&gt;esigned</td>\n",
       "      <td>1878</td>\n",
       "      <td>one eight seven eight</td>\n",
       "      <td>eighteen seventy eight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2460</th>\n",
       "      <td>the&lt;SEP&gt;U-&lt;SEP&gt;16&lt;SEP&gt;SM&lt;SEP&gt;Caen</td>\n",
       "      <td>16</td>\n",
       "      <td>sixteen</td>\n",
       "      <td>one six</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2517</th>\n",
       "      <td>and&lt;SEP&gt;MD&lt;SEP&gt;853&lt;SEP&gt;D&lt;SEP&gt;were</td>\n",
       "      <td>853</td>\n",
       "      <td>eight hundred fifty three</td>\n",
       "      <td>eight five three</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2523</th>\n",
       "      <td>the&lt;SEP&gt;C-&lt;SEP&gt;17&lt;SEP&gt;ER&lt;SEP&gt;.</td>\n",
       "      <td>17</td>\n",
       "      <td>seventeen</td>\n",
       "      <td>one seven</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2676</th>\n",
       "      <td>rless&lt;SEP&gt;above&lt;SEP&gt;13.0 pH&lt;SEP&gt;.&lt;SEP&gt;</td>\n",
       "      <td>13.0 pH</td>\n",
       "      <td>thirteen point zero pico henrys</td>\n",
       "      <td>thirteen point zero per hour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2982</th>\n",
       "      <td>USSR&lt;SEP&gt;No&lt;SEP&gt;8400-XI&lt;SEP&gt;of&lt;SEP&gt;28, 1988</td>\n",
       "      <td>8400-XI</td>\n",
       "      <td>eight four o o sil xi</td>\n",
       "      <td>eight four o o sil nine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3107</th>\n",
       "      <td>\"&lt;SEP&gt;(&lt;SEP&gt;1984&lt;SEP&gt;-&lt;SEP&gt;1985</td>\n",
       "      <td>1984</td>\n",
       "      <td>one thousand nine hundred eighty four</td>\n",
       "      <td>nineteen eighty four</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5864</th>\n",
       "      <td>&lt;SEP&gt;In&lt;SEP&gt;1996&lt;SEP&gt;-&lt;SEP&gt;1997</td>\n",
       "      <td>1996</td>\n",
       "      <td>one thousand nine hundred ninety six</td>\n",
       "      <td>nineteen ninety six</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5865</th>\n",
       "      <td>1996&lt;SEP&gt;-&lt;SEP&gt;1997&lt;SEP&gt;he&lt;SEP&gt;was</td>\n",
       "      <td>1997</td>\n",
       "      <td>one thousand nine hundred ninety seven</td>\n",
       "      <td>nineteen ninety seven</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5900</th>\n",
       "      <td>)&lt;SEP&gt;ISBN&lt;SEP&gt;975-16-0264-5&lt;SEP&gt;(&lt;SEP&gt;in</td>\n",
       "      <td>975-16-0264-5</td>\n",
       "      <td>nine seven five sil one six sil o two six four...</td>\n",
       "      <td>nine seven five five sil one six sil o two six...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5945</th>\n",
       "      <td>Set&lt;SEP&gt;to&lt;SEP&gt;wed 6th September&lt;SEP&gt;\"&lt;SEP&gt;.</td>\n",
       "      <td>wed 6th September</td>\n",
       "      <td>wednesday the sixth of september</td>\n",
       "      <td>the sixth of september</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6162</th>\n",
       "      <td>&lt;SEP&gt;4 February&lt;SEP&gt;2002&lt;SEP&gt;CRTC&lt;SEP&gt;cision</td>\n",
       "      <td>2002</td>\n",
       "      <td>two o o two</td>\n",
       "      <td>two thousand two</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6331</th>\n",
       "      <td>\"&lt;SEP&gt;6&lt;SEP&gt;.0-magnitude&lt;SEP&gt;quake&lt;SEP&gt;hits</td>\n",
       "      <td>.0-magnitude</td>\n",
       "      <td>dot o d a s h m a g n i t u d e</td>\n",
       "      <td>dot o d a s h m a g g a p i t u d a s i x u d t e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6339</th>\n",
       "      <td>50&lt;SEP&gt;,&lt;SEP&gt;8218&lt;SEP&gt;PG&lt;SEP&gt;chthaven</td>\n",
       "      <td>8218</td>\n",
       "      <td>eight thousand two hundred eighteen</td>\n",
       "      <td>eight two one eight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6379</th>\n",
       "      <td>d&lt;SEP&gt;12 August&lt;SEP&gt;2010&lt;SEP&gt;NIR&lt;SEP&gt;Network</td>\n",
       "      <td>2010</td>\n",
       "      <td>two o one o</td>\n",
       "      <td>two thousand ten</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6420</th>\n",
       "      <td>/&lt;SEP&gt;A-&lt;SEP&gt;3515&lt;SEP&gt;;&lt;SEP&gt;now</td>\n",
       "      <td>3515</td>\n",
       "      <td>three five one five</td>\n",
       "      <td>three thousand five hundred fifteen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6549</th>\n",
       "      <td>&lt;SEP&gt;&lt;SEP&gt;1994&lt;SEP&gt;-&lt;SEP&gt;95</td>\n",
       "      <td>1994</td>\n",
       "      <td>one thousand nine hundred ninety four</td>\n",
       "      <td>nineteen ninety four</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6805</th>\n",
       "      <td>best&lt;SEP&gt;of&lt;SEP&gt;31:18.85&lt;SEP&gt;minutes&lt;SEP&gt;for</td>\n",
       "      <td>31:18.85</td>\n",
       "      <td>thirty one minutes eighteen seconds and eighty...</td>\n",
       "      <td>thirty one million eighteen seconds and eighty...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6946</th>\n",
       "      <td>occur&lt;SEP&gt;in&lt;SEP&gt;1/3,000&lt;SEP&gt;women&lt;SEP&gt;.</td>\n",
       "      <td>1/3,000</td>\n",
       "      <td>one three thousandth</td>\n",
       "      <td>three three thousandth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7279</th>\n",
       "      <td>routes&lt;SEP&gt;(&lt;SEP&gt;M86&lt;SEP&gt;,&lt;SEP&gt;Q</td>\n",
       "      <td>M86</td>\n",
       "      <td>m eighty six</td>\n",
       "      <td>m eight six</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7369</th>\n",
       "      <td>won&lt;SEP&gt;the&lt;SEP&gt;2013 FIM&lt;SEP&gt;eRoad&lt;SEP&gt;acing</td>\n",
       "      <td>2013 FIM</td>\n",
       "      <td>two thousand thirteen finnish markkas</td>\n",
       "      <td>two o one three sil f m m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7403</th>\n",
       "      <td>IDW&lt;SEP&gt;in&lt;SEP&gt;APRIL 2009&lt;SEP&gt;collecting&lt;SEP&gt;</td>\n",
       "      <td>APRIL 2009</td>\n",
       "      <td>april two thousand nine</td>\n",
       "      <td>april two thousand nine rupees</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7434</th>\n",
       "      <td>,&lt;SEP&gt;in&lt;SEP&gt;1940&lt;SEP&gt;-&lt;SEP&gt;41</td>\n",
       "      <td>1940</td>\n",
       "      <td>one thousand nine hundred forty</td>\n",
       "      <td>nineteen forty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7598</th>\n",
       "      <td>UK&lt;SEP&gt;ISBN&lt;SEP&gt;978-1-84425-703-4&lt;SEP&gt;Rice&lt;SEP&gt;,</td>\n",
       "      <td>978-1-84425-703-4</td>\n",
       "      <td>nine seven eight sil one sil eight four four t...</td>\n",
       "      <td>nine seven eight sil one sil eight four four f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7723</th>\n",
       "      <td>dcasting&lt;SEP&gt;as&lt;SEP&gt;10&lt;SEP&gt;BI&lt;SEP&gt;in</td>\n",
       "      <td>10</td>\n",
       "      <td>one o</td>\n",
       "      <td>ten</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8373</th>\n",
       "      <td>ssed&lt;SEP&gt;within&lt;SEP&gt;30 million km&lt;SEP&gt;of&lt;SEP&gt;the</td>\n",
       "      <td>30 million km</td>\n",
       "      <td>30 million km</td>\n",
       "      <td>thirty million kilometers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8398</th>\n",
       "      <td>ds&lt;SEP&gt;Aircraft&lt;SEP&gt;1993&lt;SEP&gt;-&lt;SEP&gt;94</td>\n",
       "      <td>1993</td>\n",
       "      <td>one thousand nine hundred ninety three</td>\n",
       "      <td>nineteen ninety three</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8661</th>\n",
       "      <td>&lt;SEP&gt;sowieckiej&lt;SEP&gt;1919&lt;SEP&gt;-&lt;SEP&gt;1920</td>\n",
       "      <td>1919</td>\n",
       "      <td>one thousand nine hundred nineteen</td>\n",
       "      <td>nineteen nineteen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8662</th>\n",
       "      <td>1919&lt;SEP&gt;-&lt;SEP&gt;1920&lt;SEP&gt;.&lt;SEP&gt;</td>\n",
       "      <td>1920</td>\n",
       "      <td>one thousand nine hundred twenty</td>\n",
       "      <td>nineteen twenty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8702</th>\n",
       "      <td>07&lt;SEP&gt;SEOUL&lt;SEP&gt;3225&lt;SEP&gt;,&lt;SEP&gt;KIM</td>\n",
       "      <td>3225</td>\n",
       "      <td>three two two five</td>\n",
       "      <td>three thousand two hundred twenty five</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8848</th>\n",
       "      <td>listing&lt;SEP&gt;RNE&lt;SEP&gt;100229&lt;SEP&gt;)&lt;SEP&gt;\"</td>\n",
       "      <td>100229</td>\n",
       "      <td>one o o two two nine</td>\n",
       "      <td>one hundred thousand two hundred twenty nine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8920</th>\n",
       "      <td>&lt;SEP&gt;&lt;SEP&gt;2013&lt;SEP&gt;-&lt;SEP&gt;87.68%</td>\n",
       "      <td>2013</td>\n",
       "      <td>two thousand thirteen</td>\n",
       "      <td>twenty thirteen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9149</th>\n",
       "      <td>about&lt;SEP&gt;G&lt;SEP&gt;30&lt;SEP&gt;S&lt;SEP&gt;tragedy</td>\n",
       "      <td>30</td>\n",
       "      <td>three o</td>\n",
       "      <td>thirty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9413</th>\n",
       "      <td>odetool&lt;SEP&gt;for&lt;SEP&gt;7255KA&lt;SEP&gt;\"&lt;SEP&gt;.</td>\n",
       "      <td>7255KA</td>\n",
       "      <td>seven thousand two hundred fifty five kilo amp...</td>\n",
       "      <td>seven thousand two hundred fifty five kilo anrmes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9534</th>\n",
       "      <td>:&lt;SEP&gt;HR&lt;SEP&gt;4411&lt;SEP&gt;\"&lt;SEP&gt;.</td>\n",
       "      <td>4411</td>\n",
       "      <td>four thousand four hundred eleven</td>\n",
       "      <td>four four one one</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9605</th>\n",
       "      <td>students&lt;SEP&gt;in&lt;SEP&gt;1989&lt;SEP&gt;-&lt;SEP&gt;1991</td>\n",
       "      <td>1989</td>\n",
       "      <td>nineteen eighty nine</td>\n",
       "      <td>one thousand nine hundred eighty nine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9872</th>\n",
       "      <td>io&lt;SEP&gt;equalled&lt;SEP&gt;1/185&lt;SEP&gt;.&lt;SEP&gt;</td>\n",
       "      <td>1/185</td>\n",
       "      <td>one one hundred eighty fifth</td>\n",
       "      <td>one one hundred eighty fifths</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      0                  1  \\\n",
       "109        <SEP>The<SEP>12639/12640<SEP>Brindavan<SEP>s        12639/12640   \n",
       "151                again<SEP>in<SEP>1907<SEP>-<SEP>1908               1907   \n",
       "152                      1907<SEP>-<SEP>1908<SEP>.<SEP>               1908   \n",
       "547                   bill<SEP>AB<SEP>1519<SEP>(<SEP>Ma               1519   \n",
       "682                ,<SEP>p<SEP>6955<SEP>-6960<SEP>lcolm               6955   \n",
       "695   (<SEP>ISBN<SEP>978-1-78076-355-2<SEP>)<SEP>Driver  978-1-78076-355-2   \n",
       "757                 \"<SEP>BUILD<SEP>2015<SEP>News<SEP>:               2015   \n",
       "957                    (<SEP>ID-<SEP>3548<SEP>)<SEP>was               3548   \n",
       "987                  ,<SEP>CM-<SEP>4206<SEP>and<SEP>CM-               4206   \n",
       "1059         metres<SEP>(<SEP>45 yd<SEP>)<SEP>intervals              45 yd   \n",
       "1139              the<SEP>Night<SEP>(2000)02<SEP>.<SEP>           (2000)02   \n",
       "1198              cars<SEP>of<SEP>1601<SEP>-<SEP>2500cc               1601   \n",
       "1330               DFB-<SEP>Pokal<SEP>1989<SEP>-<SEP>90               1989   \n",
       "1391            arsivi<SEP>,<SEP>12.03.1990<SEP>,<SEP>p         12.03.1990   \n",
       "1410               <SEP>Between<SEP>1999<SEP>-<SEP>2000               1999   \n",
       "1524                 state<SEP>route<SEP>30<SEP>2<SEP>,                 30   \n",
       "1590          Campaign<SEP>\"<SEP>123 <SEP>Systems<SEP>,               123    \n",
       "1623             ta's<SEP>boeing<SEP>707<SEP>in<SEP>Pan                707   \n",
       "1832             <SEP>SemEval-<SEP>2007<SEP>task<SEP>05               2007   \n",
       "1833           2007<SEP>task<SEP>05<SEP>:<SEP>tilingual                 05   \n",
       "1904                Scorpion<SEP>(<SEP>A728<SEP>)<SEP>.               A728   \n",
       "2095                  the<SEP>boeing<SEP>777<SEP>.<SEP>                777   \n",
       "2102               Scotland<SEP>,<SEP>1930<SEP>-<SEP>31               1930   \n",
       "2162             <SEP>The<SEP>1878 <SEP>was<SEP>esigned              1878    \n",
       "2460                  the<SEP>U-<SEP>16<SEP>SM<SEP>Caen                 16   \n",
       "2517                  and<SEP>MD<SEP>853<SEP>D<SEP>were                853   \n",
       "2523                     the<SEP>C-<SEP>17<SEP>ER<SEP>.                 17   \n",
       "2676             rless<SEP>above<SEP>13.0 pH<SEP>.<SEP>            13.0 pH   \n",
       "2982        USSR<SEP>No<SEP>8400-XI<SEP>of<SEP>28, 1988            8400-XI   \n",
       "3107                    \"<SEP>(<SEP>1984<SEP>-<SEP>1985               1984   \n",
       "...                                                 ...                ...   \n",
       "5864                    <SEP>In<SEP>1996<SEP>-<SEP>1997               1996   \n",
       "5865                 1996<SEP>-<SEP>1997<SEP>he<SEP>was               1997   \n",
       "5900          )<SEP>ISBN<SEP>975-16-0264-5<SEP>(<SEP>in      975-16-0264-5   \n",
       "5945       Set<SEP>to<SEP>wed 6th September<SEP>\"<SEP>.  wed 6th September   \n",
       "6162       <SEP>4 February<SEP>2002<SEP>CRTC<SEP>cision               2002   \n",
       "6331        \"<SEP>6<SEP>.0-magnitude<SEP>quake<SEP>hits       .0-magnitude   \n",
       "6339              50<SEP>,<SEP>8218<SEP>PG<SEP>chthaven               8218   \n",
       "6379       d<SEP>12 August<SEP>2010<SEP>NIR<SEP>Network               2010   \n",
       "6420                    /<SEP>A-<SEP>3515<SEP>;<SEP>now               3515   \n",
       "6549                        <SEP><SEP>1994<SEP>-<SEP>95               1994   \n",
       "6805       best<SEP>of<SEP>31:18.85<SEP>minutes<SEP>for           31:18.85   \n",
       "6946           occur<SEP>in<SEP>1/3,000<SEP>women<SEP>.            1/3,000   \n",
       "7279                   routes<SEP>(<SEP>M86<SEP>,<SEP>Q                M86   \n",
       "7369       won<SEP>the<SEP>2013 FIM<SEP>eRoad<SEP>acing           2013 FIM   \n",
       "7403      IDW<SEP>in<SEP>APRIL 2009<SEP>collecting<SEP>         APRIL 2009   \n",
       "7434                     ,<SEP>in<SEP>1940<SEP>-<SEP>41               1940   \n",
       "7598   UK<SEP>ISBN<SEP>978-1-84425-703-4<SEP>Rice<SEP>,  978-1-84425-703-4   \n",
       "7723               dcasting<SEP>as<SEP>10<SEP>BI<SEP>in                 10   \n",
       "8373   ssed<SEP>within<SEP>30 million km<SEP>of<SEP>the      30 million km   \n",
       "8398              ds<SEP>Aircraft<SEP>1993<SEP>-<SEP>94               1993   \n",
       "8661            <SEP>sowieckiej<SEP>1919<SEP>-<SEP>1920               1919   \n",
       "8662                     1919<SEP>-<SEP>1920<SEP>.<SEP>               1920   \n",
       "8702                07<SEP>SEOUL<SEP>3225<SEP>,<SEP>KIM               3225   \n",
       "8848             listing<SEP>RNE<SEP>100229<SEP>)<SEP>\"             100229   \n",
       "8920                    <SEP><SEP>2013<SEP>-<SEP>87.68%               2013   \n",
       "9149               about<SEP>G<SEP>30<SEP>S<SEP>tragedy                 30   \n",
       "9413             odetool<SEP>for<SEP>7255KA<SEP>\"<SEP>.             7255KA   \n",
       "9534                      :<SEP>HR<SEP>4411<SEP>\"<SEP>.               4411   \n",
       "9605            students<SEP>in<SEP>1989<SEP>-<SEP>1991               1989   \n",
       "9872               io<SEP>equalled<SEP>1/185<SEP>.<SEP>              1/185   \n",
       "\n",
       "                                                      2  \\\n",
       "109   twelve thousand six hundred thirty nine twelve...   \n",
       "151                     one thousand nine hundred seven   \n",
       "152                     one thousand nine hundred eight   \n",
       "547                                   one five one nine   \n",
       "682                                  six nine five five   \n",
       "695   nine seven eight sil one sil seven eight o sev...   \n",
       "757                                      twenty fifteen   \n",
       "957                               three five four eight   \n",
       "987                                      four two o six   \n",
       "1059                                   forty five yards   \n",
       "1139                             two thousand sil o two   \n",
       "1198                       one thousand six hundred one   \n",
       "1330              one thousand nine hundred eighty nine   \n",
       "1391              the third of december nineteen ninety   \n",
       "1410              one thousand nine hundred ninety nine   \n",
       "1524                                              three   \n",
       "1590                                      one two three   \n",
       "1623                                      seven o seven   \n",
       "1832                                      two o o seven   \n",
       "1833                                             o five   \n",
       "1904                                  a seven two eight   \n",
       "2095                                  seven seven seven   \n",
       "2102                   one thousand nine hundred thirty   \n",
       "2162                              one eight seven eight   \n",
       "2460                                            sixteen   \n",
       "2517                          eight hundred fifty three   \n",
       "2523                                          seventeen   \n",
       "2676                    thirteen point zero pico henrys   \n",
       "2982                              eight four o o sil xi   \n",
       "3107              one thousand nine hundred eighty four   \n",
       "...                                                 ...   \n",
       "5864               one thousand nine hundred ninety six   \n",
       "5865             one thousand nine hundred ninety seven   \n",
       "5900  nine seven five sil one six sil o two six four...   \n",
       "5945                   wednesday the sixth of september   \n",
       "6162                                        two o o two   \n",
       "6331                    dot o d a s h m a g n i t u d e   \n",
       "6339                eight thousand two hundred eighteen   \n",
       "6379                                        two o one o   \n",
       "6420                                three five one five   \n",
       "6549              one thousand nine hundred ninety four   \n",
       "6805  thirty one minutes eighteen seconds and eighty...   \n",
       "6946                               one three thousandth   \n",
       "7279                                       m eighty six   \n",
       "7369              two thousand thirteen finnish markkas   \n",
       "7403                            april two thousand nine   \n",
       "7434                    one thousand nine hundred forty   \n",
       "7598  nine seven eight sil one sil eight four four t...   \n",
       "7723                                              one o   \n",
       "8373                                      30 million km   \n",
       "8398             one thousand nine hundred ninety three   \n",
       "8661                 one thousand nine hundred nineteen   \n",
       "8662                   one thousand nine hundred twenty   \n",
       "8702                                 three two two five   \n",
       "8848                               one o o two two nine   \n",
       "8920                              two thousand thirteen   \n",
       "9149                                            three o   \n",
       "9413  seven thousand two hundred fifty five kilo amp...   \n",
       "9534                  four thousand four hundred eleven   \n",
       "9605                               nineteen eighty nine   \n",
       "9872                       one one hundred eighty fifth   \n",
       "\n",
       "                                                      3  \n",
       "109   twelve thousand six hundred thirty nine one tw...  \n",
       "151                                    nineteen o seven  \n",
       "152                                    nineteen o eight  \n",
       "547                                    fifteen nineteen  \n",
       "682                six thousand nine hundred fifty five  \n",
       "695   nine seven eight sil one sil one sil seven eig...  \n",
       "757                                      two o one five  \n",
       "957             three thousand five hundred forty eight  \n",
       "987                       four thousand two hundred six  \n",
       "1059                                  forty five yeards  \n",
       "1139                                two o o o sil o two  \n",
       "1198                                      sixteen o one  \n",
       "1330                               nineteen eighty nine  \n",
       "1391                 the third of march nineteen ninety  \n",
       "1410                               nineteen ninety nine  \n",
       "1524                                             thirty  \n",
       "1590                           one hundred twenty three  \n",
       "1623                                seven hundred seven  \n",
       "1832                                 two thousand seven  \n",
       "1833                                               five  \n",
       "1904                               a seven twenty eight  \n",
       "2095                        seven hundred seventy seven  \n",
       "2102                                    nineteen thirty  \n",
       "2162                             eighteen seventy eight  \n",
       "2460                                            one six  \n",
       "2517                                   eight five three  \n",
       "2523                                          one seven  \n",
       "2676                       thirteen point zero per hour  \n",
       "2982                            eight four o o sil nine  \n",
       "3107                               nineteen eighty four  \n",
       "...                                                 ...  \n",
       "5864                                nineteen ninety six  \n",
       "5865                              nineteen ninety seven  \n",
       "5900  nine seven five five sil one six sil o two six...  \n",
       "5945                             the sixth of september  \n",
       "6162                                   two thousand two  \n",
       "6331  dot o d a s h m a g g a p i t u d a s i x u d t e  \n",
       "6339                                eight two one eight  \n",
       "6379                                   two thousand ten  \n",
       "6420                three thousand five hundred fifteen  \n",
       "6549                               nineteen ninety four  \n",
       "6805  thirty one million eighteen seconds and eighty...  \n",
       "6946                             three three thousandth  \n",
       "7279                                        m eight six  \n",
       "7369                          two o one three sil f m m  \n",
       "7403                     april two thousand nine rupees  \n",
       "7434                                     nineteen forty  \n",
       "7598  nine seven eight sil one sil eight four four f...  \n",
       "7723                                                ten  \n",
       "8373                          thirty million kilometers  \n",
       "8398                              nineteen ninety three  \n",
       "8661                                  nineteen nineteen  \n",
       "8662                                    nineteen twenty  \n",
       "8702             three thousand two hundred twenty five  \n",
       "8848       one hundred thousand two hundred twenty nine  \n",
       "8920                                    twenty thirteen  \n",
       "9149                                             thirty  \n",
       "9413  seven thousand two hundred fifty five kilo anrmes  \n",
       "9534                                  four four one one  \n",
       "9605              one thousand nine hundred eighty nine  \n",
       "9872                      one one hundred eighty fifths  \n",
       "\n",
       "[80 rows x 4 columns]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_pred = 10000\n",
    "predictions =  score_all_numeric(list(val_data_numeric['before_phrase'])[:n_pred])\n",
    "res = pd.DataFrame(zip(list(val_data_numeric['before_phrase'])[:n_pred],list(val_data_numeric['before'])[:n_pred], \n",
    "                       list(val_data_numeric['after'])[:n_pred],predictions))\n",
    "\n",
    "print(np.mean(res[2] == res[3]))\n",
    "res[res[2] != res[3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_pred = 5000\n",
    "predictions =  score_all_numeric(list(dev_data_numeric['before_phrase'])[:n_pred])\n",
    "res = pd.DataFrame(zip(list(dev_data_numeric['before_phrase'])[:n_pred],list(dev_data_numeric['before'])[:n_pred], \n",
    "                       list(dev_data_numeric['after'])[:n_pred],predictions))\n",
    "\n",
    "print(np.mean(res[2] == res[3]))\n",
    "res[res[2] != res[3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./all_letters_model_1.ckpt\n",
      "()\n",
      "Training Model: all_letters_model_1\n",
      "Epoch   1/100 Batch   50/326 - Loss:  0.321, Seconds: 52.89\n",
      "Epoch   1/100 Batch  100/326 - Loss:  0.006, Seconds: 51.90\n",
      "Epoch   1/100 Batch  150/326 - Loss:  0.005, Seconds: 51.09\n",
      "Epoch   1/100 Batch  200/326 - Loss:  0.004, Seconds: 53.12\n",
      "Epoch   1/100 Batch  250/326 - Loss:  0.004, Seconds: 52.66\n",
      "Epoch   1/100 Batch  300/326 - Loss:  0.004, Seconds: 51.78\n",
      "Testing Loss:  0.003, Seconds: 38.88\n",
      "New Record!\n",
      "Epoch   2/100 Batch   50/326 - Loss:  0.067, Seconds: 54.08\n",
      "Epoch   2/100 Batch  100/326 - Loss:  0.006, Seconds: 53.21\n",
      "Epoch   2/100 Batch  150/326 - Loss:  0.005, Seconds: 51.88\n",
      "Epoch   2/100 Batch  200/326 - Loss:  0.004, Seconds: 54.39\n",
      "Epoch   2/100 Batch  250/326 - Loss:  0.004, Seconds: 53.88\n",
      "Epoch   2/100 Batch  300/326 - Loss:  0.004, Seconds: 53.20\n",
      "Testing Loss:  0.003, Seconds: 39.59\n",
      "New Record!\n",
      "Epoch   3/100 Batch   50/326 - Loss:  0.042, Seconds: 55.56\n",
      "Epoch   3/100 Batch  100/326 - Loss:  0.005, Seconds: 54.84\n",
      "Epoch   3/100 Batch  150/326 - Loss:  0.004, Seconds: 53.53\n",
      "Epoch   3/100 Batch  200/326 - Loss:  0.003, Seconds: 56.20\n",
      "Epoch   3/100 Batch  250/326 - Loss:  0.004, Seconds: 56.11\n",
      "Epoch   3/100 Batch  300/326 - Loss:  0.003, Seconds: 54.64\n",
      "Testing Loss:  0.003, Seconds: 40.51\n",
      "New Record!\n",
      "Epoch   4/100 Batch   50/326 - Loss:  0.033, Seconds: 57.27\n",
      "Epoch   4/100 Batch  100/326 - Loss:  0.004, Seconds: 56.22\n",
      "Epoch   4/100 Batch  150/326 - Loss:  0.004, Seconds: 55.02\n",
      "Epoch   4/100 Batch  200/326 - Loss:  0.003, Seconds: 57.81\n",
      "Epoch   4/100 Batch  250/326 - Loss:  0.003, Seconds: 56.90\n",
      "Epoch   4/100 Batch  300/326 - Loss:  0.003, Seconds: 56.26\n",
      "Testing Loss:  0.003, Seconds: 41.61\n",
      "New Record!\n",
      "Epoch   5/100 Batch   50/326 - Loss:  0.030, Seconds: 58.67\n",
      "Epoch   5/100 Batch  100/326 - Loss:  0.004, Seconds: 57.62\n",
      "Epoch   5/100 Batch  150/326 - Loss:  0.003, Seconds: 56.42\n",
      "Epoch   5/100 Batch  200/326 - Loss:  0.003, Seconds: 58.76\n",
      "Epoch   5/100 Batch  250/326 - Loss:  0.003, Seconds: 58.19\n",
      "Epoch   5/100 Batch  300/326 - Loss:  0.003, Seconds: 57.72\n",
      "Testing Loss:  0.002, Seconds: 42.58\n",
      "New Record!\n",
      "Epoch   6/100 Batch   50/326 - Loss:  0.026, Seconds: 60.32\n",
      "Epoch   6/100 Batch  100/326 - Loss:  0.004, Seconds: 58.93\n",
      "Epoch   6/100 Batch  150/326 - Loss:  0.003, Seconds: 57.99\n",
      "Epoch   6/100 Batch  200/326 - Loss:  0.003, Seconds: 60.45\n",
      "Epoch   6/100 Batch  250/326 - Loss:  0.003, Seconds: 60.17\n",
      "Epoch   6/100 Batch  300/326 - Loss:  0.003, Seconds: 59.16\n",
      "Testing Loss:  0.002, Seconds: 43.51\n",
      "New Record!\n",
      "Epoch   7/100 Batch   50/326 - Loss:  0.027, Seconds: 61.71\n",
      "Epoch   7/100 Batch  100/326 - Loss:  0.003, Seconds: 60.73\n",
      "Epoch   7/100 Batch  150/326 - Loss:  0.003, Seconds: 59.28\n",
      "Epoch   7/100 Batch  200/326 - Loss:  0.002, Seconds: 61.75\n",
      "Epoch   7/100 Batch  250/326 - Loss:  0.003, Seconds: 61.35\n",
      "Epoch   7/100 Batch  300/326 - Loss:  0.003, Seconds: 60.67\n",
      "Testing Loss:  0.002, Seconds: 45.11\n",
      "New Record!\n",
      "Epoch   8/100 Batch   50/326 - Loss:  0.030, Seconds: 63.16\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-0a25f9b9fc0b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m                     learning_rate, embedding_size, direction, char_to_int_letters)\n\u001b[1;32m     13\u001b[0m train(model_letters, epochs, log_string, int_before_dev_letters, int_after_dev_letters, \n\u001b[0;32m---> 14\u001b[0;31m       int_before_val_letters, int_after_val_letters, char_to_int_letters, reset = False)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-16-443e5b783dbd>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, epochs, log_string, int_before_train, int_after_train, int_before_test, int_after_test, char_to_int, reset, early_stopping)\u001b[0m\n\u001b[1;32m    112\u001b[0m                                               \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minput_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m                                               \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtargets_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtarget_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m                                               model.keep_prob: keep_probability})\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sourabhjha/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sourabhjha/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sourabhjha/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/sourabhjha/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sourabhjha/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "batch_size = 400\n",
    "num_layers = 3\n",
    "rnn_size = 128\n",
    "embedding_size = len(chars_letters)\n",
    "learning_rate = 0.0003\n",
    "direction = 2\n",
    "keep_probability = 0.75\n",
    "\n",
    "log_string = 'all_letters_model_1'\n",
    "model_letters = build_graph(keep_probability, rnn_size, num_layers, batch_size, \n",
    "                    learning_rate, embedding_size, direction, char_to_int_letters)\n",
    "train(model_letters, epochs, log_string, int_before_dev_letters, int_after_dev_letters, \n",
    "      int_before_val_letters, int_after_val_letters, char_to_int_letters, reset = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "batch_size = 500\n",
    "num_layers = 3\n",
    "rnn_size = 128\n",
    "embedding_size = len(chars_letters)\n",
    "learning_rate = 0.0005\n",
    "direction = 2\n",
    "keep_probability = 0.75\n",
    "\n",
    "def token_to_ints_letters(token):\n",
    "    return [char_to_int_letters[c] for c in token if c in chars_letters]\n",
    "\n",
    "\n",
    "def score_all_letters(phrases, max_target_length = 150):\n",
    "    checkpoint = \"./all_letters_model_1.ckpt\"\n",
    "    epochs = 100\n",
    "    batch_size = 500\n",
    "    num_layers = 3\n",
    "    rnn_size = 128\n",
    "    embedding_size = len(chars_letters)\n",
    "    learning_rate = 0.0005\n",
    "    direction = 2\n",
    "    keep_probability = 0.75\n",
    "#     tokens1 = [token_to_ints(prev_token) + [char_to_int['<SEP>']] + token_to_ints(token) \n",
    "#                for prev_token, token in zip(prev_tokens, tokens)] \n",
    "    tokens1 = [reduce(lambda a,b : a + b, [token_to_ints_letters(p) + [char_to_int_letters['<SEP>']] \n",
    "                                           for p in phrase.split(\"<SEP>\")])[:-1] for phrase in phrases] \n",
    "    \n",
    "    pad_tokens = pad_sentence_batch(tokens1, char_to_int_letters)\n",
    "        \n",
    "        # Need the lengths for the _lengths parameters\n",
    "    pad_tokens_lengths = []\n",
    "    pad_targets_lengths = []\n",
    "    for token in pad_tokens:\n",
    "        pad_tokens_lengths.append(len(token))    \n",
    "        pad_targets_lengths.append(len(token) + max_target_length)\n",
    "    model = build_graph(keep_probability, rnn_size, num_layers, len(phrases), \n",
    "                                learning_rate, embedding_size, direction, char_to_int_letters)\n",
    "    with tf.Session() as sess:\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess, checkpoint)\n",
    "        \n",
    "        output_logits = sess.run(model.predictions, {model.inputs: pad_tokens,\n",
    "                                                    model.inputs_length: pad_tokens_lengths,\n",
    "                                                    model.targets_length: [max_target_length],\n",
    "                                                    model.keep_prob: 1.0})\n",
    "    pad = char_to_int_rest['<PAD>']\n",
    "    return [\"\".join([int_to_char_letters[i] for i in output_logit if i != pad]).replace(\"<EOS>\",\"\").replace(\"<SELF>\", phrases[j].split(\"<SEP>\")[2])\n",
    "            for j, output_logit in enumerate(output_logits)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./all_letters_model_1.ckpt\n",
      "0.984244879586\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>rograms&lt;SEP&gt;&lt;SEP&gt;NORML&lt;SEP&gt;\"&lt;SEP&gt;.</td>\n",
       "      <td>NORML</td>\n",
       "      <td>NORML</td>\n",
       "      <td>n o r m l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>&lt;SEP&gt;&lt;SEP&gt;USAR&lt;SEP&gt;task&lt;SEP&gt;forces</td>\n",
       "      <td>USAR</td>\n",
       "      <td>USAR</td>\n",
       "      <td>u s a r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>&lt;SEP&gt;\"&lt;SEP&gt;COPYING&lt;SEP&gt;file&lt;SEP&gt;from</td>\n",
       "      <td>COPYING</td>\n",
       "      <td>COPYING</td>\n",
       "      <td>c o p y i n g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>,&lt;SEP&gt;the&lt;SEP&gt;TULF&lt;SEP&gt;remained&lt;SEP&gt;he</td>\n",
       "      <td>TULF</td>\n",
       "      <td>t u l f</td>\n",
       "      <td>TULF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>State&lt;SEP&gt;bill&lt;SEP&gt;AB&lt;SEP&gt;1519&lt;SEP&gt;(</td>\n",
       "      <td>AB</td>\n",
       "      <td>a b</td>\n",
       "      <td>AB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>&lt;SEP&gt;&lt;SEP&gt;WISDOM&lt;SEP&gt;Television&lt;SEP&gt;</td>\n",
       "      <td>WISDOM</td>\n",
       "      <td>WISDOM</td>\n",
       "      <td>w i s d o m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>&lt;SEP&gt;&lt;SEP&gt;SHODAN&lt;SEP&gt;:&lt;SEP&gt;True</td>\n",
       "      <td>SHODAN</td>\n",
       "      <td>s h o d a n</td>\n",
       "      <td>SHODAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>unit&lt;SEP&gt;and&lt;SEP&gt;ALU&lt;SEP&gt;into&lt;SEP&gt;what</td>\n",
       "      <td>ALU</td>\n",
       "      <td>ALU</td>\n",
       "      <td>a l u</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>&lt;SEP&gt;&lt;SEP&gt;PL&lt;SEP&gt;360&lt;SEP&gt;,</td>\n",
       "      <td>PL</td>\n",
       "      <td>p l</td>\n",
       "      <td>place</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>&lt;SEP&gt;&lt;SEP&gt;AAG&lt;SEP&gt;rocomputer&lt;SEP&gt;</td>\n",
       "      <td>AAG</td>\n",
       "      <td>AAG</td>\n",
       "      <td>a a g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>518</th>\n",
       "      <td>\"&lt;SEP&gt;Tomato&lt;SEP&gt;BRASSINOSTEROID&lt;SEP&gt;NSENSITIV...</td>\n",
       "      <td>BRASSINOSTEROID</td>\n",
       "      <td>b r a s s i n o s t e r o i d</td>\n",
       "      <td>BRASSINOSTEROID</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>730</th>\n",
       "      <td>amounts&lt;SEP&gt;of&lt;SEP&gt;CADMIUM&lt;SEP&gt;116&lt;SEP&gt;,</td>\n",
       "      <td>CADMIUM</td>\n",
       "      <td>CADMIUM</td>\n",
       "      <td>c a d m i u m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>991</th>\n",
       "      <td>&lt;SEP&gt;&lt;SEP&gt;FYROM&lt;SEP&gt;,&lt;SEP&gt;2011</td>\n",
       "      <td>FYROM</td>\n",
       "      <td>f y r o m</td>\n",
       "      <td>FYROM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1154</th>\n",
       "      <td>Coup&lt;SEP&gt;:&lt;SEP&gt;PEGOT&lt;SEP&gt;\"&lt;SEP&gt;.</td>\n",
       "      <td>PEGOT</td>\n",
       "      <td>p e g o t</td>\n",
       "      <td>PEGOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1155</th>\n",
       "      <td>of&lt;SEP&gt;5&lt;SEP&gt;ACS&lt;SEP&gt;'&lt;SEP&gt;projects</td>\n",
       "      <td>ACS</td>\n",
       "      <td>a c s</td>\n",
       "      <td>ACS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1175</th>\n",
       "      <td>55&lt;SEP&gt;(&lt;SEP&gt;DAF&lt;SEP&gt;)&lt;SEP&gt;-</td>\n",
       "      <td>DAF</td>\n",
       "      <td>DAF</td>\n",
       "      <td>d a f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1418</th>\n",
       "      <td>TEST&lt;SEP&gt;A&lt;SEP&gt;DESENZANO&lt;SEP&gt;\"&lt;SEP&gt;(</td>\n",
       "      <td>DESENZANO</td>\n",
       "      <td>d e s e n z a n o</td>\n",
       "      <td>DESENZANO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1464</th>\n",
       "      <td>&lt;SEP&gt;\"&lt;SEP&gt;ASSEMBLEA&lt;SEP&gt;PD&lt;SEP&gt;/</td>\n",
       "      <td>ASSEMBLEA</td>\n",
       "      <td>a s s e m b l e a</td>\n",
       "      <td>ASSEMBLEA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1556</th>\n",
       "      <td>FINLAND&lt;SEP&gt;:&lt;SEP&gt;PERTTI&lt;SEP&gt;KURIKAN&lt;SEP&gt;VAT</td>\n",
       "      <td>PERTTI</td>\n",
       "      <td>p e r t t i</td>\n",
       "      <td>PERTTI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1589</th>\n",
       "      <td>.&lt;SEP&gt;458&lt;SEP&gt;GEX&lt;SEP&gt;268&lt;SEP&gt;.</td>\n",
       "      <td>GEX</td>\n",
       "      <td>GEX</td>\n",
       "      <td>g e x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1592</th>\n",
       "      <td>Institute&lt;SEP&gt;(&lt;SEP&gt;LII&lt;SEP&gt;)&lt;SEP&gt;.</td>\n",
       "      <td>LII</td>\n",
       "      <td>LII</td>\n",
       "      <td>l i i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1710</th>\n",
       "      <td>&lt;SEP&gt;&lt;SEP&gt;BYTE&lt;SEP&gt;concluded&lt;SEP&gt;t</td>\n",
       "      <td>BYTE</td>\n",
       "      <td>BYTE</td>\n",
       "      <td>b y t e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1735</th>\n",
       "      <td>Industry&lt;SEP&gt;(&lt;SEP&gt;MITI&lt;SEP&gt;)&lt;SEP&gt;played</td>\n",
       "      <td>MITI</td>\n",
       "      <td>MITI</td>\n",
       "      <td>m i t i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1801</th>\n",
       "      <td>&lt;SEP&gt;Volleyball&lt;SEP&gt;XIII&lt;SEP&gt;World&lt;SEP&gt;nship</td>\n",
       "      <td>XIII</td>\n",
       "      <td>thirteenth</td>\n",
       "      <td>thirteen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1907</th>\n",
       "      <td>&lt;SEP&gt;\"&lt;SEP&gt;TROPICOS&lt;SEP&gt;Web&lt;SEP&gt;display</td>\n",
       "      <td>TROPICOS</td>\n",
       "      <td>t r o p i c o s</td>\n",
       "      <td>TROPICOS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1941</th>\n",
       "      <td>&lt;SEP&gt;The&lt;SEP&gt;OTCO&lt;SEP&gt;Program&lt;SEP&gt;ers</td>\n",
       "      <td>OTCO</td>\n",
       "      <td>OTCO</td>\n",
       "      <td>o t c o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001</th>\n",
       "      <td>&lt;SEP&gt;\"&lt;SEP&gt;YPSILANTI&lt;SEP&gt;TOWNSHIP&lt;SEP&gt;:</td>\n",
       "      <td>YPSILANTI</td>\n",
       "      <td>YPSILANTI</td>\n",
       "      <td>y p s i l a n t i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021</th>\n",
       "      <td>&lt;SEP&gt;&lt;SEP&gt;KUA&lt;SEP&gt;holds&lt;SEP&gt;ility</td>\n",
       "      <td>KUA</td>\n",
       "      <td>k u a</td>\n",
       "      <td>KUA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2097</th>\n",
       "      <td>&lt;SEP&gt;Each&lt;SEP&gt;CHANT&lt;SEP&gt;had&lt;SEP&gt;four</td>\n",
       "      <td>CHANT</td>\n",
       "      <td>CHANT</td>\n",
       "      <td>c h a n t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2191</th>\n",
       "      <td>rmoured&lt;SEP&gt;and&lt;SEP&gt;CAESAR&lt;SEP&gt;155mm&lt;SEP&gt;tzers</td>\n",
       "      <td>CAESAR</td>\n",
       "      <td>CAESAR</td>\n",
       "      <td>c a e s a r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33616</th>\n",
       "      <td>;&lt;SEP&gt;15&lt;SEP&gt;APR&lt;SEP&gt;1&lt;SEP&gt;Oct 1968</td>\n",
       "      <td>APR</td>\n",
       "      <td>april</td>\n",
       "      <td>a p r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33637</th>\n",
       "      <td>&lt;SEP&gt;414-6640-3&lt;SEP&gt;EINSTEIN&lt;SEP&gt;&lt;SEP&gt;A</td>\n",
       "      <td>EINSTEIN</td>\n",
       "      <td>EINSTEIN</td>\n",
       "      <td>e i n s t e n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33705</th>\n",
       "      <td>th&lt;SEP&gt;American&lt;SEP&gt;AT&lt;SEP&gt;6&lt;SEP&gt;as</td>\n",
       "      <td>AT</td>\n",
       "      <td>AT</td>\n",
       "      <td>a t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33798</th>\n",
       "      <td>ORDER&lt;SEP&gt;,&lt;SEP&gt;REPORTA&lt;SEP&gt;&lt;SEP&gt;,</td>\n",
       "      <td>REPORTA</td>\n",
       "      <td>r e p o r t a</td>\n",
       "      <td>REPORTA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34068</th>\n",
       "      <td>L&lt;SEP&gt;and&lt;SEP&gt;OMA&lt;SEP&gt;1&lt;SEP&gt;cleave</td>\n",
       "      <td>OMA</td>\n",
       "      <td>o m a</td>\n",
       "      <td>OMA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34128</th>\n",
       "      <td>the&lt;SEP&gt;3&lt;SEP&gt;DO&lt;SEP&gt;console&lt;SEP&gt;,</td>\n",
       "      <td>DO</td>\n",
       "      <td>d o</td>\n",
       "      <td>DO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34301</th>\n",
       "      <td>&lt;SEP&gt;\"&lt;SEP&gt;BATROUNEY&lt;SEP&gt;Jennifer&lt;SEP&gt;ne</td>\n",
       "      <td>BATROUNEY</td>\n",
       "      <td>b a t r o u n e y</td>\n",
       "      <td>BATROUNEY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34344</th>\n",
       "      <td>Venture&lt;SEP&gt;(&lt;SEP&gt;AREVA&lt;SEP&gt;Resources&lt;SEP&gt;a</td>\n",
       "      <td>AREVA</td>\n",
       "      <td>AREVA</td>\n",
       "      <td>a r e v a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34367</th>\n",
       "      <td>Creation&lt;SEP&gt;at&lt;SEP&gt;AM&lt;SEP&gt;1&lt;SEP&gt;or</td>\n",
       "      <td>AM</td>\n",
       "      <td>AM</td>\n",
       "      <td>a m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34368</th>\n",
       "      <td>1&lt;SEP&gt;or&lt;SEP&gt;AM&lt;SEP&gt;2&lt;SEP&gt;?</td>\n",
       "      <td>AM</td>\n",
       "      <td>AM</td>\n",
       "      <td>a m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34422</th>\n",
       "      <td>CIRUS&lt;SEP&gt;and&lt;SEP&gt;DHRUVA&lt;SEP&gt;Reactors&lt;SEP&gt;,</td>\n",
       "      <td>DHRUVA</td>\n",
       "      <td>DHRUVA</td>\n",
       "      <td>d h r u v a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34423</th>\n",
       "      <td>&lt;SEP&gt;&lt;SEP&gt;AFCOM&lt;SEP&gt;is&lt;SEP&gt;an</td>\n",
       "      <td>AFCOM</td>\n",
       "      <td>AFCOM</td>\n",
       "      <td>a f c o m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34461</th>\n",
       "      <td>&lt;SEP&gt;&lt;SEP&gt;AIMS&lt;SEP&gt;ernational&lt;SEP&gt;</td>\n",
       "      <td>AIMS</td>\n",
       "      <td>AIMS</td>\n",
       "      <td>a i m s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34497</th>\n",
       "      <td>,&lt;SEP&gt;and&lt;SEP&gt;YACHAD&lt;SEP&gt;.&lt;SEP&gt;</td>\n",
       "      <td>YACHAD</td>\n",
       "      <td>YACHAD</td>\n",
       "      <td>y a c h a d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34503</th>\n",
       "      <td>)&lt;SEP&gt;Baldwin&lt;SEP&gt;AS&lt;SEP&gt;16&lt;SEP&gt;.</td>\n",
       "      <td>AS</td>\n",
       "      <td>AS</td>\n",
       "      <td>a s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34553</th>\n",
       "      <td>AN&lt;SEP&gt;/&lt;SEP&gt;ARC&lt;SEP&gt;5&lt;SEP&gt;control</td>\n",
       "      <td>ARC</td>\n",
       "      <td>ARC</td>\n",
       "      <td>a r c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34568</th>\n",
       "      <td>&lt;SEP&gt;The&lt;SEP&gt;ANTIC&lt;SEP&gt;screen&lt;SEP&gt;etry</td>\n",
       "      <td>ANTIC</td>\n",
       "      <td>ANTIC</td>\n",
       "      <td>a n t i c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34616</th>\n",
       "      <td>;&lt;SEP&gt;Psalm&lt;SEP&gt;IV&lt;SEP&gt;,&lt;SEP&gt;op</td>\n",
       "      <td>IV</td>\n",
       "      <td>i v</td>\n",
       "      <td>the fourth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34697</th>\n",
       "      <td>oundation&lt;SEP&gt;(&lt;SEP&gt;AUF&lt;SEP&gt;)&lt;SEP&gt;Great</td>\n",
       "      <td>AUF</td>\n",
       "      <td>AUF</td>\n",
       "      <td>a u f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34775</th>\n",
       "      <td>&lt;SEP&gt;\"&lt;SEP&gt;FASHOLA&lt;SEP&gt;APPOINTS&lt;SEP&gt;EW</td>\n",
       "      <td>FASHOLA</td>\n",
       "      <td>f a s h o l a</td>\n",
       "      <td>FASHOLA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34793</th>\n",
       "      <td>Pakistan&lt;SEP&gt;on&lt;SEP&gt;AAJ&lt;SEP&gt;TV&lt;SEP&gt;with</td>\n",
       "      <td>AAJ</td>\n",
       "      <td>AAJ</td>\n",
       "      <td>a a j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34891</th>\n",
       "      <td>John&lt;SEP&gt;(&lt;SEP&gt;PL&lt;SEP&gt;856&lt;SEP&gt;J</td>\n",
       "      <td>PL</td>\n",
       "      <td>p l</td>\n",
       "      <td>place</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35213</th>\n",
       "      <td>&lt;SEP&gt;&lt;SEP&gt;NUST&lt;SEP&gt;has&lt;SEP&gt;been</td>\n",
       "      <td>NUST</td>\n",
       "      <td>n u s t</td>\n",
       "      <td>NUST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35215</th>\n",
       "      <td>of&lt;SEP&gt;the&lt;SEP&gt;OHR&lt;SEP&gt;was&lt;SEP&gt;the</td>\n",
       "      <td>OHR</td>\n",
       "      <td>OHR</td>\n",
       "      <td>o h r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35223</th>\n",
       "      <td>Resign&lt;SEP&gt;for&lt;SEP&gt;OKA&lt;SEP&gt;\"&lt;SEP&gt;.</td>\n",
       "      <td>OKA</td>\n",
       "      <td>OKA</td>\n",
       "      <td>o k a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35277</th>\n",
       "      <td>mber 22nd&lt;SEP&gt;,&lt;SEP&gt;ORP&lt;SEP&gt;Kujawiak&lt;SEP&gt;nd</td>\n",
       "      <td>ORP</td>\n",
       "      <td>o r p</td>\n",
       "      <td>ORP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35308</th>\n",
       "      <td>&lt;SEP&gt;&lt;SEP&gt;OX&lt;SEP&gt;2010&lt;SEP&gt;:</td>\n",
       "      <td>OX</td>\n",
       "      <td>OX</td>\n",
       "      <td>o x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35388</th>\n",
       "      <td>minated&lt;SEP&gt;for&lt;SEP&gt;ASIAN&lt;SEP&gt;Media&lt;SEP&gt;wards</td>\n",
       "      <td>ASIAN</td>\n",
       "      <td>ASIAN</td>\n",
       "      <td>a s i a n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35433</th>\n",
       "      <td>Search&lt;SEP&gt;the&lt;SEP&gt;YIP&lt;SEP&gt;database&lt;SEP&gt;\"</td>\n",
       "      <td>YIP</td>\n",
       "      <td>YIP</td>\n",
       "      <td>y i p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35538</th>\n",
       "      <td>FOR&lt;SEP&gt;VIDEO&lt;SEP&gt;TECHNOLOGY&lt;SEP&gt;,&lt;SEP&gt;vol</td>\n",
       "      <td>TECHNOLOGY</td>\n",
       "      <td>TECHNOLOGY</td>\n",
       "      <td>t e c h n o l o g y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>560 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       0                1  \\\n",
       "126                  rograms<SEP><SEP>NORML<SEP>\"<SEP>.            NORML   \n",
       "143                   <SEP><SEP>USAR<SEP>task<SEP>forces             USAR   \n",
       "166                 <SEP>\"<SEP>COPYING<SEP>file<SEP>from          COPYING   \n",
       "173               ,<SEP>the<SEP>TULF<SEP>remained<SEP>he             TULF   \n",
       "207                 State<SEP>bill<SEP>AB<SEP>1519<SEP>(               AB   \n",
       "268                 <SEP><SEP>WISDOM<SEP>Television<SEP>           WISDOM   \n",
       "353                      <SEP><SEP>SHODAN<SEP>:<SEP>True           SHODAN   \n",
       "379               unit<SEP>and<SEP>ALU<SEP>into<SEP>what              ALU   \n",
       "426                           <SEP><SEP>PL<SEP>360<SEP>,               PL   \n",
       "490                    <SEP><SEP>AAG<SEP>rocomputer<SEP>              AAG   \n",
       "518    \"<SEP>Tomato<SEP>BRASSINOSTEROID<SEP>NSENSITIV...  BRASSINOSTEROID   \n",
       "730             amounts<SEP>of<SEP>CADMIUM<SEP>116<SEP>,          CADMIUM   \n",
       "991                       <SEP><SEP>FYROM<SEP>,<SEP>2011            FYROM   \n",
       "1154                    Coup<SEP>:<SEP>PEGOT<SEP>\"<SEP>.            PEGOT   \n",
       "1155                 of<SEP>5<SEP>ACS<SEP>'<SEP>projects              ACS   \n",
       "1175                        55<SEP>(<SEP>DAF<SEP>)<SEP>-              DAF   \n",
       "1418                TEST<SEP>A<SEP>DESENZANO<SEP>\"<SEP>(        DESENZANO   \n",
       "1464                   <SEP>\"<SEP>ASSEMBLEA<SEP>PD<SEP>/        ASSEMBLEA   \n",
       "1556        FINLAND<SEP>:<SEP>PERTTI<SEP>KURIKAN<SEP>VAT           PERTTI   \n",
       "1589                     .<SEP>458<SEP>GEX<SEP>268<SEP>.              GEX   \n",
       "1592                 Institute<SEP>(<SEP>LII<SEP>)<SEP>.              LII   \n",
       "1710                  <SEP><SEP>BYTE<SEP>concluded<SEP>t             BYTE   \n",
       "1735            Industry<SEP>(<SEP>MITI<SEP>)<SEP>played             MITI   \n",
       "1801        <SEP>Volleyball<SEP>XIII<SEP>World<SEP>nship             XIII   \n",
       "1907             <SEP>\"<SEP>TROPICOS<SEP>Web<SEP>display         TROPICOS   \n",
       "1941               <SEP>The<SEP>OTCO<SEP>Program<SEP>ers             OTCO   \n",
       "2001             <SEP>\"<SEP>YPSILANTI<SEP>TOWNSHIP<SEP>:        YPSILANTI   \n",
       "2021                   <SEP><SEP>KUA<SEP>holds<SEP>ility              KUA   \n",
       "2097                <SEP>Each<SEP>CHANT<SEP>had<SEP>four            CHANT   \n",
       "2191      rmoured<SEP>and<SEP>CAESAR<SEP>155mm<SEP>tzers           CAESAR   \n",
       "...                                                  ...              ...   \n",
       "33616                ;<SEP>15<SEP>APR<SEP>1<SEP>Oct 1968              APR   \n",
       "33637           <SEP>414-6640-3<SEP>EINSTEIN<SEP><SEP>A         EINSTEIN   \n",
       "33705                th<SEP>American<SEP>AT<SEP>6<SEP>as               AT   \n",
       "33798                ORDER<SEP>,<SEP>REPORTA<SEP><SEP>,          REPORTA   \n",
       "34068                 L<SEP>and<SEP>OMA<SEP>1<SEP>cleave              OMA   \n",
       "34128                 the<SEP>3<SEP>DO<SEP>console<SEP>,               DO   \n",
       "34301           <SEP>\"<SEP>BATROUNEY<SEP>Jennifer<SEP>ne        BATROUNEY   \n",
       "34344        Venture<SEP>(<SEP>AREVA<SEP>Resources<SEP>a            AREVA   \n",
       "34367                Creation<SEP>at<SEP>AM<SEP>1<SEP>or               AM   \n",
       "34368                        1<SEP>or<SEP>AM<SEP>2<SEP>?               AM   \n",
       "34422        CIRUS<SEP>and<SEP>DHRUVA<SEP>Reactors<SEP>,           DHRUVA   \n",
       "34423                      <SEP><SEP>AFCOM<SEP>is<SEP>an            AFCOM   \n",
       "34461                 <SEP><SEP>AIMS<SEP>ernational<SEP>             AIMS   \n",
       "34497                    ,<SEP>and<SEP>YACHAD<SEP>.<SEP>           YACHAD   \n",
       "34503                  )<SEP>Baldwin<SEP>AS<SEP>16<SEP>.               AS   \n",
       "34553                 AN<SEP>/<SEP>ARC<SEP>5<SEP>control              ARC   \n",
       "34568             <SEP>The<SEP>ANTIC<SEP>screen<SEP>etry            ANTIC   \n",
       "34616                    ;<SEP>Psalm<SEP>IV<SEP>,<SEP>op               IV   \n",
       "34697            oundation<SEP>(<SEP>AUF<SEP>)<SEP>Great              AUF   \n",
       "34775             <SEP>\"<SEP>FASHOLA<SEP>APPOINTS<SEP>EW          FASHOLA   \n",
       "34793            Pakistan<SEP>on<SEP>AAJ<SEP>TV<SEP>with              AAJ   \n",
       "34891                    John<SEP>(<SEP>PL<SEP>856<SEP>J               PL   \n",
       "35213                    <SEP><SEP>NUST<SEP>has<SEP>been             NUST   \n",
       "35215                 of<SEP>the<SEP>OHR<SEP>was<SEP>the              OHR   \n",
       "35223                 Resign<SEP>for<SEP>OKA<SEP>\"<SEP>.              OKA   \n",
       "35277        mber 22nd<SEP>,<SEP>ORP<SEP>Kujawiak<SEP>nd              ORP   \n",
       "35308                        <SEP><SEP>OX<SEP>2010<SEP>:               OX   \n",
       "35388      minated<SEP>for<SEP>ASIAN<SEP>Media<SEP>wards            ASIAN   \n",
       "35433          Search<SEP>the<SEP>YIP<SEP>database<SEP>\"              YIP   \n",
       "35538         FOR<SEP>VIDEO<SEP>TECHNOLOGY<SEP>,<SEP>vol       TECHNOLOGY   \n",
       "\n",
       "                                   2                    3  \n",
       "126                            NORML            n o r m l  \n",
       "143                             USAR              u s a r  \n",
       "166                          COPYING        c o p y i n g  \n",
       "173                          t u l f                 TULF  \n",
       "207                              a b                   AB  \n",
       "268                           WISDOM          w i s d o m  \n",
       "353                      s h o d a n               SHODAN  \n",
       "379                              ALU                a l u  \n",
       "426                              p l                place  \n",
       "490                              AAG                a a g  \n",
       "518    b r a s s i n o s t e r o i d      BRASSINOSTEROID  \n",
       "730                          CADMIUM        c a d m i u m  \n",
       "991                        f y r o m                FYROM  \n",
       "1154                       p e g o t                PEGOT  \n",
       "1155                           a c s                  ACS  \n",
       "1175                             DAF                d a f  \n",
       "1418               d e s e n z a n o            DESENZANO  \n",
       "1464               a s s e m b l e a            ASSEMBLEA  \n",
       "1556                     p e r t t i               PERTTI  \n",
       "1589                             GEX                g e x  \n",
       "1592                             LII                l i i  \n",
       "1710                            BYTE              b y t e  \n",
       "1735                            MITI              m i t i  \n",
       "1801                      thirteenth             thirteen  \n",
       "1907                 t r o p i c o s             TROPICOS  \n",
       "1941                            OTCO              o t c o  \n",
       "2001                       YPSILANTI    y p s i l a n t i  \n",
       "2021                           k u a                  KUA  \n",
       "2097                           CHANT            c h a n t  \n",
       "2191                          CAESAR          c a e s a r  \n",
       "...                              ...                  ...  \n",
       "33616                          april                a p r  \n",
       "33637                       EINSTEIN        e i n s t e n  \n",
       "33705                             AT                  a t  \n",
       "33798                  r e p o r t a              REPORTA  \n",
       "34068                          o m a                  OMA  \n",
       "34128                            d o                   DO  \n",
       "34301              b a t r o u n e y            BATROUNEY  \n",
       "34344                          AREVA            a r e v a  \n",
       "34367                             AM                  a m  \n",
       "34368                             AM                  a m  \n",
       "34422                         DHRUVA          d h r u v a  \n",
       "34423                          AFCOM            a f c o m  \n",
       "34461                           AIMS              a i m s  \n",
       "34497                         YACHAD          y a c h a d  \n",
       "34503                             AS                  a s  \n",
       "34553                            ARC                a r c  \n",
       "34568                          ANTIC            a n t i c  \n",
       "34616                            i v           the fourth  \n",
       "34697                            AUF                a u f  \n",
       "34775                  f a s h o l a              FASHOLA  \n",
       "34793                            AAJ                a a j  \n",
       "34891                            p l                place  \n",
       "35213                        n u s t                 NUST  \n",
       "35215                            OHR                o h r  \n",
       "35223                            OKA                o k a  \n",
       "35277                          o r p                  ORP  \n",
       "35308                             OX                  o x  \n",
       "35388                          ASIAN            a s i a n  \n",
       "35433                            YIP                y i p  \n",
       "35538                     TECHNOLOGY  t e c h n o l o g y  \n",
       "\n",
       "[560 rows x 4 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions =  score_all_letters(list(val_data_letters['before_phrase']))\n",
    "res = pd.DataFrame(zip(list(val_data_letters['before_phrase']),list(val_data_letters['before']), \n",
    "                       list(val_data_letters['after']),predictions))\n",
    "\n",
    "print(np.mean(res[2] == res[3]))\n",
    "res[res[2] != res[3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./all_rest_model_1.ckpt\n",
      "()\n",
      "Training Model: all_rest_model_1\n",
      "Epoch   1/100 Batch   50/653 - Loss:  0.137, Seconds: 55.41\n",
      "Epoch   1/100 Batch  100/653 - Loss:  0.003, Seconds: 55.18\n",
      "Epoch   1/100 Batch  150/653 - Loss:  0.002, Seconds: 55.72\n",
      "Epoch   1/100 Batch  200/653 - Loss:  0.002, Seconds: 55.01\n",
      "Epoch   1/100 Batch  250/653 - Loss:  0.002, Seconds: 57.55\n",
      "Epoch   1/100 Batch  300/653 - Loss:  0.002, Seconds: 52.47\n",
      "Epoch   1/100 Batch  350/653 - Loss:  0.002, Seconds: 60.02\n",
      "Epoch   1/100 Batch  400/653 - Loss:  0.002, Seconds: 57.79\n",
      "Epoch   1/100 Batch  450/653 - Loss:  0.002, Seconds: 61.68\n",
      "Epoch   1/100 Batch  500/653 - Loss:  0.002, Seconds: 55.10\n",
      "Epoch   1/100 Batch  550/653 - Loss:  0.002, Seconds: 62.72\n",
      "Epoch   1/100 Batch  600/653 - Loss:  0.002, Seconds: 56.18\n",
      "Epoch   1/100 Batch  650/653 - Loss:  0.002, Seconds: 57.15\n",
      "Testing Loss:  0.002, Seconds: 87.59\n",
      "New Record!\n",
      "Epoch   2/100 Batch   50/653 - Loss:  0.084, Seconds: 60.10\n",
      "Epoch   2/100 Batch  100/653 - Loss:  0.003, Seconds: 60.06\n",
      "Epoch   2/100 Batch  150/653 - Loss:  0.002, Seconds: 59.77\n",
      "Epoch   2/100 Batch  200/653 - Loss:  0.002, Seconds: 59.79\n",
      "Epoch   2/100 Batch  250/653 - Loss:  0.002, Seconds: 62.09\n",
      "Epoch   2/100 Batch  300/653 - Loss:  0.002, Seconds: 56.74\n",
      "Epoch   2/100 Batch  350/653 - Loss:  0.002, Seconds: 59.81\n",
      "Epoch   2/100 Batch  400/653 - Loss:  0.002, Seconds: 59.26\n",
      "Epoch   2/100 Batch  450/653 - Loss:  0.002, Seconds: 63.34\n",
      "Epoch   2/100 Batch  500/653 - Loss:  0.002, Seconds: 56.45\n",
      "Epoch   2/100 Batch  550/653 - Loss:  0.002, Seconds: 64.49\n",
      "Epoch   2/100 Batch  600/653 - Loss:  0.002, Seconds: 57.86\n",
      "Epoch   2/100 Batch  650/653 - Loss:  0.002, Seconds: 59.02\n",
      "Testing Loss:  0.002, Seconds: 88.97\n",
      "New Record!\n",
      "Epoch   3/100 Batch   50/653 - Loss:  0.049, Seconds: 61.65\n",
      "Epoch   3/100 Batch  100/653 - Loss:  0.002, Seconds: 61.48\n",
      "Epoch   3/100 Batch  150/653 - Loss:  0.002, Seconds: 61.76\n",
      "Epoch   3/100 Batch  200/653 - Loss:  0.002, Seconds: 61.52\n",
      "Epoch   3/100 Batch  250/653 - Loss:  0.001, Seconds: 64.05\n",
      "Epoch   3/100 Batch  300/653 - Loss:  0.001, Seconds: 58.44\n",
      "Epoch   3/100 Batch  350/653 - Loss:  0.001, Seconds: 61.38\n",
      "Epoch   3/100 Batch  400/653 - Loss:  0.001, Seconds: 61.30\n",
      "Epoch   3/100 Batch  450/653 - Loss:  0.001, Seconds: 65.44\n",
      "Epoch   3/100 Batch  500/653 - Loss:  0.001, Seconds: 58.27\n",
      "Epoch   3/100 Batch  550/653 - Loss:  0.001, Seconds: 66.22\n",
      "Epoch   3/100 Batch  600/653 - Loss:  0.001, Seconds: 59.53\n",
      "Epoch   3/100 Batch  650/653 - Loss:  0.001, Seconds: 60.77\n",
      "Testing Loss:  0.002, Seconds: 91.28\n",
      "New Record!\n",
      "Epoch   4/100 Batch   50/653 - Loss:  0.035, Seconds: 63.65\n",
      "Epoch   4/100 Batch  100/653 - Loss:  0.002, Seconds: 63.38\n",
      "Epoch   4/100 Batch  150/653 - Loss:  0.001, Seconds: 63.67\n",
      "Epoch   4/100 Batch  200/653 - Loss:  0.001, Seconds: 63.06\n",
      "Epoch   4/100 Batch  250/653 - Loss:  0.001, Seconds: 65.90\n",
      "Epoch   4/100 Batch  300/653 - Loss:  0.001, Seconds: 60.08\n",
      "Epoch   4/100 Batch  350/653 - Loss:  0.001, Seconds: 67.21\n",
      "Epoch   4/100 Batch  400/653 - Loss:  0.001, Seconds: 65.08\n",
      "Epoch   4/100 Batch  450/653 - Loss:  0.001, Seconds: 67.28\n",
      "Epoch   4/100 Batch  500/653 - Loss:  0.001, Seconds: 60.38\n",
      "Epoch   4/100 Batch  550/653 - Loss:  0.001, Seconds: 68.58\n",
      "Epoch   4/100 Batch  600/653 - Loss:  0.001, Seconds: 61.63\n",
      "Epoch   4/100 Batch  650/653 - Loss:  0.001, Seconds: 62.97\n",
      "Testing Loss:  0.002, Seconds: 93.41\n",
      "New Record!\n",
      "Epoch   5/100 Batch   50/653 - Loss:  0.034, Seconds: 66.80\n",
      "Epoch   5/100 Batch  100/653 - Loss:  0.002, Seconds: 67.55\n",
      "Epoch   5/100 Batch  150/653 - Loss:  0.001, Seconds: 66.16\n",
      "Epoch   5/100 Batch  200/653 - Loss:  0.001, Seconds: 68.37\n",
      "Epoch   5/100 Batch  250/653 - Loss:  0.001, Seconds: 67.61\n",
      "Epoch   5/100 Batch  300/653 - Loss:  0.001, Seconds: 63.36\n",
      "Epoch   5/100 Batch  350/653 - Loss:  0.001, Seconds: 68.10\n",
      "Epoch   5/100 Batch  400/653 - Loss:  0.001, Seconds: 65.16\n",
      "Epoch   5/100 Batch  450/653 - Loss:  0.001, Seconds: 69.29\n",
      "Epoch   5/100 Batch  500/653 - Loss:  0.001, Seconds: 62.41\n",
      "Epoch   5/100 Batch  550/653 - Loss:  0.001, Seconds: 71.45\n",
      "Epoch   5/100 Batch  600/653 - Loss:  0.001, Seconds: 63.12\n",
      "Epoch   5/100 Batch  650/653 - Loss:  0.001, Seconds: 64.25\n",
      "Testing Loss:  0.002, Seconds: 95.57\n",
      "New Record!\n",
      "Epoch   6/100 Batch   50/653 - Loss:  0.037, Seconds: 67.00\n",
      "Epoch   6/100 Batch  100/653 - Loss:  0.002, Seconds: 66.70\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-143-26f83fa854fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m                     learning_rate, embedding_size, direction, char_to_int_rest)\n\u001b[1;32m     13\u001b[0m train(model_letters, epochs, log_string, int_before_dev_rest, int_after_dev_rest, \n\u001b[0;32m---> 14\u001b[0;31m       int_before_val_rest, int_after_val_rest, char_to_int_rest, reset = False)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-16-443e5b783dbd>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, epochs, log_string, int_before_train, int_after_train, int_before_test, int_after_test, char_to_int, reset, early_stopping)\u001b[0m\n\u001b[1;32m    112\u001b[0m                                               \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minput_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m                                               \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtargets_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtarget_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m                                               model.keep_prob: keep_probability})\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sourabhjha/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sourabhjha/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sourabhjha/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/sourabhjha/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sourabhjha/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "batch_size = 400\n",
    "num_layers = 3\n",
    "rnn_size = 128\n",
    "embedding_size = len(chars_rest)\n",
    "learning_rate = 0.0004\n",
    "direction = 2\n",
    "keep_probability = 0.75\n",
    "\n",
    "log_string = 'all_rest_model_1'\n",
    "model_letters = build_graph(keep_probability, rnn_size, num_layers, batch_size, \n",
    "                    learning_rate, embedding_size, direction, char_to_int_rest)\n",
    "train(model_letters, epochs, log_string, int_before_dev_rest, int_after_dev_rest, \n",
    "      int_before_val_rest, int_after_val_rest, char_to_int_rest, reset = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "batch_size = 300\n",
    "num_layers = 3\n",
    "rnn_size = 128\n",
    "embedding_size = len(chars_rest)\n",
    "learning_rate = 0.0005\n",
    "direction = 2\n",
    "keep_probability = 0.75\n",
    "\n",
    "def token_to_ints_rest(token):\n",
    "    return [char_to_int_rest[c] for c in token if c in chars_rest]\n",
    "\n",
    "\n",
    "def score_all_rest(phrases, max_target_length = 150):\n",
    "    checkpoint = \"./all_rest_model_1.ckpt\"\n",
    "    epochs = 100\n",
    "    batch_size = 500\n",
    "    num_layers = 3\n",
    "    rnn_size = 128\n",
    "    embedding_size = len(chars_rest)\n",
    "    learning_rate = 0.0005\n",
    "    direction = 2\n",
    "    keep_probability = 0.75\n",
    "#     tokens1 = [token_to_ints(prev_token) + [char_to_int['<SEP>']] + token_to_ints(token) \n",
    "#                for prev_token, token in zip(prev_tokens, tokens)] \n",
    "    tokens1 = [reduce(lambda a,b : a + b, [token_to_ints_rest(p) + [char_to_int_rest['<SEP>']] \n",
    "                                           for p in phrase.split(\"<SEP>\")])[:-1] for phrase in phrases] \n",
    "    \n",
    "    pad_tokens = pad_sentence_batch(tokens1, char_to_int_rest)\n",
    "        \n",
    "        # Need the lengths for the _lengths parameters\n",
    "    pad_tokens_lengths = []\n",
    "    pad_targets_lengths = []\n",
    "    for token in pad_tokens:\n",
    "        pad_tokens_lengths.append(len(token))    \n",
    "        pad_targets_lengths.append(len(token) + max_target_length)\n",
    "    model = build_graph(keep_probability, rnn_size, num_layers, len(phrases), \n",
    "                                learning_rate, embedding_size, direction, char_to_int_rest)\n",
    "    with tf.Session() as sess:\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess, checkpoint)\n",
    "        \n",
    "        output_logits = sess.run(model.predictions, {model.inputs: pad_tokens,\n",
    "                                                    model.inputs_length: pad_tokens_lengths,\n",
    "                                                    model.targets_length: [max_target_length],\n",
    "                                                    model.keep_prob: 1.0})\n",
    "    pad = char_to_int_rest['<PAD>']\n",
    "    return [\"\".join([int_to_char_rest[i] for i in output_logit if i != pad]).replace(\"<EOS>\",\"\").replace(\"<SELF>\", phrases[j].split(\"<SEP>\")[2])\n",
    "            for j, output_logit in enumerate(output_logits)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./all_rest_model_1.ckpt\n",
      "0.992082641046\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>passports&lt;SEP&gt;,&lt;SEP&gt;id's&lt;SEP&gt;,&lt;SEP&gt;ipods</td>\n",
       "      <td>id's</td>\n",
       "      <td>id's</td>\n",
       "      <td>i d's</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>6&lt;SEP&gt;Ferdinand&lt;SEP&gt;I&lt;SEP&gt;,&lt;SEP&gt;Holy</td>\n",
       "      <td>I</td>\n",
       "      <td>the first</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>&lt;SEP&gt;oncaptivos&lt;SEP&gt;meos&lt;SEP&gt;,&lt;SEP&gt;qui</td>\n",
       "      <td>meos</td>\n",
       "      <td>m e o s</td>\n",
       "      <td>meos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>&lt;SEP&gt;Nacionalna&lt;SEP&gt;ili&lt;SEP&gt;etnicka&lt;SEP&gt;ost</td>\n",
       "      <td>ili</td>\n",
       "      <td>i l i</td>\n",
       "      <td>ili</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>the&lt;SEP&gt;protein&lt;SEP&gt;junctophilin-&lt;SEP&gt;3&lt;SEP&gt;.</td>\n",
       "      <td>junctophilin-</td>\n",
       "      <td>j u n c t o p h i l i n</td>\n",
       "      <td>j u n c t o p p i l i n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>&lt;SEP&gt;2003&lt;SEP&gt;-&lt;SEP&gt;2004&lt;SEP&gt;:</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446</th>\n",
       "      <td>illiliter&lt;SEP&gt;(&lt;SEP&gt;mIU&lt;SEP&gt;/&lt;SEP&gt;ml</td>\n",
       "      <td>mIU</td>\n",
       "      <td>mIU</td>\n",
       "      <td>m i u</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>nal&lt;SEP&gt;Fantasy&lt;SEP&gt;IV's&lt;SEP&gt;Kain&lt;SEP&gt;ghwind</td>\n",
       "      <td>IV's</td>\n",
       "      <td>four's</td>\n",
       "      <td>the fourth's</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>nal&lt;SEP&gt;Fantasy&lt;SEP&gt;XIII's&lt;SEP&gt;Lightning&lt;SEP&gt;.</td>\n",
       "      <td>XIII's</td>\n",
       "      <td>thirteen's</td>\n",
       "      <td>the eigthenth's</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576</th>\n",
       "      <td>nvented&lt;SEP&gt;the&lt;SEP&gt;vaporised&lt;SEP&gt;oil&lt;SEP&gt;burner</td>\n",
       "      <td>vaporised</td>\n",
       "      <td>vaporised</td>\n",
       "      <td>vaporized</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>849</th>\n",
       "      <td>l&lt;SEP&gt;fricative&lt;SEP&gt;/h&lt;SEP&gt;/&lt;SEP&gt;is</td>\n",
       "      <td>/h</td>\n",
       "      <td>per hour</td>\n",
       "      <td>per hour houre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>854</th>\n",
       "      <td>third&lt;SEP&gt;in&lt;SEP&gt;UGO's&lt;SEP&gt;list&lt;SEP&gt;of</td>\n",
       "      <td>UGO's</td>\n",
       "      <td>UGO's</td>\n",
       "      <td>u g o's</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>973</th>\n",
       "      <td>WTA&lt;SEP&gt;Tier&lt;SEP&gt;I&lt;SEP&gt;Series&lt;SEP&gt;dule</td>\n",
       "      <td>I</td>\n",
       "      <td>one</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1196</th>\n",
       "      <td>etween&lt;SEP&gt;1999&lt;SEP&gt;-&lt;SEP&gt;2000&lt;SEP&gt;,</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1216</th>\n",
       "      <td>&lt;SEP&gt;i&lt;SEP&gt;nukupuu&lt;SEP&gt;(&lt;SEP&gt;mignathus</td>\n",
       "      <td>nukupuu</td>\n",
       "      <td>nukupuu</td>\n",
       "      <td>nukupuus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1218</th>\n",
       "      <td>species&lt;SEP&gt;of&lt;SEP&gt;nukupuu&lt;SEP&gt;once&lt;SEP&gt;found</td>\n",
       "      <td>nukupuu</td>\n",
       "      <td>nukupuu</td>\n",
       "      <td>nukupuuus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1358</th>\n",
       "      <td>satay&lt;SEP&gt;bilad&lt;SEP&gt;maar&lt;SEP&gt;ah&lt;SEP&gt;tartanka</td>\n",
       "      <td>maar</td>\n",
       "      <td>maar</td>\n",
       "      <td>m a a r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1478</th>\n",
       "      <td>&lt;SEP&gt;\"&lt;SEP&gt;JAFFA'S&lt;SEP&gt;NEW&lt;SEP&gt;CINEMA</td>\n",
       "      <td>JAFFA'S</td>\n",
       "      <td>JAFFA'S</td>\n",
       "      <td>j a f f a s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1612</th>\n",
       "      <td>.&lt;SEP&gt;22&lt;SEP&gt;-&lt;SEP&gt;24&lt;SEP&gt;(</td>\n",
       "      <td>-</td>\n",
       "      <td>to</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1719</th>\n",
       "      <td>,&lt;SEP&gt;1930&lt;SEP&gt;-&lt;SEP&gt;31&lt;SEP&gt;.</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1889</th>\n",
       "      <td>,&lt;SEP&gt;Kam&lt;SEP&gt;biu&lt;SEP&gt;;&lt;SEP&gt;Fearn</td>\n",
       "      <td>biu</td>\n",
       "      <td>b i u</td>\n",
       "      <td>biu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2112</th>\n",
       "      <td>of&lt;SEP&gt;Kani&lt;SEP&gt;Kli&lt;SEP&gt;on&lt;SEP&gt;Mayotte</td>\n",
       "      <td>Kli</td>\n",
       "      <td>k e acute l i</td>\n",
       "      <td>Kli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2461</th>\n",
       "      <td>(&lt;SEP&gt;1984&lt;SEP&gt;-&lt;SEP&gt;1985&lt;SEP&gt;)</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2509</th>\n",
       "      <td>&lt;SEP&gt;It&lt;SEP&gt;ihas&lt;SEP&gt;been&lt;SEP&gt;found</td>\n",
       "      <td>ihas</td>\n",
       "      <td>i h a s</td>\n",
       "      <td>ihas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2534</th>\n",
       "      <td>India&lt;SEP&gt;1911&lt;SEP&gt;-&lt;SEP&gt;1912&lt;SEP&gt;.</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2576</th>\n",
       "      <td>vision&lt;SEP&gt;1991&lt;SEP&gt;-&lt;SEP&gt;1992&lt;SEP&gt;:</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2703</th>\n",
       "      <td>1991&lt;SEP&gt;,&lt;SEP&gt;Tlcom&lt;SEP&gt;ParisTech&lt;SEP&gt;d</td>\n",
       "      <td>Tlcom</td>\n",
       "      <td>t e acute l e acute c o m</td>\n",
       "      <td>Tlcom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3163</th>\n",
       "      <td>winter&lt;SEP&gt;1973&lt;SEP&gt;-&lt;SEP&gt;1974&lt;SEP&gt;Paris</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3261</th>\n",
       "      <td>/&lt;SEP&gt;&lt;SEP&gt;aen&lt;SEP&gt;.&lt;SEP&gt;t</td>\n",
       "      <td>aen</td>\n",
       "      <td>a e n</td>\n",
       "      <td>aen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3328</th>\n",
       "      <td>uistics&lt;SEP&gt;3.1&lt;SEP&gt;:&lt;SEP&gt;79&lt;SEP&gt;-</td>\n",
       "      <td>:</td>\n",
       "      <td>:</td>\n",
       "      <td>to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64787</th>\n",
       "      <td>world&lt;SEP&gt;war&lt;SEP&gt;I&lt;SEP&gt;(&lt;SEP&gt;1st</td>\n",
       "      <td>I</td>\n",
       "      <td>one</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64808</th>\n",
       "      <td>term&lt;SEP&gt;na&lt;SEP&gt;reu&lt;SEP&gt;sha&lt;SEP&gt;,</td>\n",
       "      <td>reu</td>\n",
       "      <td>r e u</td>\n",
       "      <td>reu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64809</th>\n",
       "      <td>na&lt;SEP&gt;reu&lt;SEP&gt;sha&lt;SEP&gt;,&lt;SEP&gt;which</td>\n",
       "      <td>sha</td>\n",
       "      <td>sha</td>\n",
       "      <td>s h a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64873</th>\n",
       "      <td>of&lt;SEP&gt;Menander&lt;SEP&gt;I&lt;SEP&gt;and&lt;SEP&gt;other</td>\n",
       "      <td>I</td>\n",
       "      <td>the first</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64978</th>\n",
       "      <td>ith&lt;SEP&gt;Pakhtun&lt;SEP&gt;khwa&lt;SEP&gt;Milli&lt;SEP&gt;Awami</td>\n",
       "      <td>khwa</td>\n",
       "      <td>k h w a</td>\n",
       "      <td>khwa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65203</th>\n",
       "      <td>sal&lt;SEP&gt;Channel&lt;SEP&gt;zeigt&lt;SEP&gt;die&lt;SEP&gt;Serie</td>\n",
       "      <td>zeigt</td>\n",
       "      <td>zeigt</td>\n",
       "      <td>z e i g t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65616</th>\n",
       "      <td>etwork&lt;SEP&gt;also&lt;SEP&gt;televises&lt;SEP&gt;the&lt;SEP&gt;rangler</td>\n",
       "      <td>televises</td>\n",
       "      <td>televises</td>\n",
       "      <td>televizes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65690</th>\n",
       "      <td>PVC-&lt;SEP&gt;170&lt;SEP&gt;-&lt;SEP&gt;5&lt;SEP&gt;Boats</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65789</th>\n",
       "      <td>&lt;SEP&gt;Pokhoronen&lt;SEP&gt;byl&lt;SEP&gt;dvazhdy&lt;SEP&gt;ivo</td>\n",
       "      <td>byl</td>\n",
       "      <td>byl</td>\n",
       "      <td>b y l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65822</th>\n",
       "      <td>&lt;SEP&gt;86&lt;SEP&gt;-&lt;SEP&gt;89&lt;SEP&gt;Norton</td>\n",
       "      <td>-</td>\n",
       "      <td>to</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65846</th>\n",
       "      <td>&lt;SEP&gt;22&lt;SEP&gt;-&lt;SEP&gt;57&lt;SEP&gt;,</td>\n",
       "      <td>-</td>\n",
       "      <td>to</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66002</th>\n",
       "      <td>&lt;SEP&gt;Thihathura&lt;SEP&gt;I&lt;SEP&gt;succeeded&lt;SEP&gt;e</td>\n",
       "      <td>I</td>\n",
       "      <td>the first</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66040</th>\n",
       "      <td>Pope&lt;SEP&gt;Adrian&lt;SEP&gt;I&lt;SEP&gt;grants&lt;SEP&gt;him</td>\n",
       "      <td>I</td>\n",
       "      <td>the first</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66242</th>\n",
       "      <td>women&lt;SEP&gt;de&lt;SEP&gt;qiji&lt;SEP&gt;haozhuang&lt;SEP&gt;,</td>\n",
       "      <td>qiji</td>\n",
       "      <td>qiji</td>\n",
       "      <td>qijiecute</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66254</th>\n",
       "      <td>onctions&lt;SEP&gt;et&lt;SEP&gt;ses&lt;SEP&gt;maladies&lt;SEP&gt;.</td>\n",
       "      <td>ses</td>\n",
       "      <td>s e s</td>\n",
       "      <td>ses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66307</th>\n",
       "      <td>2016&lt;SEP&gt;)&lt;SEP&gt;lessthan3.com&lt;SEP&gt;:&lt;SEP&gt;Grammy</td>\n",
       "      <td>lessthan3.com</td>\n",
       "      <td>l e s s t h a n t h r e e dot c o m</td>\n",
       "      <td>l e s s t h a n t h t e n t h t r e t h n t e ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66639</th>\n",
       "      <td>century&lt;SEP&gt;the&lt;SEP&gt;jekt&lt;SEP&gt;had&lt;SEP&gt;been</td>\n",
       "      <td>jekt</td>\n",
       "      <td>jekt</td>\n",
       "      <td>j e k t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66935</th>\n",
       "      <td>t&lt;SEP&gt;itchyhcud&lt;SEP&gt;hxangd&lt;SEP&gt;nauseab&lt;SEP&gt;.</td>\n",
       "      <td>hxangd</td>\n",
       "      <td>h x a n g d</td>\n",
       "      <td>hxangd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67017</th>\n",
       "      <td>&lt;SEP&gt;46&lt;SEP&gt;-&lt;SEP&gt;55&lt;SEP&gt;Ammann</td>\n",
       "      <td>-</td>\n",
       "      <td>to</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67111</th>\n",
       "      <td>stival&lt;SEP&gt;2011&lt;SEP&gt;-&lt;SEP&gt;2012&lt;SEP&gt;will</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67544</th>\n",
       "      <td>any&lt;SEP&gt;of&lt;SEP&gt;ALF's&lt;SEP&gt;production&lt;SEP&gt;</td>\n",
       "      <td>ALF's</td>\n",
       "      <td>ALF's</td>\n",
       "      <td>a l f's</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67917</th>\n",
       "      <td>Class&lt;SEP&gt;4&lt;SEP&gt;:&lt;SEP&gt;4&lt;SEP&gt;:</td>\n",
       "      <td>:</td>\n",
       "      <td>:</td>\n",
       "      <td>to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67996</th>\n",
       "      <td>,&lt;SEP&gt;1948&lt;SEP&gt;-&lt;SEP&gt;49&lt;SEP&gt;\"</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68123</th>\n",
       "      <td>.&lt;SEP&gt;87&lt;SEP&gt;-&lt;SEP&gt;89&lt;SEP&gt;.</td>\n",
       "      <td>-</td>\n",
       "      <td>to</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68358</th>\n",
       "      <td>\"&lt;SEP&gt;Odprti&lt;SEP&gt;kop&lt;SEP&gt;:&lt;SEP&gt;Pogled</td>\n",
       "      <td>kop</td>\n",
       "      <td>k o p</td>\n",
       "      <td>kop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68424</th>\n",
       "      <td>nd&lt;SEP&gt;cerebral&lt;SEP&gt;localisations&lt;SEP&gt;.&lt;SEP&gt;</td>\n",
       "      <td>localisations</td>\n",
       "      <td>localisations</td>\n",
       "      <td>localizations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68843</th>\n",
       "      <td>;&lt;SEP&gt;14&lt;SEP&gt;-&lt;SEP&gt;15&lt;SEP&gt;.</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68915</th>\n",
       "      <td>able&lt;SEP&gt;to&lt;SEP&gt;neutralise&lt;SEP&gt;enemies&lt;SEP&gt;by</td>\n",
       "      <td>neutralise</td>\n",
       "      <td>neutralize</td>\n",
       "      <td>neuttralize</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69051</th>\n",
       "      <td>(&lt;SEP&gt;Drama&lt;SEP&gt;ver&lt;SEP&gt;.&lt;SEP&gt;</td>\n",
       "      <td>ver</td>\n",
       "      <td>ver</td>\n",
       "      <td>v e r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69109</th>\n",
       "      <td>Moon&lt;SEP&gt;(&lt;SEP&gt;NO)&lt;SEP&gt;\"&lt;SEP&gt;(</td>\n",
       "      <td>NO)</td>\n",
       "      <td>nitrogen monoxide</td>\n",
       "      <td>nitrogers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>548 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       0              1  \\\n",
       "43              passports<SEP>,<SEP>id's<SEP>,<SEP>ipods           id's   \n",
       "60                  6<SEP>Ferdinand<SEP>I<SEP>,<SEP>Holy              I   \n",
       "147               <SEP>oncaptivos<SEP>meos<SEP>,<SEP>qui           meos   \n",
       "354          <SEP>Nacionalna<SEP>ili<SEP>etnicka<SEP>ost            ili   \n",
       "366        the<SEP>protein<SEP>junctophilin-<SEP>3<SEP>.  junctophilin-   \n",
       "441                       <SEP>2003<SEP>-<SEP>2004<SEP>:              -   \n",
       "446                 illiliter<SEP>(<SEP>mIU<SEP>/<SEP>ml            mIU   \n",
       "480         nal<SEP>Fantasy<SEP>IV's<SEP>Kain<SEP>ghwind           IV's   \n",
       "481       nal<SEP>Fantasy<SEP>XIII's<SEP>Lightning<SEP>.         XIII's   \n",
       "576     nvented<SEP>the<SEP>vaporised<SEP>oil<SEP>burner      vaporised   \n",
       "849                  l<SEP>fricative<SEP>/h<SEP>/<SEP>is             /h   \n",
       "854               third<SEP>in<SEP>UGO's<SEP>list<SEP>of          UGO's   \n",
       "973               WTA<SEP>Tier<SEP>I<SEP>Series<SEP>dule              I   \n",
       "1196                etween<SEP>1999<SEP>-<SEP>2000<SEP>,              -   \n",
       "1216            <SEP>i<SEP>nukupuu<SEP>(<SEP>mignathus       nukupuu   \n",
       "1218      species<SEP>of<SEP>nukupuu<SEP>once<SEP>found       nukupuu   \n",
       "1358        satay<SEP>bilad<SEP>maar<SEP>ah<SEP>tartanka           maar   \n",
       "1478               <SEP>\"<SEP>JAFFA'S<SEP>NEW<SEP>CINEMA        JAFFA'S   \n",
       "1612                         .<SEP>22<SEP>-<SEP>24<SEP>(              -   \n",
       "1719                       ,<SEP>1930<SEP>-<SEP>31<SEP>.              -   \n",
       "1889                   ,<SEP>Kam<SEP>biu<SEP>;<SEP>Fearn            biu   \n",
       "2112             of<SEP>Kani<SEP>Kli<SEP>on<SEP>Mayotte           Kli   \n",
       "2461                     (<SEP>1984<SEP>-<SEP>1985<SEP>)              -   \n",
       "2509                 <SEP>It<SEP>ihas<SEP>been<SEP>found           ihas   \n",
       "2534                 India<SEP>1911<SEP>-<SEP>1912<SEP>.              -   \n",
       "2576                vision<SEP>1991<SEP>-<SEP>1992<SEP>:              -   \n",
       "2703          1991<SEP>,<SEP>Tlcom<SEP>ParisTech<SEP>d        Tlcom   \n",
       "3163            winter<SEP>1973<SEP>-<SEP>1974<SEP>Paris              -   \n",
       "3261                         /<SEP><SEP>aen<SEP>.<SEP>t            aen   \n",
       "3328                  uistics<SEP>3.1<SEP>:<SEP>79<SEP>-              :   \n",
       "...                                                  ...            ...   \n",
       "64787                  world<SEP>war<SEP>I<SEP>(<SEP>1st              I   \n",
       "64808                  term<SEP>na<SEP>reu<SEP>sha<SEP>,            reu   \n",
       "64809                 na<SEP>reu<SEP>sha<SEP>,<SEP>which            sha   \n",
       "64873            of<SEP>Menander<SEP>I<SEP>and<SEP>other              I   \n",
       "64978       ith<SEP>Pakhtun<SEP>khwa<SEP>Milli<SEP>Awami           khwa   \n",
       "65203        sal<SEP>Channel<SEP>zeigt<SEP>die<SEP>Serie          zeigt   \n",
       "65616  etwork<SEP>also<SEP>televises<SEP>the<SEP>rangler      televises   \n",
       "65690                 PVC-<SEP>170<SEP>-<SEP>5<SEP>Boats              -   \n",
       "65789        <SEP>Pokhoronen<SEP>byl<SEP>dvazhdy<SEP>ivo            byl   \n",
       "65822                    <SEP>86<SEP>-<SEP>89<SEP>Norton              -   \n",
       "65846                         <SEP>22<SEP>-<SEP>57<SEP>,              -   \n",
       "66002          <SEP>Thihathura<SEP>I<SEP>succeeded<SEP>e              I   \n",
       "66040           Pope<SEP>Adrian<SEP>I<SEP>grants<SEP>him              I   \n",
       "66242         women<SEP>de<SEP>qiji<SEP>haozhuang<SEP>,          qiji   \n",
       "66254         onctions<SEP>et<SEP>ses<SEP>maladies<SEP>.            ses   \n",
       "66307      2016<SEP>)<SEP>lessthan3.com<SEP>:<SEP>Grammy  lessthan3.com   \n",
       "66639          century<SEP>the<SEP>jekt<SEP>had<SEP>been           jekt   \n",
       "66935       t<SEP>itchyhcud<SEP>hxangd<SEP>nauseab<SEP>.         hxangd   \n",
       "67017                    <SEP>46<SEP>-<SEP>55<SEP>Ammann              -   \n",
       "67111            stival<SEP>2011<SEP>-<SEP>2012<SEP>will              -   \n",
       "67544           any<SEP>of<SEP>ALF's<SEP>production<SEP>          ALF's   \n",
       "67917                      Class<SEP>4<SEP>:<SEP>4<SEP>:              :   \n",
       "67996                      ,<SEP>1948<SEP>-<SEP>49<SEP>\"              -   \n",
       "68123                        .<SEP>87<SEP>-<SEP>89<SEP>.              -   \n",
       "68358              \"<SEP>Odprti<SEP>kop<SEP>:<SEP>Pogled            kop   \n",
       "68424       nd<SEP>cerebral<SEP>localisations<SEP>.<SEP>  localisations   \n",
       "68843                        ;<SEP>14<SEP>-<SEP>15<SEP>.              -   \n",
       "68915      able<SEP>to<SEP>neutralise<SEP>enemies<SEP>by     neutralise   \n",
       "69051                     (<SEP>Drama<SEP>ver<SEP>.<SEP>            ver   \n",
       "69109                     Moon<SEP>(<SEP>NO)<SEP>\"<SEP>(            NO)   \n",
       "\n",
       "                                         2  \\\n",
       "43                                    id's   \n",
       "60                               the first   \n",
       "147                                m e o s   \n",
       "354                                  i l i   \n",
       "366                j u n c t o p h i l i n   \n",
       "441                                      -   \n",
       "446                                    mIU   \n",
       "480                                 four's   \n",
       "481                             thirteen's   \n",
       "576                              vaporised   \n",
       "849                               per hour   \n",
       "854                                  UGO's   \n",
       "973                                    one   \n",
       "1196                                     -   \n",
       "1216                               nukupuu   \n",
       "1218                               nukupuu   \n",
       "1358                                  maar   \n",
       "1478                               JAFFA'S   \n",
       "1612                                    to   \n",
       "1719                                     -   \n",
       "1889                                 b i u   \n",
       "2112                         k e acute l i   \n",
       "2461                                     -   \n",
       "2509                               i h a s   \n",
       "2534                                     -   \n",
       "2576                                     -   \n",
       "2703             t e acute l e acute c o m   \n",
       "3163                                     -   \n",
       "3261                                 a e n   \n",
       "3328                                     :   \n",
       "...                                    ...   \n",
       "64787                                  one   \n",
       "64808                                r e u   \n",
       "64809                                  sha   \n",
       "64873                            the first   \n",
       "64978                              k h w a   \n",
       "65203                                zeigt   \n",
       "65616                            televises   \n",
       "65690                                    -   \n",
       "65789                                  byl   \n",
       "65822                                   to   \n",
       "65846                                   to   \n",
       "66002                            the first   \n",
       "66040                            the first   \n",
       "66242                                qiji   \n",
       "66254                                s e s   \n",
       "66307  l e s s t h a n t h r e e dot c o m   \n",
       "66639                                 jekt   \n",
       "66935                          h x a n g d   \n",
       "67017                                   to   \n",
       "67111                                    -   \n",
       "67544                                ALF's   \n",
       "67917                                    :   \n",
       "67996                                    -   \n",
       "68123                                   to   \n",
       "68358                                k o p   \n",
       "68424                        localisations   \n",
       "68843                                    -   \n",
       "68915                           neutralize   \n",
       "69051                                  ver   \n",
       "69109                    nitrogen monoxide   \n",
       "\n",
       "                                                       3  \n",
       "43                                                 i d's  \n",
       "60                                                     I  \n",
       "147                                                 meos  \n",
       "354                                                  ili  \n",
       "366                              j u n c t o p p i l i n  \n",
       "441                                                   to  \n",
       "446                                                m i u  \n",
       "480                                         the fourth's  \n",
       "481                                      the eigthenth's  \n",
       "576                                            vaporized  \n",
       "849                                       per hour houre  \n",
       "854                                              u g o's  \n",
       "973                                                    I  \n",
       "1196                                                  to  \n",
       "1216                                            nukupuus  \n",
       "1218                                           nukupuuus  \n",
       "1358                                             m a a r  \n",
       "1478                                         j a f f a s  \n",
       "1612                                                   -  \n",
       "1719                                                  to  \n",
       "1889                                                 biu  \n",
       "2112                                                Kli  \n",
       "2461                                                  to  \n",
       "2509                                                ihas  \n",
       "2534                                                  to  \n",
       "2576                                                  to  \n",
       "2703                                             Tlcom  \n",
       "3163                                                  to  \n",
       "3261                                                 aen  \n",
       "3328                                                  to  \n",
       "...                                                  ...  \n",
       "64787                                                  I  \n",
       "64808                                                reu  \n",
       "64809                                              s h a  \n",
       "64873                                                  I  \n",
       "64978                                               khwa  \n",
       "65203                                          z e i g t  \n",
       "65616                                          televizes  \n",
       "65690                                                 to  \n",
       "65789                                              b y l  \n",
       "65822                                                  -  \n",
       "65846                                                  -  \n",
       "66002                                                  I  \n",
       "66040                                                  I  \n",
       "66242                                          qijiecute  \n",
       "66254                                                ses  \n",
       "66307  l e s s t h a n t h t e n t h t r e t h n t e ...  \n",
       "66639                                            j e k t  \n",
       "66935                                             hxangd  \n",
       "67017                                                  -  \n",
       "67111                                                 to  \n",
       "67544                                            a l f's  \n",
       "67917                                                 to  \n",
       "67996                                                 to  \n",
       "68123                                                  -  \n",
       "68358                                                kop  \n",
       "68424                                      localizations  \n",
       "68843                                                 to  \n",
       "68915                                        neuttralize  \n",
       "69051                                              v e r  \n",
       "69109                                          nitrogers  \n",
       "\n",
       "[548 rows x 4 columns]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions =  score_all_rest(list(val_rest['before_phrase']))\n",
    "res = pd.DataFrame(zip(list(val_rest['before_phrase']),list(val_rest['before']), \n",
    "                       list(val_rest['after']),predictions))\n",
    "\n",
    "print(np.mean(res[2] == res[3]))\n",
    "res[res[2] != res[3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98249961282329257"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(res[res[1] == \":\"][2] == res[res[1] == \":\"][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./all_proper_nouns_model_1.ckpt\n",
      "()\n",
      "Training Model: all_proper_nouns_model_1\n",
      "Epoch   1/100 Batch   50/821 - Loss:  1.234, Seconds: 72.85\n",
      "Epoch   1/100 Batch  100/821 - Loss:  0.004, Seconds: 76.54\n",
      "Epoch   1/100 Batch  150/821 - Loss:  0.004, Seconds: 71.14\n",
      "Epoch   1/100 Batch  200/821 - Loss:  0.003, Seconds: 68.99\n",
      "Epoch   1/100 Batch  250/821 - Loss:  0.003, Seconds: 74.87\n",
      "Epoch   1/100 Batch  300/821 - Loss:  0.002, Seconds: 92.74\n",
      "Epoch   1/100 Batch  350/821 - Loss:  0.002, Seconds: 77.37\n",
      "Epoch   1/100 Batch  400/821 - Loss:  0.002, Seconds: 78.97\n",
      "Epoch   1/100 Batch  450/821 - Loss:  0.002, Seconds: 73.31\n",
      "Epoch   1/100 Batch  500/821 - Loss:  0.002, Seconds: 73.01\n",
      "Epoch   1/100 Batch  550/821 - Loss:  0.002, Seconds: 81.09\n",
      "Epoch   1/100 Batch  600/821 - Loss:  0.002, Seconds: 76.37\n",
      "Epoch   1/100 Batch  650/821 - Loss:  0.002, Seconds: 74.63\n",
      "Epoch   1/100 Batch  700/821 - Loss:  0.002, Seconds: 75.42\n",
      "Epoch   1/100 Batch  750/821 - Loss:  0.002, Seconds: 78.07\n",
      "Epoch   1/100 Batch  800/821 - Loss:  0.001, Seconds: 69.02\n",
      "Testing Loss:  0.002, Seconds: 135.56\n",
      "New Record!\n",
      "Epoch   2/100 Batch   50/821 - Loss:  0.059, Seconds: 77.93\n",
      "Epoch   2/100 Batch  100/821 - Loss:  0.002, Seconds: 83.00\n",
      "Epoch   2/100 Batch  150/821 - Loss:  0.002, Seconds: 77.35\n",
      "Epoch   2/100 Batch  200/821 - Loss:  0.001, Seconds: 75.13\n",
      "Epoch   2/100 Batch  250/821 - Loss:  0.001, Seconds: 81.29\n",
      "Epoch   2/100 Batch  300/821 - Loss:  0.002, Seconds: 92.58\n",
      "Epoch   2/100 Batch  350/821 - Loss:  0.001, Seconds: 78.31\n",
      "Epoch   2/100 Batch  400/821 - Loss:  0.001, Seconds: 80.54\n",
      "Epoch   2/100 Batch  450/821 - Loss:  0.001, Seconds: 75.51\n",
      "Epoch   2/100 Batch  500/821 - Loss:  0.001, Seconds: 75.24\n",
      "Epoch   2/100 Batch  550/821 - Loss:  0.001, Seconds: 83.18\n",
      "Epoch   2/100 Batch  600/821 - Loss:  0.001, Seconds: 78.19\n",
      "Epoch   2/100 Batch  650/821 - Loss:  0.001, Seconds: 77.41\n",
      "Epoch   2/100 Batch  700/821 - Loss:  0.001, Seconds: 78.01\n",
      "Epoch   2/100 Batch  750/821 - Loss:  0.001, Seconds: 80.41\n",
      "Epoch   2/100 Batch  800/821 - Loss:  0.001, Seconds: 71.43\n",
      "Testing Loss:  0.002, Seconds: 139.06\n",
      "New Record!\n",
      "Epoch   3/100 Batch   50/821 - Loss:  0.062, Seconds: 80.51\n",
      "Epoch   3/100 Batch  100/821 - Loss:  0.001, Seconds: 85.57\n",
      "Epoch   3/100 Batch  150/821 - Loss:  0.002, Seconds: 80.28\n",
      "Epoch   3/100 Batch  200/821 - Loss:  0.001, Seconds: 78.07\n",
      "Epoch   3/100 Batch  250/821 - Loss:  0.001, Seconds: 84.53\n",
      "Epoch   3/100 Batch  300/821 - Loss:  0.001, Seconds: 95.13\n",
      "Epoch   3/100 Batch  350/821 - Loss:  0.001, Seconds: 80.75\n",
      "Epoch   3/100 Batch  400/821 - Loss:  0.001, Seconds: 83.01\n",
      "Epoch   3/100 Batch  450/821 - Loss:  0.001, Seconds: 78.10\n",
      "Epoch   3/100 Batch  500/821 - Loss:  0.001, Seconds: 77.85\n",
      "Epoch   3/100 Batch  550/821 - Loss:  0.001, Seconds: 86.19\n",
      "Epoch   3/100 Batch  600/821 - Loss:  0.001, Seconds: 81.20\n",
      "Epoch   3/100 Batch  650/821 - Loss:  0.001, Seconds: 83.58\n",
      "Epoch   3/100 Batch  700/821 - Loss:  0.001, Seconds: 83.68\n",
      "Epoch   3/100 Batch  750/821 - Loss:  0.001, Seconds: 86.70\n",
      "Epoch   3/100 Batch  800/821 - Loss:  0.001, Seconds: 75.16\n",
      "Testing Loss:  0.002, Seconds: 148.51\n",
      "New Record!\n",
      "Epoch   4/100 Batch   50/821 - Loss:  0.049, Seconds: 85.62\n",
      "Epoch   4/100 Batch  100/821 - Loss:  0.002, Seconds: 90.04\n",
      "Epoch   4/100 Batch  150/821 - Loss:  0.002, Seconds: 82.50\n",
      "Epoch   4/100 Batch  200/821 - Loss:  0.001, Seconds: 81.66\n",
      "Epoch   4/100 Batch  250/821 - Loss:  0.001, Seconds: 87.25\n",
      "Epoch   4/100 Batch  300/821 - Loss:  0.001, Seconds: 100.01\n",
      "Epoch   4/100 Batch  350/821 - Loss:  0.001, Seconds: 83.40\n",
      "Epoch   4/100 Batch  400/821 - Loss:  0.001, Seconds: 86.18\n",
      "Epoch   4/100 Batch  450/821 - Loss:  0.001, Seconds: 80.62\n",
      "Epoch   4/100 Batch  500/821 - Loss:  0.001, Seconds: 80.10\n",
      "Epoch   4/100 Batch  550/821 - Loss:  0.001, Seconds: 88.69\n",
      "Epoch   4/100 Batch  600/821 - Loss:  0.001, Seconds: 83.35\n",
      "Epoch   4/100 Batch  650/821 - Loss:  0.001, Seconds: 82.36\n",
      "Epoch   4/100 Batch  700/821 - Loss:  0.001, Seconds: 83.41\n",
      "Epoch   4/100 Batch  750/821 - Loss:  0.001, Seconds: 85.99\n",
      "Epoch   4/100 Batch  800/821 - Loss:  0.001, Seconds: 76.33\n",
      "Testing Loss:  0.002, Seconds: 147.47\n",
      "New Record!\n",
      "Epoch   5/100 Batch   50/821 - Loss:  0.041, Seconds: 85.82\n",
      "Epoch   5/100 Batch  100/821 - Loss:  0.001, Seconds: 90.86\n",
      "Epoch   5/100 Batch  150/821 - Loss:  0.001, Seconds: 84.50\n",
      "Epoch   5/100 Batch  200/821 - Loss:  0.001, Seconds: 82.80\n",
      "Epoch   5/100 Batch  250/821 - Loss:  0.001, Seconds: 89.71\n",
      "Epoch   5/100 Batch  300/821 - Loss:  0.001, Seconds: 101.28\n",
      "Epoch   5/100 Batch  350/821 - Loss:  0.001, Seconds: 85.76\n",
      "Epoch   5/100 Batch  400/821 - Loss:  0.001, Seconds: 88.36\n",
      "Epoch   5/100 Batch  450/821 - Loss:  0.001, Seconds: 83.17\n",
      "Epoch   5/100 Batch  500/821 - Loss:  0.001, Seconds: 82.84\n",
      "Epoch   5/100 Batch  550/821 - Loss:  0.001, Seconds: 91.35\n",
      "Epoch   5/100 Batch  600/821 - Loss:  0.001, Seconds: 86.33\n",
      "Epoch   5/100 Batch  650/821 - Loss:  0.001, Seconds: 85.05\n",
      "Epoch   5/100 Batch  700/821 - Loss:  0.001, Seconds: 85.88\n",
      "Epoch   5/100 Batch  750/821 - Loss:  0.001, Seconds: 88.08\n",
      "Epoch   5/100 Batch  800/821 - Loss:  0.001, Seconds: 79.08\n",
      "Testing Loss:  0.001, Seconds: 151.65\n",
      "New Record!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-960dff6ce3ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m                     learning_rate, embedding_size, direction, char_to_int_proper_nouns)\n\u001b[1;32m     13\u001b[0m train(model_proper_nouns, epochs, log_string, int_before_dev_proper_nouns, int_after_dev_proper_nouns, \n\u001b[0;32m---> 14\u001b[0;31m       int_before_val_proper_nouns, int_after_val_proper_nouns, char_to_int_proper_nouns, reset = False)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-16-443e5b783dbd>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, epochs, log_string, int_before_train, int_after_train, int_before_test, int_after_test, char_to_int, reset, early_stopping)\u001b[0m\n\u001b[1;32m    112\u001b[0m                                               \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minput_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m                                               \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtargets_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtarget_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m                                               model.keep_prob: keep_probability})\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sourabhjha/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sourabhjha/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sourabhjha/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/sourabhjha/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sourabhjha/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "batch_size = 600\n",
    "num_layers = 3\n",
    "rnn_size = 128\n",
    "embedding_size = len(chars_proper_nouns)\n",
    "learning_rate = 0.0006\n",
    "direction = 2\n",
    "keep_probability = 0.75\n",
    "\n",
    "log_string = 'all_proper_nouns_model_1'\n",
    "model_proper_nouns = build_graph(keep_probability, rnn_size, num_layers, batch_size, \n",
    "                    learning_rate, embedding_size, direction, char_to_int_proper_nouns)\n",
    "train(model_proper_nouns, epochs, log_string, int_before_dev_proper_nouns, int_after_dev_proper_nouns, \n",
    "      int_before_val_proper_nouns, int_after_val_proper_nouns, char_to_int_proper_nouns, reset = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def token_to_ints_proper_nouns(token):\n",
    "    return [char_to_int_proper_nouns[c] for c in token if c in chars_proper_nouns]\n",
    "\n",
    "\n",
    "def score_all_proper_nouns(phrases, max_target_length = 150):\n",
    "    checkpoint = \"./all_proper_nouns_model_1.ckpt\"\n",
    "    epochs = 100\n",
    "    batch_size = 500\n",
    "    num_layers = 3\n",
    "    rnn_size = 128\n",
    "    embedding_size = len(chars_proper_nouns)\n",
    "    learning_rate = 0.0005\n",
    "    direction = 2\n",
    "    keep_probability = 0.75\n",
    "#     tokens1 = [token_to_ints(prev_token) + [char_to_int['<SEP>']] + token_to_ints(token) \n",
    "#                for prev_token, token in zip(prev_tokens, tokens)] \n",
    "    tokens1 = [reduce(lambda a,b : a + b, [token_to_ints_proper_nouns(p) + [char_to_int_proper_nouns['<SEP>']] \n",
    "                                           for p in phrase.split(\"<SEP>\")])[:-1] for phrase in phrases] \n",
    "    \n",
    "    pad_tokens = pad_sentence_batch(tokens1, char_to_int_proper_nouns)\n",
    "        \n",
    "        # Need the lengths for the _lengths parameters\n",
    "    pad_tokens_lengths = []\n",
    "    pad_targets_lengths = []\n",
    "    for token in pad_tokens:\n",
    "        pad_tokens_lengths.append(len(token))    \n",
    "        pad_targets_lengths.append(len(token) + max_target_length)\n",
    "    model = build_graph(keep_probability, rnn_size, num_layers, len(phrases), \n",
    "                                learning_rate, embedding_size, direction, char_to_int_proper_nouns)\n",
    "    with tf.Session() as sess:\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess, checkpoint)\n",
    "        \n",
    "        output_logits = sess.run(model.predictions, {model.inputs: pad_tokens,\n",
    "                                                    model.inputs_length: pad_tokens_lengths,\n",
    "                                                    model.targets_length: [max_target_length],\n",
    "                                                    model.keep_prob: 1.0})\n",
    "    pad = char_to_int_proper_nouns['<PAD>']\n",
    "    return [\"\".join([int_to_char_proper_nouns[i] for i in output_logit if i != pad]).replace(\"<EOS>\",\"\").replace(\"<SELF>\", phrases[j].split(\"<SEP>\")[2])\n",
    "            for j, output_logit in enumerate(output_logits)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./all_proper_nouns_model_1.ckpt\n",
      "0.9941\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Bulgaria&lt;SEP&gt;)&lt;SEP&gt;Boev&lt;SEP&gt;,&lt;SEP&gt;Z.</td>\n",
       "      <td>Boev</td>\n",
       "      <td>b o e v</td>\n",
       "      <td>Boev</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>and&lt;SEP&gt;Mynydd&lt;SEP&gt;Tarw&lt;SEP&gt;.&lt;SEP&gt;</td>\n",
       "      <td>Tarw</td>\n",
       "      <td>t a r w</td>\n",
       "      <td>Tarw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750</th>\n",
       "      <td>&lt;SEP&gt;&lt;SEP&gt;Knai&lt;SEP&gt;Thoma&lt;SEP&gt;and</td>\n",
       "      <td>Knai</td>\n",
       "      <td>k n a i</td>\n",
       "      <td>Knai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875</th>\n",
       "      <td>Primiero&lt;SEP&gt;,&lt;SEP&gt;Imer&lt;SEP&gt;,&lt;SEP&gt;Mezzano</td>\n",
       "      <td>Imer</td>\n",
       "      <td>i m e r</td>\n",
       "      <td>Imer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>941</th>\n",
       "      <td>holidays&lt;SEP&gt;\"&lt;SEP&gt;Revitalise&lt;SEP&gt;.&lt;SEP&gt;</td>\n",
       "      <td>Revitalise</td>\n",
       "      <td>revitalize</td>\n",
       "      <td>Revitalise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1166</th>\n",
       "      <td>&lt;SEP&gt;&lt;SEP&gt;Agmo&lt;SEP&gt;2007&lt;SEP&gt;,</td>\n",
       "      <td>Agmo</td>\n",
       "      <td>a g m o</td>\n",
       "      <td>Agmo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1524</th>\n",
       "      <td>&lt;SEP&gt;De&lt;SEP&gt;Asse&lt;SEP&gt;et&lt;SEP&gt;partibus</td>\n",
       "      <td>Asse</td>\n",
       "      <td>a s s e</td>\n",
       "      <td>Asse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1547</th>\n",
       "      <td>files&lt;SEP&gt;for&lt;SEP&gt;SoS&lt;SEP&gt;\"&lt;SEP&gt;.</td>\n",
       "      <td>SoS</td>\n",
       "      <td>s o s</td>\n",
       "      <td>SoS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1553</th>\n",
       "      <td>,&lt;SEP&gt;52, &lt;SEP&gt;No&lt;SEP&gt;3,&lt;SEP&gt;,</td>\n",
       "      <td>No</td>\n",
       "      <td>number</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1577</th>\n",
       "      <td>kings&lt;SEP&gt;owner&lt;SEP&gt;Zygi&lt;SEP&gt;Wilf&lt;SEP&gt;potted</td>\n",
       "      <td>Zygi</td>\n",
       "      <td>Zygi</td>\n",
       "      <td>z y g i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1588</th>\n",
       "      <td>commune&lt;SEP&gt;in&lt;SEP&gt;Dolj&lt;SEP&gt;County&lt;SEP&gt;,</td>\n",
       "      <td>Dolj</td>\n",
       "      <td>d o l j</td>\n",
       "      <td>Dolj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1674</th>\n",
       "      <td>&lt;SEP&gt;&lt;SEP&gt;Puiu&lt;SEP&gt;,&lt;SEP&gt;Tibi</td>\n",
       "      <td>Puiu</td>\n",
       "      <td>p u i u</td>\n",
       "      <td>Puiu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1991</th>\n",
       "      <td>to&lt;SEP&gt;B&lt;SEP&gt;#Louis&lt;SEP&gt;Andriessen&lt;SEP&gt;</td>\n",
       "      <td>#Louis</td>\n",
       "      <td>hash tag louis</td>\n",
       "      <td>hash tag logisses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2219</th>\n",
       "      <td>,&lt;SEP&gt;and&lt;SEP&gt;Fipp&lt;SEP&gt;sing&lt;SEP&gt;of</td>\n",
       "      <td>Fipp</td>\n",
       "      <td>Fipp</td>\n",
       "      <td>f i p p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2279</th>\n",
       "      <td>&lt;SEP&gt;The&lt;SEP&gt;Nightrise&lt;SEP&gt;orporation&lt;SEP&gt;</td>\n",
       "      <td>Nightrise</td>\n",
       "      <td>Nightrise</td>\n",
       "      <td>nightrizes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2590</th>\n",
       "      <td>Jeroen&lt;SEP&gt;De&lt;SEP&gt;Dauw&lt;SEP&gt;and&lt;SEP&gt;Yaron</td>\n",
       "      <td>Dauw</td>\n",
       "      <td>d a u w</td>\n",
       "      <td>Dauw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2666</th>\n",
       "      <td>venskberg&lt;SEP&gt;,&lt;SEP&gt;Aila&lt;SEP&gt;(&lt;SEP&gt;ed</td>\n",
       "      <td>Aila</td>\n",
       "      <td>a i l a</td>\n",
       "      <td>Aila</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2682</th>\n",
       "      <td>\"&lt;SEP&gt;El&lt;SEP&gt;NiAo&lt;SEP&gt;forces&lt;SEP&gt;PHL</td>\n",
       "      <td>NiAo</td>\n",
       "      <td>n i a o</td>\n",
       "      <td>NiAo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2724</th>\n",
       "      <td>and&lt;SEP&gt;Abu&lt;SEP&gt;Nuwar&lt;SEP&gt;over&lt;SEP&gt;ership</td>\n",
       "      <td>Nuwar</td>\n",
       "      <td>Nuwar</td>\n",
       "      <td>n u w a r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2835</th>\n",
       "      <td>Hamada&lt;SEP&gt;(&lt;SEP&gt;Eps&lt;SEP&gt;.&lt;SEP&gt;</td>\n",
       "      <td>Eps</td>\n",
       "      <td>Eps</td>\n",
       "      <td>e p's</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2915</th>\n",
       "      <td>\"&lt;SEP&gt;La&lt;SEP&gt;Ho'iho'i&lt;SEP&gt;Ea&lt;SEP&gt;/</td>\n",
       "      <td>Ho'iho'i</td>\n",
       "      <td>h o i h o i</td>\n",
       "      <td>Ho'iho'i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3237</th>\n",
       "      <td>Z.&lt;SEP&gt;;&lt;SEP&gt;Abam&lt;SEP&gt;,&lt;SEP&gt;M. A.</td>\n",
       "      <td>Abam</td>\n",
       "      <td>a b a m</td>\n",
       "      <td>Abam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3507</th>\n",
       "      <td>L'Ordre&lt;SEP&gt;du&lt;SEP&gt;Trsor&lt;SEP&gt;Sacr&lt;SEP&gt;(</td>\n",
       "      <td>Trsor</td>\n",
       "      <td>t r e acute s o r</td>\n",
       "      <td>Trsor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3528</th>\n",
       "      <td>was&lt;SEP&gt;named&lt;SEP&gt;Qasr&lt;SEP&gt;ul&lt;SEP&gt;Aza</td>\n",
       "      <td>Qasr</td>\n",
       "      <td>Qasr</td>\n",
       "      <td>q a s r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3874</th>\n",
       "      <td>&lt;SEP&gt;Vikas&lt;SEP&gt;Amte&lt;SEP&gt;married&lt;SEP&gt;to</td>\n",
       "      <td>Amte</td>\n",
       "      <td>Amte</td>\n",
       "      <td>a m t e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3879</th>\n",
       "      <td>&lt;SEP&gt;&lt;SEP&gt;Saouaf&lt;SEP&gt;SJ&lt;SEP&gt;,</td>\n",
       "      <td>Saouaf</td>\n",
       "      <td>s a o u a f</td>\n",
       "      <td>Saouaf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4002</th>\n",
       "      <td>the&lt;SEP&gt;USSR&lt;SEP&gt;No&lt;SEP&gt;8400-XI&lt;SEP&gt;of</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>number</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4054</th>\n",
       "      <td>;&lt;SEP&gt;Sang&lt;SEP&gt;Lyul&lt;SEP&gt;Min&lt;SEP&gt;;</td>\n",
       "      <td>Lyul</td>\n",
       "      <td>l y u l</td>\n",
       "      <td>Lyul</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4192</th>\n",
       "      <td>;&lt;SEP&gt;Van&lt;SEP&gt;Vugt&lt;SEP&gt;,&lt;SEP&gt;M</td>\n",
       "      <td>Vugt</td>\n",
       "      <td>Vugt</td>\n",
       "      <td>v u g t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4230</th>\n",
       "      <td>&lt;SEP&gt;&lt;SEP&gt;No&lt;SEP&gt;605&lt;SEP&gt;quadron</td>\n",
       "      <td>No</td>\n",
       "      <td>number</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4364</th>\n",
       "      <td>&lt;SEP&gt;Hu&lt;SEP&gt;Ge's&lt;SEP&gt;2005&lt;SEP&gt;short</td>\n",
       "      <td>Ge's</td>\n",
       "      <td>g e's</td>\n",
       "      <td>Ge's</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4588</th>\n",
       "      <td>of&lt;SEP&gt;the&lt;SEP&gt;shm'Ecl&lt;SEP&gt;and&lt;SEP&gt;nothing</td>\n",
       "      <td>shm'Ecl</td>\n",
       "      <td>s h m e c l</td>\n",
       "      <td>s h h e c l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4624</th>\n",
       "      <td>nov&lt;SEP&gt;Regni&lt;SEP&gt;Veg&lt;SEP&gt;.&lt;SEP&gt;</td>\n",
       "      <td>Veg</td>\n",
       "      <td>Veg</td>\n",
       "      <td>v e g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4631</th>\n",
       "      <td>love&lt;SEP&gt;with&lt;SEP&gt;Iria&lt;SEP&gt;,&lt;SEP&gt;a</td>\n",
       "      <td>Iria</td>\n",
       "      <td>i r i a</td>\n",
       "      <td>Iria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4650</th>\n",
       "      <td>of&lt;SEP&gt;Hastings&lt;SEP&gt;Ndlovu&lt;SEP&gt;\"&lt;SEP&gt;.</td>\n",
       "      <td>Ndlovu</td>\n",
       "      <td>n d l o v u</td>\n",
       "      <td>Ndlovu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4671</th>\n",
       "      <td>&lt;SEP&gt;\"&lt;SEP&gt;Phic&lt;SEP&gt;3&lt;SEP&gt;likevirus</td>\n",
       "      <td>Phic</td>\n",
       "      <td>p h i c</td>\n",
       "      <td>Phic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5027</th>\n",
       "      <td>&lt;SEP&gt;\"&lt;SEP&gt;DivX&lt;SEP&gt;Plus&lt;SEP&gt;for</td>\n",
       "      <td>DivX</td>\n",
       "      <td>DivX</td>\n",
       "      <td>d i v x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5265</th>\n",
       "      <td>ritories&lt;SEP&gt;of&lt;SEP&gt;Rukn&lt;SEP&gt;al&lt;SEP&gt;Dawla</td>\n",
       "      <td>Rukn</td>\n",
       "      <td>r u k n</td>\n",
       "      <td>Rukn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5322</th>\n",
       "      <td>The&lt;SEP&gt;name&lt;SEP&gt;Uaru&lt;SEP&gt;comes&lt;SEP&gt;from</td>\n",
       "      <td>Uaru</td>\n",
       "      <td>u a r u</td>\n",
       "      <td>Uaru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5580</th>\n",
       "      <td>,&lt;SEP&gt;Martin&lt;SEP&gt;Dzur&lt;SEP&gt;and&lt;SEP&gt;Dobri</td>\n",
       "      <td>Dzur</td>\n",
       "      <td>d z u r</td>\n",
       "      <td>Dzur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5736</th>\n",
       "      <td>&lt;SEP&gt;The&lt;SEP&gt;Tunp&lt;SEP&gt;Stele&lt;SEP&gt;is</td>\n",
       "      <td>Tunp</td>\n",
       "      <td>t u n p</td>\n",
       "      <td>Tunp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5898</th>\n",
       "      <td>&lt;SEP&gt;20&lt;SEP&gt;Ans&lt;SEP&gt;(&lt;SEP&gt;in</td>\n",
       "      <td>Ans</td>\n",
       "      <td>a n's</td>\n",
       "      <td>Ans</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6348</th>\n",
       "      <td>songs&lt;SEP&gt;\"&lt;SEP&gt;Aadu&lt;SEP&gt;Pambe&lt;SEP&gt;\"</td>\n",
       "      <td>Aadu</td>\n",
       "      <td>a a d u</td>\n",
       "      <td>Aadu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6870</th>\n",
       "      <td>Fore&lt;SEP&gt;I&lt;SEP&gt;Diiie&lt;SEP&gt;\"&lt;SEP&gt;.</td>\n",
       "      <td>Diiie</td>\n",
       "      <td>Diiie</td>\n",
       "      <td>d i i i e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7002</th>\n",
       "      <td>York&lt;SEP&gt;Botany&lt;SEP&gt;OrganisationBrockway&lt;SEP&gt;,...</td>\n",
       "      <td>OrganisationBrockway</td>\n",
       "      <td>OrganisationBrockway</td>\n",
       "      <td>organizthorbar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7096</th>\n",
       "      <td>olica&lt;SEP&gt;Medii&lt;SEP&gt;Aevi&lt;SEP&gt;,&lt;SEP&gt;vol</td>\n",
       "      <td>Aevi</td>\n",
       "      <td>a e v i</td>\n",
       "      <td>Aevi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7677</th>\n",
       "      <td>Azadi&lt;SEP&gt;Ki&lt;SEP&gt;Raah&lt;SEP&gt;Par&lt;SEP&gt;(</td>\n",
       "      <td>Raah</td>\n",
       "      <td>Raah</td>\n",
       "      <td>r a a h</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8076</th>\n",
       "      <td>island&lt;SEP&gt;of&lt;SEP&gt;Ohe&lt;SEP&gt;.&lt;SEP&gt;</td>\n",
       "      <td>Ohe</td>\n",
       "      <td>o h e</td>\n",
       "      <td>Ohe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8155</th>\n",
       "      <td>done&lt;SEP&gt;caused&lt;SEP&gt;Gaou&lt;SEP&gt;to&lt;SEP&gt;deeply</td>\n",
       "      <td>Gaou</td>\n",
       "      <td>g a o u</td>\n",
       "      <td>Gaou</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8393</th>\n",
       "      <td>zoekadres&lt;SEP&gt;:&lt;SEP&gt;Pelikaanweg&lt;SEP&gt;50&lt;SEP&gt;,</td>\n",
       "      <td>Pelikaanweg</td>\n",
       "      <td>pelikaan weg</td>\n",
       "      <td>pelikan weg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8460</th>\n",
       "      <td>the&lt;SEP&gt;name&lt;SEP&gt;Estres&lt;SEP&gt;.&lt;SEP&gt;</td>\n",
       "      <td>Estres</td>\n",
       "      <td>Estres</td>\n",
       "      <td>e s t r e acute e's</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8506</th>\n",
       "      <td>the&lt;SEP&gt;vintage&lt;SEP&gt;Cuve&lt;SEP&gt;Sir&lt;SEP&gt;Winston</td>\n",
       "      <td>Cuve</td>\n",
       "      <td>Cuve</td>\n",
       "      <td>cuve</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8605</th>\n",
       "      <td>tian&lt;SEP&gt;backed&lt;SEP&gt;Hrvat&lt;SEP&gt;trace&lt;SEP&gt;a</td>\n",
       "      <td>Hrvat</td>\n",
       "      <td>h r v a t</td>\n",
       "      <td>Hrvat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8796</th>\n",
       "      <td>2009&lt;SEP&gt;N.Y.&lt;SEP&gt;Misc&lt;SEP&gt;.&lt;SEP&gt;</td>\n",
       "      <td>Misc</td>\n",
       "      <td>Misc</td>\n",
       "      <td>m i s c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9090</th>\n",
       "      <td>&lt;SEP&gt;\"&lt;SEP&gt;Abdu'l&lt;SEP&gt;Baha&lt;SEP&gt;in</td>\n",
       "      <td>Abdu'l</td>\n",
       "      <td>a b d u l</td>\n",
       "      <td>Abdu'l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9119</th>\n",
       "      <td>the&lt;SEP&gt;present&lt;SEP&gt;Schonbrunnerstrasse&lt;SEP&gt;be...</td>\n",
       "      <td>Schonbrunnerstrasse</td>\n",
       "      <td>schonbrunner strasse</td>\n",
       "      <td>schonbrnasser strasse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9548</th>\n",
       "      <td>de&lt;SEP&gt;las&lt;SEP&gt;Yslas&lt;SEP&gt;Filipinas&lt;SEP&gt;(</td>\n",
       "      <td>Yslas</td>\n",
       "      <td>Yslas</td>\n",
       "      <td>y s l a's</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9934</th>\n",
       "      <td>icer&lt;SEP&gt;Steven&lt;SEP&gt;Acor&lt;SEP&gt;decided&lt;SEP&gt;to</td>\n",
       "      <td>Acor</td>\n",
       "      <td>a c o r</td>\n",
       "      <td>Acor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9969</th>\n",
       "      <td>oneya&lt;SEP&gt;Kumar&lt;SEP&gt;Tiu&lt;SEP&gt;.&lt;SEP&gt;</td>\n",
       "      <td>Tiu</td>\n",
       "      <td>t i u</td>\n",
       "      <td>Tiu</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      0                     1  \\\n",
       "23                 Bulgaria<SEP>)<SEP>Boev<SEP>,<SEP>Z.                  Boev   \n",
       "173                  and<SEP>Mynydd<SEP>Tarw<SEP>.<SEP>                  Tarw   \n",
       "750                    <SEP><SEP>Knai<SEP>Thoma<SEP>and                  Knai   \n",
       "875           Primiero<SEP>,<SEP>Imer<SEP>,<SEP>Mezzano                  Imer   \n",
       "941            holidays<SEP>\"<SEP>Revitalise<SEP>.<SEP>            Revitalise   \n",
       "1166                      <SEP><SEP>Agmo<SEP>2007<SEP>,                  Agmo   \n",
       "1524               <SEP>De<SEP>Asse<SEP>et<SEP>partibus                  Asse   \n",
       "1547                  files<SEP>for<SEP>SoS<SEP>\"<SEP>.                   SoS   \n",
       "1553                     ,<SEP>52, <SEP>No<SEP>3,<SEP>,                    No   \n",
       "1577       kings<SEP>owner<SEP>Zygi<SEP>Wilf<SEP>potted                  Zygi   \n",
       "1588           commune<SEP>in<SEP>Dolj<SEP>County<SEP>,                  Dolj   \n",
       "1674                      <SEP><SEP>Puiu<SEP>,<SEP>Tibi                  Puiu   \n",
       "1991            to<SEP>B<SEP>#Louis<SEP>Andriessen<SEP>                #Louis   \n",
       "2219                 ,<SEP>and<SEP>Fipp<SEP>sing<SEP>of                  Fipp   \n",
       "2279         <SEP>The<SEP>Nightrise<SEP>orporation<SEP>             Nightrise   \n",
       "2590           Jeroen<SEP>De<SEP>Dauw<SEP>and<SEP>Yaron                  Dauw   \n",
       "2666              venskberg<SEP>,<SEP>Aila<SEP>(<SEP>ed                  Aila   \n",
       "2682               \"<SEP>El<SEP>NiAo<SEP>forces<SEP>PHL                  NiAo   \n",
       "2724          and<SEP>Abu<SEP>Nuwar<SEP>over<SEP>ership                 Nuwar   \n",
       "2835                    Hamada<SEP>(<SEP>Eps<SEP>.<SEP>                   Eps   \n",
       "2915                 \"<SEP>La<SEP>Ho'iho'i<SEP>Ea<SEP>/              Ho'iho'i   \n",
       "3237                  Z.<SEP>;<SEP>Abam<SEP>,<SEP>M. A.                  Abam   \n",
       "3507          L'Ordre<SEP>du<SEP>Trsor<SEP>Sacr<SEP>(                Trsor   \n",
       "3528              was<SEP>named<SEP>Qasr<SEP>ul<SEP>Aza                  Qasr   \n",
       "3874             <SEP>Vikas<SEP>Amte<SEP>married<SEP>to                  Amte   \n",
       "3879                      <SEP><SEP>Saouaf<SEP>SJ<SEP>,                Saouaf   \n",
       "4002             the<SEP>USSR<SEP>No<SEP>8400-XI<SEP>of                    No   \n",
       "4054                  ;<SEP>Sang<SEP>Lyul<SEP>Min<SEP>;                  Lyul   \n",
       "4192                     ;<SEP>Van<SEP>Vugt<SEP>,<SEP>M                  Vugt   \n",
       "4230                   <SEP><SEP>No<SEP>605<SEP>quadron                    No   \n",
       "4364                <SEP>Hu<SEP>Ge's<SEP>2005<SEP>short                  Ge's   \n",
       "4588         of<SEP>the<SEP>shm'Ecl<SEP>and<SEP>nothing               shm'Ecl   \n",
       "4624                   nov<SEP>Regni<SEP>Veg<SEP>.<SEP>                   Veg   \n",
       "4631                 love<SEP>with<SEP>Iria<SEP>,<SEP>a                  Iria   \n",
       "4650             of<SEP>Hastings<SEP>Ndlovu<SEP>\"<SEP>.                Ndlovu   \n",
       "4671                <SEP>\"<SEP>Phic<SEP>3<SEP>likevirus                  Phic   \n",
       "5027                   <SEP>\"<SEP>DivX<SEP>Plus<SEP>for                  DivX   \n",
       "5265          ritories<SEP>of<SEP>Rukn<SEP>al<SEP>Dawla                  Rukn   \n",
       "5322           The<SEP>name<SEP>Uaru<SEP>comes<SEP>from                  Uaru   \n",
       "5580            ,<SEP>Martin<SEP>Dzur<SEP>and<SEP>Dobri                  Dzur   \n",
       "5736                 <SEP>The<SEP>Tunp<SEP>Stele<SEP>is                  Tunp   \n",
       "5898                       <SEP>20<SEP>Ans<SEP>(<SEP>in                   Ans   \n",
       "6348               songs<SEP>\"<SEP>Aadu<SEP>Pambe<SEP>\"                  Aadu   \n",
       "6870                   Fore<SEP>I<SEP>Diiie<SEP>\"<SEP>.                 Diiie   \n",
       "7002  York<SEP>Botany<SEP>OrganisationBrockway<SEP>,...  OrganisationBrockway   \n",
       "7096             olica<SEP>Medii<SEP>Aevi<SEP>,<SEP>vol                  Aevi   \n",
       "7677                Azadi<SEP>Ki<SEP>Raah<SEP>Par<SEP>(                  Raah   \n",
       "8076                   island<SEP>of<SEP>Ohe<SEP>.<SEP>                   Ohe   \n",
       "8155         done<SEP>caused<SEP>Gaou<SEP>to<SEP>deeply                  Gaou   \n",
       "8393       zoekadres<SEP>:<SEP>Pelikaanweg<SEP>50<SEP>,           Pelikaanweg   \n",
       "8460                the<SEP>name<SEP>Estres<SEP>.<SEP>               Estres   \n",
       "8506      the<SEP>vintage<SEP>Cuve<SEP>Sir<SEP>Winston                 Cuve   \n",
       "8605          tian<SEP>backed<SEP>Hrvat<SEP>trace<SEP>a                 Hrvat   \n",
       "8796                  2009<SEP>N.Y.<SEP>Misc<SEP>.<SEP>                  Misc   \n",
       "9090                  <SEP>\"<SEP>Abdu'l<SEP>Baha<SEP>in                Abdu'l   \n",
       "9119  the<SEP>present<SEP>Schonbrunnerstrasse<SEP>be...   Schonbrunnerstrasse   \n",
       "9548           de<SEP>las<SEP>Yslas<SEP>Filipinas<SEP>(                 Yslas   \n",
       "9934        icer<SEP>Steven<SEP>Acor<SEP>decided<SEP>to                  Acor   \n",
       "9969                 oneya<SEP>Kumar<SEP>Tiu<SEP>.<SEP>                   Tiu   \n",
       "\n",
       "                         2                      3  \n",
       "23                 b o e v                   Boev  \n",
       "173                t a r w                   Tarw  \n",
       "750                k n a i                   Knai  \n",
       "875                i m e r                   Imer  \n",
       "941             revitalize             Revitalise  \n",
       "1166               a g m o                   Agmo  \n",
       "1524               a s s e                   Asse  \n",
       "1547                 s o s                    SoS  \n",
       "1553                number                     No  \n",
       "1577                  Zygi                z y g i  \n",
       "1588               d o l j                   Dolj  \n",
       "1674               p u i u                   Puiu  \n",
       "1991        hash tag louis      hash tag logisses  \n",
       "2219                  Fipp                f i p p  \n",
       "2279             Nightrise             nightrizes  \n",
       "2590               d a u w                   Dauw  \n",
       "2666               a i l a                   Aila  \n",
       "2682               n i a o                   NiAo  \n",
       "2724                 Nuwar              n u w a r  \n",
       "2835                   Eps                  e p's  \n",
       "2915           h o i h o i               Ho'iho'i  \n",
       "3237               a b a m                   Abam  \n",
       "3507     t r e acute s o r                 Trsor  \n",
       "3528                  Qasr                q a s r  \n",
       "3874                  Amte                a m t e  \n",
       "3879           s a o u a f                 Saouaf  \n",
       "4002                    No                 number  \n",
       "4054               l y u l                   Lyul  \n",
       "4192                  Vugt                v u g t  \n",
       "4230                number                     No  \n",
       "4364                 g e's                   Ge's  \n",
       "4588           s h m e c l            s h h e c l  \n",
       "4624                   Veg                  v e g  \n",
       "4631               i r i a                   Iria  \n",
       "4650           n d l o v u                 Ndlovu  \n",
       "4671               p h i c                   Phic  \n",
       "5027                  DivX                d i v x  \n",
       "5265               r u k n                   Rukn  \n",
       "5322               u a r u                   Uaru  \n",
       "5580               d z u r                   Dzur  \n",
       "5736               t u n p                   Tunp  \n",
       "5898                 a n's                    Ans  \n",
       "6348               a a d u                   Aadu  \n",
       "6870                 Diiie              d i i i e  \n",
       "7002  OrganisationBrockway         organizthorbar  \n",
       "7096               a e v i                   Aevi  \n",
       "7677                  Raah                r a a h  \n",
       "8076                 o h e                    Ohe  \n",
       "8155               g a o u                   Gaou  \n",
       "8393          pelikaan weg            pelikan weg  \n",
       "8460               Estres    e s t r e acute e's  \n",
       "8506                 Cuve                   cuve  \n",
       "8605             h r v a t                  Hrvat  \n",
       "8796                  Misc                m i s c  \n",
       "9090             a b d u l                 Abdu'l  \n",
       "9119  schonbrunner strasse  schonbrnasser strasse  \n",
       "9548                 Yslas              y s l a's  \n",
       "9934               a c o r                   Acor  \n",
       "9969                 t i u                    Tiu  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_pred = 10000\n",
    "predictions =  score_all_proper_nouns(list(val_proper_nouns['before_phrase'])[:n_pred])\n",
    "res = pd.DataFrame(zip(list(val_proper_nouns['before_phrase'])[:n_pred],list(val_proper_nouns['before'])[:n_pred], \n",
    "                       list(val_proper_nouns['after'])[:n_pred],predictions))\n",
    "\n",
    "print(np.mean(res[2] == res[3]))\n",
    "res[res[2] != res[3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.91176470588235292"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(res[res[1] == \"No\"][2] == res[res[1] == \"No\"][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "No        32\n",
       "number     2\n",
       "Name: 3, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[res[1] == \"No\"][3].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_all[test_all.before == \"&\"][['before', 'after', 'sentence_id']].groupby(['before', 'after']).count() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data[train_data.before.map(lambda x : x.lower()) == \"centre\"][['before', 'after', 'sentence_id']].groupby(['before', 'after']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99937467655683976"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_map = {\"&\" : \"and\", \"centre\" : \"center\", \"#\":\"number\", \"theatre\" : \"theater\", \"i.e.\" : \"i e\", \"metre\" : \"meter\",\n",
    "            }\n",
    "np.mean(val_clean.after == val_clean.before.map(lambda x : clean_map[x.lower()] if x.lower() in clean_map.keys() else \n",
    "                                                x[:-3].lower() + \"or\" if x[-3:] == \"our\" and len(x) > 4 else \n",
    "                                                x[:-5].lower() + \"ored\" if x[-5:] == \"oured\" and len(x) > 6 else x)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_clean[(val_clean.before.map(lambda x : clean_map[x.lower()] if x.lower() in clean_map.keys() else \n",
    "                                x[:-3].lower() + \"or\" if x[-3:] == \"our\" and len(x) > 4 else \n",
    "                                x[:-5].lower() + \"ored\" if x[-5:] == \"oured\" and len(x) > 6 else x)\n",
    "           != val_clean.after)][['before', 'after', 'sentence_id']].groupby(\n",
    "                                    ['before', 'after']).count().sort_values(by = 'sentence_id', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_proper_nouns[val_proper_nouns.before != val_proper_nouns.after][['before', 'after', 'sentence_id']].groupby(\n",
    "                                    ['before', 'after']).count().sort_values(by = 'sentence_id', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.998197298333551"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(val_punct.shape[0]*1.0 + val_clean.shape[0]*0.99937467655683976 + val_proper_nouns.shape[0]*0.9941 + \n",
    " val_elec.shape[0]*0.792315680166 + val_rest.shape[0]*0.992082641046 + val_data_numeric.shape[0]*0.992 + \n",
    " val_data_letters.shape[0]*0.984244879586)/(val_punct.shape[0] + val_clean.shape[0] + val_proper_nouns.shape[0] + \n",
    " val_elec.shape[0] + val_rest.shape[0] + val_data_numeric.shape[0] + val_data_letters.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sourabhjha/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/home/sourabhjha/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n",
      "/home/sourabhjha/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/home/sourabhjha/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "test_data['prev_before'] = test_data['before'].shift(1)\n",
    "test_data['prev_before'][test_data.sentence_id != test_data.sentence_id.shift(1)] = \"\"\n",
    "test_data['prev_prev_before'] = test_data['before'].shift(2)\n",
    "test_data['prev_prev_before'][test_data.sentence_id != test_data.sentence_id.shift(2)] = \"\"\n",
    "test_data['next_before'] = test_data['before'].shift(-1)\n",
    "test_data['next_before'][test_data.sentence_id != test_data.sentence_id.shift(-1)] = \"\"\n",
    "test_data['next_next_before'] = test_data['before'].shift(-2)\n",
    "test_data['next_next_before'][test_data.sentence_id != test_data.sentence_id.shift(-2)] = \"\"\n",
    "\n",
    "test_data['before_phrase'] = (test_data.prev_prev_before + \"<SEP>\" + test_data.prev_before + \"<SEP>\" +  test_data.before \n",
    "                                + \"<SEP>\" + test_data.next_before + \"<SEP>\" + test_data.next_next_before)\n",
    "\n",
    "test_data['before_phrase'] = test_data['before_phrase'].map(truncate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((45740, 8),\n",
       " (19937, 8),\n",
       " (186826, 8),\n",
       " (595016, 8),\n",
       " (69555, 8),\n",
       " (674, 8),\n",
       " (38298, 8))"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_numeric = test_data[(test_data.before.map(lambda x : max([i in x for i in \"0123456789\"])) == 1) & \n",
    "                       (test_data.before.map(lambda x : 1.0 if len(rx.findall(x)) > 0 and \n",
    "                        \" \" not in x and len(x) > 5 else 0.0) == 0.0)]\n",
    "test_data_letters = test_data[(test_data.index.isin(test_data_numeric.index) == False) & \n",
    "                ((test_data.before.map(lambda x : x.upper() == x)) | \n",
    "                 (test_data.before.map(lambda x : max([i in \"aeiouy\" for i in x.lower()])) == 0)) &\n",
    "                (test_data.before.map(lambda x : min([i in \"abcdefghijklmnopqrstuvwxyz. \" for i in x.lower()])) == 1) &\n",
    "                (test_data.before.map(len) > 1)]\n",
    "test_rest = test_data[(test_data.index.isin(test_data_numeric.index) == False) & \n",
    "         (test_data.index.isin(test_data_letters.index) == False)]\n",
    "test_punct = test_rest[test_rest.before.isin(punctuations)]\n",
    "test_rest = test_rest[test_rest.before.isin(punctuations) == False]\n",
    "\n",
    "test_clean = test_rest[test_rest.before.map(lambda x : x.lower() in clean_words)]\n",
    "test_rest = test_rest[test_rest.before.map(lambda x : x.lower() in clean_words) == False]\n",
    "\n",
    "test_elec = test_rest[test_rest.before.map(lambda x : len(rel.findall(x)) > 0)]\n",
    "test_rest = test_rest[test_rest.before.map(lambda x : len(rel.findall(x)) == 0)]\n",
    "\n",
    "test_proper_nouns = test_rest[test_rest.before.map(lambda x : len(rpn.findall(x)) > 0)]\n",
    "test_rest = test_rest[test_rest.before.map(lambda x : len(rpn.findall(x)) == 0)]\n",
    "\n",
    "test_data_numeric.shape, test_data_letters.shape, test_punct.shape, test_clean.shape, test_proper_nouns.shape, test_elec.shape, test_rest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./all_numeric_model_1.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sourabhjha/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./all_letters_model_1.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sourabhjha/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./all_rest_model_1.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sourabhjha/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n",
      "/home/sourabhjha/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./all_proper_nouns_model_1.ckpt\n"
     ]
    }
   ],
   "source": [
    "test_data_numeric['after'] = score_all_numeric(list(test_data_numeric['before_phrase']))\n",
    "test_data_letters['after'] = score_all_letters(list(test_data_letters['before_phrase']))\n",
    "test_rest1 = test_rest[test_rest.before.map(len) < 80]\n",
    "test_rest1['after'] = score_all_rest(list(test_rest1['before_phrase']))\n",
    "test_rest2 = test_rest[test_rest.before.map(len) >= 80]\n",
    "test_rest2['after'] = test_rest2.before\n",
    "test_clean['after'] = test_clean.before\n",
    "test_clean.after = test_clean.before.map(lambda x : clean_map[x.lower()] if x.lower() in clean_map.keys() else \n",
    "                                         x[:-3].lower() + \"or\" if x[-3:] == \"our\" and len(x) > 4 else \n",
    "                                x[:-5].lower() + \"ored\" if x[-5:] == \"oured\" and len(x) > 6 else x)\n",
    "test_punct['after'] = test_punct.before\n",
    "test_proper_nouns['after'] = score_all_proper_nouns(list(test_proper_nouns['before_phrase']))\n",
    "test_elec['after'] = test_elec.before.map(clean_elec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_all = test_data_numeric.append(test_data_letters).append(test_rest1).append(test_rest2).append(test_clean).append(\n",
    "    test_punct).append(test_proper_nouns).append(test_elec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_all['id'] = test_all.apply(lambda row : str(row['sentence_id']) + \"_\" + str(row['token_id']), axis = 1)\n",
    "test_all[['id', 'after']].to_csv(\"./seq2seq_submission.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((956046, 10), (956046, 2))"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_all.shape, sample_submission.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
